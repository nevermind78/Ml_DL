{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S√©ance 9: Apprentissage Non Supervis√©\n",
        "\n",
        "::: {.callout-note icon=false}\n",
        "## Informations de la s√©ance\n",
        "- **Type**: Cours\n",
        "- **Dur√©e**: 2h\n",
        "- **Objectifs**: Obj8, Obj9\n",
        ":::\n",
        "\n",
        "## D√©finitions et Principes\n",
        "\n",
        "L'**apprentissage non supervis√©** est un type d'apprentissage o√π le mod√®le apprend √† partir de **donn√©es non √©tiquet√©es**, sans r√©ponses connues.\n",
        "\n",
        "**Objectif principal**: D√©couvrir des structures, des patterns ou des regroupements naturels dans les donn√©es.\n",
        "\n",
        "::: {.callout-tip}\n",
        "## Pourquoi l'apprentissage non supervis√© ?\n",
        "- Les donn√©es √©tiquet√©es sont rares ou co√ªteuses √† obtenir\n",
        "- Exploration de donn√©es inconnues\n",
        "- R√©duction de dimension pour visualisation\n",
        "- D√©tection d'anomalies\n",
        ":::\n",
        "\n",
        "## Clustering (Regroupement)\n",
        "\n",
        "Le **clustering** consiste √† regrouper des donn√©es similaires dans des clusters (groupes).\n",
        "\n",
        "### k-means\n",
        "\n",
        "L'algorithme **k-means** est l'une des m√©thodes de clustering les plus populaires.\n",
        "\n",
        "**Principe**:\n",
        "\n",
        "1. Choisir k points initiaux (centro√Ødes)\n",
        "2. Assigner chaque point au centro√Øde le plus proche\n",
        "3. Recalculer les centro√Ødes (moyenne des points du cluster)\n",
        "4. R√©p√©ter jusqu'√† convergence"
      ],
      "id": "1c8ae836"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Donn√©es non √©tiquet√©es\n",
        "X = np.array([[1, 2], [1, 4], [1, 0],\n",
        "              [10, 2], [10, 4], [10, 0]])\n",
        "\n",
        "# Clustering avec k=2\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "print(\"Labels des clusters:\", labels)\n",
        "print(\"Centro√Ødes:\", kmeans.cluster_centers_)"
      ],
      "id": "e1b157a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Avantages**:\n",
        "\n",
        "- Simple et rapide\n",
        "- √âvolutif pour grands datasets\n",
        "- R√©sultats faciles √† interpr√©ter\n",
        "\n",
        "**Inconv√©nients**:\n",
        "\n",
        "- N√©cessite de sp√©cifier k\n",
        "- Sensible aux valeurs aberrantes\n",
        "- Suppose des clusters sph√©riques et de taille similaire\n",
        "\n",
        "### DBSCAN (Density-Based Spatial Clustering)\n",
        "\n",
        "**DBSCAN** regroupe les points bas√©s sur la densit√©.\n",
        "\n",
        "**Param√®tres cl√©s**:\n",
        "\n",
        "- **eps**: distance maximale entre deux points pour √™tre consid√©r√©s voisins\n",
        "- **min_samples**: nombre minimum de points pour former un cluster dense"
      ],
      "id": "4e07a0ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Clustering par densit√©\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=2)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "print(\"Labels DBSCAN:\", labels)\n",
        "# -1 = bruit (outliers)"
      ],
      "id": "8eeab5c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Avantages**:\n",
        "\n",
        "- Pas besoin de sp√©cifier le nombre de clusters\n",
        "- D√©tecte les clusters de forme arbitraire\n",
        "- Robuste aux outliers\n",
        "\n",
        "**Inconv√©nients**:\n",
        "\n",
        "- Sensible aux param√®tres eps et min_samples\n",
        "- Difficult√© avec des densit√©s vari√©es\n",
        "\n",
        "### Autres m√©thodes\n",
        "\n",
        "- **Agglomerative Clustering**: approche hi√©rarchique\n",
        "- **Gaussian Mixture Models (GMM)**: mod√®le probabiliste\n",
        "- **Mean Shift**: bas√© sur la densit√© de noyau\n",
        "\n",
        "## Mesures de Qualit√©\n",
        "\n",
        "Comment √©valuer la qualit√© d'un clustering sans labels vrais ?\n",
        "\n",
        "### Silhouette Score\n",
        "\n",
        "Mesure de coh√©rence intra-cluster et s√©paration inter-cluster.\n",
        "\n",
        "**Valeurs**:\n",
        "\n",
        "- Proche de 1: bonne s√©paration\n",
        "- Proche de 0: clusters se chevauchent\n",
        "- N√©gatif: mauvais clustering"
      ],
      "id": "bdf0be75"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "score = silhouette_score(X, labels)\n",
        "print(f\"Silhouette Score: {score:.3f}\")"
      ],
      "id": "9169b35f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inertie (Elbow Method)\n",
        "\n",
        "Somme des distances carr√©es des points √† leur centro√Øde."
      ],
      "id": "22b524e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inertias = []\n",
        "K = range(1, 10)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(K, inertias, 'bo-')\n",
        "plt.xlabel('Nombre de clusters (k)')\n",
        "plt.ylabel('Inertie')\n",
        "plt.title('M√©thode du coude (Elbow Method)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "e20d59c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Davies-Bouldin Index\n",
        "\n",
        "Mesure de similarit√© moyenne entre clusters.\n",
        "\n",
        "## Applications R√©elles\n",
        "\n",
        "### Segmentation Client"
      ],
      "id": "66a74269"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Exemple fictif de segmentation client\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
        "    'revenu_annuel_k': [40, 45, 50, 80, 90, 30, 35, 25],\n",
        "    'score_depense': [8, 7, 6, 9, 8, 3, 4, 2]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalisation\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df)\n",
        "\n",
        "# Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "print(df.groupby('cluster').mean())"
      ],
      "id": "b7e5debf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regroupement de Documents\n",
        "\n",
        "- Groupement d'articles par th√®me\n",
        "- Organisation d'emails\n",
        "- Cat√©gorisation de produits\n",
        "\n",
        "### Analyse d'Images\n",
        "\n",
        "- Segmentation d'image\n",
        "- Regroupement de pixels similaires\n",
        "- Compression d'image\n",
        "\n",
        "## R√©duction de Dimension\n",
        "\n",
        "### Pourquoi r√©duire la dimension ?\n",
        "\n",
        "- Visualisation de donn√©es multidimensionnelles\n",
        "- R√©duction du bruit\n",
        "- Acc√©l√©ration des algorithmes\n",
        "- √âviter le \"fl√©au de la dimension\"\n",
        "\n",
        "### PCA (Principal Component Analysis)\n",
        "\n",
        "**PCA** transforme les donn√©es en composantes orthogonales capturant la variance maximale."
      ],
      "id": "21dc943e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Donn√©es de d√©monstration\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(100, 5)  # 100 √©chantillons, 5 features\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(f\"Variance expliqu√©e: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "\n",
        "# Visualisation\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.xlabel('Premi√®re composante principale')\n",
        "plt.ylabel('Deuxi√®me composante principale')\n",
        "plt.title('PCA - Visualisation 2D')\n",
        "plt.show()"
      ],
      "id": "98805932",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "M√©thode non lin√©aire particuli√®rement efficace pour la visualisation."
      ],
      "id": "689d7c46"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
        "plt.xlabel('t-SNE 1')\n",
        "plt.ylabel('t-SNE 2')\n",
        "plt.title('t-SNE - Visualisation 2D')\n",
        "plt.show()"
      ],
      "id": "33fb2f14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercices de R√©flexion\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Question 1\n",
        "Pour chacun des sc√©narios suivants, proposez une m√©thode de clustering adapt√©e et justifiez votre choix :\n",
        "\n",
        "a) Segmentation de clients avec des variables d√©mographiques et comportementales\n",
        "b) D√©tection de fraudes dans des transactions bancaires\n",
        "c) Regroupement de documents textuels\n",
        "d) Analyse de pixels d'une image satellite\n",
        ":::\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Correction Question 1\n",
        "\n",
        "**a) Segmentation de clients:**\n",
        "\n",
        "- **M√©thode recommand√©e**: **k-means**\n",
        "- **Justification**:\n",
        "  - Variables d√©mographiques et comportementales ‚Üí donn√©es num√©riques\n",
        "  - Nombre de segments g√©n√©ralement connu √† l'avance (ex: 3-5 segments)\n",
        "  - Besoin d'interpr√©tabilit√© pour le marketing\n",
        "  - Rapide et efficace sur grands volumes de clients"
      ],
      "id": "3f6d6d27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Exemple de segmentation client\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Pr√©paration\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(client_features)\n",
        "\n",
        "# K-means avec 4 segments\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "segments = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Profilage des segments\n",
        "profiles = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "profiles['segment'] = segments\n",
        "print(profiles.groupby('segment').mean())"
      ],
      "id": "7d6e5698",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**b) D√©tection de fraudes:**\n",
        "\n",
        "- **M√©thode recommand√©e**: **DBSCAN** ou **Isolation Forest**\n",
        "- **Justification**:\n",
        "\n",
        "  - Fraudes = anomalies (outliers)\n",
        "  - DBSCAN identifie les points de bruit (label -1)\n",
        "  - Pas besoin de conna√Ætre le nombre de types de fraude\n",
        "  - D√©tecte des patterns de fraude de formes vari√©es"
      ],
      "id": "9355c849"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# DBSCAN pour d√©tecter les anomalies\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(transactions_features)\n",
        "\n",
        "# Points anormaux (potentielles fraudes)\n",
        "anomalies = transactions_features[labels == -1]\n",
        "print(f\"Nombre de transactions suspectes: {len(anomalies)}\")"
      ],
      "id": "8b489de7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**c) Regroupement de documents textuels:**\n",
        "\n",
        "- **M√©thode recommand√©e**: **k-means** sur TF-IDF + **Hierarchical Clustering**\n",
        "- **Justification**:\n",
        "\n",
        "  - TF-IDF transforme texte en vecteurs num√©riques\n",
        "  - K-means efficace en haute dimension (nombreux mots)\n",
        "  - Hierarchical permet d'explorer la hi√©rarchie des th√®mes\n",
        "  - Peut combiner avec topic modeling (LDA)"
      ],
      "id": "4fbbeacc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Vectorisation des textes\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "doc_clusters = kmeans.fit_predict(X_tfidf)\n",
        "\n",
        "# Top mots par cluster\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i in range(5):\n",
        "    center = kmeans.cluster_centers_[i]\n",
        "    top_terms = [terms[j] for j in center.argsort()[-10:]]\n",
        "    print(f\"Cluster {i}: {', '.join(top_terms)}\")"
      ],
      "id": "a712b5c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**d) Analyse de pixels d'image satellite:**\n",
        "\n",
        "- **M√©thode recommand√©e**: **k-means** ou **Mean Shift**\n",
        "- **Justification**:\n",
        "\n",
        "  - Segmentation d'image = clustering de pixels (RGB ou multi-spectral)\n",
        "  - K-means rapide pour millions de pixels\n",
        "  - Mean Shift d√©tecte automatiquement le nombre de segments\n",
        "  - Peut identifier zones (for√™t, eau, ville, etc.)"
      ],
      "id": "c40593d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Image satellite (exemple)\n",
        "# image shape: (height, width, channels)\n",
        "pixels = image.reshape(-1, image.shape[2])  # Reshape en (n_pixels, channels)\n",
        "\n",
        "# K-means sur pixels\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(pixels)\n",
        "\n",
        "# Reconstruction de l'image segment√©e\n",
        "segmented_image = labels.reshape(image.shape[:2])\n",
        "plt.imshow(segmented_image, cmap='tab10')\n",
        "plt.title('Segmentation de l\\'image satellite')\n",
        "plt.show()"
      ],
      "id": "fb365bd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Question 2\n",
        "Pour un dataset avec 10 000 √©chantillons et 50 features :\n",
        "\n",
        "a) Expliquez comment d√©terminer le nombre optimal de clusters pour k-means\n",
        "b) Proposez une approche pour visualiser la structure des clusters\n",
        "c) Quel avantage PCA peut-il apporter avant le clustering ?\n",
        ":::\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Correction Question 2\n",
        "\n",
        "**a) D√©terminer le nombre optimal de clusters:**\n",
        "\n",
        "**M√©thode 1: Elbow Method (M√©thode du coude)**"
      ],
      "id": "4cf02cea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tester diff√©rentes valeurs de k\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    \n",
        "    inertias.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
        "\n",
        "# Visualisation\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Courbe du coude\n",
        "ax1.plot(K_range, inertias, 'bo-')\n",
        "ax1.set_xlabel('Nombre de clusters (k)')\n",
        "ax1.set_ylabel('Inertie')\n",
        "ax1.set_title('M√©thode du Coude')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Silhouette score\n",
        "ax2.plot(K_range, silhouette_scores, 'go-')\n",
        "ax2.set_xlabel('Nombre de clusters (k)')\n",
        "ax2.set_ylabel('Silhouette Score')\n",
        "ax2.set_title('Score Silhouette')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Le k optimal est au \"coude\" de la courbe d'inertie\n",
        "# ET avec un bon silhouette score"
      ],
      "id": "41c3c47e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**M√©thode 2: Gap Statistic**"
      ],
      "id": "0ed1e4d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Compare l'inertie observ√©e vs inertie sur donn√©es al√©atoires\n",
        "def gap_statistic(X, k_max=10, n_refs=10):\n",
        "    gaps = []\n",
        "    for k in range(1, k_max + 1):\n",
        "        # Inertie sur donn√©es r√©elles\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(X)\n",
        "        real_inertia = kmeans.inertia_\n",
        "        \n",
        "        # Inertie moyenne sur donn√©es de r√©f√©rence\n",
        "        ref_inertias = []\n",
        "        for _ in range(n_refs):\n",
        "            X_ref = np.random.uniform(X.min(), X.max(), X.shape)\n",
        "            kmeans_ref = KMeans(n_clusters=k, random_state=42)\n",
        "            kmeans_ref.fit(X_ref)\n",
        "            ref_inertias.append(kmeans_ref.inertia_)\n",
        "        \n",
        "        gap = np.log(np.mean(ref_inertias)) - np.log(real_inertia)\n",
        "        gaps.append(gap)\n",
        "    \n",
        "    return gaps\n",
        "\n",
        "# K optimal = premier k o√π gap commence √† d√©cro√Ætre"
      ],
      "id": "35a6f3ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**M√©thode 3: Silhouette Analysis d√©taill√©e**"
      ],
      "id": "1d3c67d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "for k in [2, 3, 4, 5]:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    \n",
        "    silhouette_vals = silhouette_samples(X, labels)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    y_lower = 10\n",
        "    \n",
        "    for i in range(k):\n",
        "        cluster_silhouette_vals = silhouette_vals[labels == i]\n",
        "        cluster_silhouette_vals.sort()\n",
        "        \n",
        "        size = cluster_silhouette_vals.shape[0]\n",
        "        y_upper = y_lower + size\n",
        "        \n",
        "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                         0, cluster_silhouette_vals,\n",
        "                         alpha=0.7)\n",
        "        y_lower = y_upper + 10\n",
        "    \n",
        "    plt.title(f'Silhouette Plot (k={k})')\n",
        "    plt.xlabel('Coefficient Silhouette')\n",
        "    plt.ylabel('Cluster')\n",
        "    plt.axvline(x=silhouette_score(X, labels), color=\"red\", linestyle=\"--\")\n",
        "    plt.show()"
      ],
      "id": "f8f881bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**b) Visualiser la structure des clusters:**\n",
        "\n",
        "**Approche 1: PCA pour r√©duction 2D/3D**"
      ],
      "id": "379c06f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# R√©duction √† 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
        "                     c=labels, cmap='viridis', alpha=0.6)\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
        "plt.title('Clusters visualis√©s avec PCA')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Variance expliqu√©e totale: {sum(pca.explained_variance_ratio_):.2%}\")"
      ],
      "id": "d6ec0e2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Approche 2: t-SNE pour visualisation non-lin√©aire**"
      ],
      "id": "322ad2b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# t-SNE (plus lent mais meilleure visualisation)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\n",
        "plt.title('Clusters visualis√©s avec t-SNE')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "id": "10739cbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Approche 3: Pairplot des features importantes**"
      ],
      "id": "3e12300a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# S√©lectionner top features par variance\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.5)\n",
        "X_selected = selector.fit_transform(X)\n",
        "\n",
        "# Pairplot avec 4-5 features les plus variables\n",
        "df_plot = pd.DataFrame(X_selected[:, :5], columns=[f'F{i}' for i in range(5)])\n",
        "df_plot['cluster'] = labels\n",
        "\n",
        "sns.pairplot(df_plot, hue='cluster', palette='tab10')\n",
        "plt.show()"
      ],
      "id": "9373afb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**c) Avantages de PCA avant le clustering:**\n",
        "\n",
        "**1. R√©duction de dimension ‚Üí Efficacit√© computationnelle**"
      ],
      "id": "f5f11aba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Sans PCA: 50 features\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "kmeans_full = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_full.fit(X)  # X: (10000, 50)\n",
        "time_full = time.time() - start\n",
        "\n",
        "# Avec PCA: 10 features (gardant 95% de variance)\n",
        "pca = PCA(n_components=0.95)  # Garde 95% de variance\n",
        "X_pca = pca.fit_transform(X)  # X_pca: (10000, ~10)\n",
        "\n",
        "start = time.time()\n",
        "kmeans_pca = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_pca.fit(X_pca)\n",
        "time_pca = time.time() - start\n",
        "\n",
        "print(f\"Temps sans PCA: {time_full:.2f}s\")\n",
        "print(f\"Temps avec PCA: {time_pca:.2f}s\")\n",
        "print(f\"Acc√©l√©ration: {time_full/time_pca:.1f}x\")\n",
        "print(f\"Dimensions r√©duites: {X.shape[1]} ‚Üí {X_pca.shape[1]}\")"
      ],
      "id": "04e076c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. R√©duction du bruit**"
      ],
      "id": "848af8ee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# PCA √©limine les composantes de faible variance (souvent du bruit)\n",
        "pca_full = PCA()\n",
        "pca_full.fit(X)\n",
        "\n",
        "# Afficher la variance par composante\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
        "         pca_full.explained_variance_ratio_, 'bo-')\n",
        "plt.xlabel('Composante')\n",
        "plt.ylabel('Variance expliqu√©e')\n",
        "plt.title('Scree Plot')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
        "         np.cumsum(pca_full.explained_variance_ratio_), 'ro-')\n",
        "plt.xlabel('Nombre de composantes')\n",
        "plt.ylabel('Variance cumul√©e')\n",
        "plt.axhline(y=0.95, color='g', linestyle='--', label='95%')\n",
        "plt.title('Variance Cumul√©e')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Garder les composantes qui expliquent 95% de la variance\n",
        "# ‚Üí √©limine le bruit des derni√®res composantes"
      ],
      "id": "b2dcfa71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3. √âvite la mal√©diction de la dimensionnalit√©**"
      ],
      "id": "e1fdd029"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# En haute dimension, les distances deviennent moins significatives\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# Calcul des distances moyennes\n",
        "distances_full = pdist(X[:100])  # Sur 100 √©chantillons pour rapidit√©\n",
        "distances_pca = pdist(X_pca[:100])\n",
        "\n",
        "print(f\"Distance moyenne (50D): {np.mean(distances_full):.2f}\")\n",
        "print(f\"Distance moyenne (10D): {np.mean(distances_pca):.2f}\")\n",
        "print(f\"√âcart-type distances (50D): {np.std(distances_full):.2f}\")\n",
        "print(f\"√âcart-type distances (10D): {np.std(distances_pca):.2f}\")\n",
        "\n",
        "# En dimension r√©duite, les distances sont plus discriminantes"
      ],
      "id": "c5914687",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4. D√©corr√©lation des features**"
      ],
      "id": "7281b80c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# PCA produit des composantes non-corr√©l√©es\n",
        "# ‚Üí Am√©liore k-means qui suppose ind√©pendance\n",
        "\n",
        "# Corr√©lation avant PCA\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(np.corrcoef(X.T), cmap='coolwarm', center=0,\n",
        "            cbar_kws={'label': 'Corr√©lation'})\n",
        "plt.title('Corr√©lations avant PCA')\n",
        "\n",
        "# Corr√©lation apr√®s PCA\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(np.corrcoef(X_pca.T), cmap='coolwarm', center=0,\n",
        "            cbar_kws={'label': 'Corr√©lation'})\n",
        "plt.title('Corr√©lations apr√®s PCA')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Apr√®s PCA: corr√©lations nulles entre composantes"
      ],
      "id": "f0798b6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**R√©sum√© des avantages:**\n",
        "\n",
        "| Avantage | Description | Impact |\n",
        "|----------|-------------|--------|\n",
        "| **Efficacit√©** | 50 ‚Üí 10 dimensions | 5-10x plus rapide |\n",
        "| **D√©bruitage** | √âlimine variance faible | Clusters plus nets |\n",
        "| **Distances** | Plus discriminantes en faible dim | Meilleur clustering |\n",
        "| **D√©corr√©lation** | Features ind√©pendantes | K-means plus efficace |\n",
        "| **Visualisation** | R√©duction √† 2-3D | Interpr√©tation facile |\n",
        ":::\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Question 3\n",
        "Impl√©mentez un pipeline complet de clustering sur le dataset Iris :\n",
        "\n",
        "1. Chargez les donn√©es (ignorer les labels pour l'apprentissage non supervis√©)\n",
        "2. Appliquez PCA pour r√©duire √† 2 dimensions\n",
        "3. Testez k-means avec k=2,3,4 et comparez les r√©sultats\n",
        "4. Visualisez les clusters obtenus\n",
        "5. Calculez le silhouette score pour chaque k\n",
        ":::\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Correction Question 3"
      ],
      "id": "0e17c588"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "\n",
        "# 1. Chargement des donn√©es (SANS utiliser les labels pour clustering)\n",
        "print(\"=\" * 70)\n",
        "print(\"PIPELINE COMPLET DE CLUSTERING - DATASET IRIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features seulement (ignorer iris.target)\n",
        "feature_names = iris.feature_names\n",
        "true_labels = iris.target  # Gard√© seulement pour √©valuation finale\n",
        "\n",
        "print(f\"\\n1. Chargement des donn√©es:\")\n",
        "print(f\"   Dimensions: {X.shape}\")\n",
        "print(f\"   Features: {feature_names}\")\n",
        "\n",
        "# Normalisation (important avant PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(f\"   ‚úì Donn√©es normalis√©es\")\n",
        "\n",
        "# 2. Application de PCA pour r√©duction √† 2D\n",
        "print(f\"\\n2. R√©duction de dimension avec PCA:\")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"   Variance expliqu√©e par composante: {pca.explained_variance_ratio_}\")\n",
        "print(f\"   Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\n",
        "print(f\"   Dimensions: {X.shape[1]}D ‚Üí {X_pca.shape[1]}D\")\n",
        "\n",
        "# Visualisation des donn√©es apr√®s PCA (sans clustering)\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
        "                     c=true_labels, cmap='viridis', \n",
        "                     alpha=0.6, edgecolors='w')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "plt.title('Dataset Iris apr√®s PCA (color√© par vraies classes)')\n",
        "plt.colorbar(scatter, label='Vraie classe')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Test de k-means avec k=2,3,4\n",
        "print(f\"\\n3. Clustering k-means avec diff√©rentes valeurs de k:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "K_values = [2, 3, 4]\n",
        "results = []\n",
        "\n",
        "for k in K_values:\n",
        "    # Clustering\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_pca)\n",
        "    \n",
        "    # M√©triques\n",
        "    inertia = kmeans.inertia_\n",
        "    silhouette = silhouette_score(X_pca, labels)\n",
        "    \n",
        "    # Comparaison avec vraies classes (juste pour curiosit√©)\n",
        "    ari = adjusted_rand_score(true_labels, labels)\n",
        "    \n",
        "    results.append({\n",
        "        'k': k,\n",
        "        'Inertie': inertia,\n",
        "        'Silhouette': silhouette,\n",
        "        'ARI (vs vrai)': ari,\n",
        "        'labels': labels,\n",
        "        'centroids': kmeans.cluster_centers_\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nk = {k}:\")\n",
        "    print(f\"   Inertie: {inertia:.2f}\")\n",
        "    print(f\"   Silhouette Score: {silhouette:.3f}\")\n",
        "    print(f\"   Taille des clusters: {np.bincount(labels)}\")\n",
        "    print(f\"   ARI (comparaison avec vraies classes): {ari:.3f}\")\n",
        "\n",
        "# DataFrame des r√©sultats\n",
        "df_results = pd.DataFrame([{k: v for k, v in r.items() if k not in ['labels', 'centroids']} \n",
        "                          for r in results])\n",
        "print(f\"\\nüìä Tableau r√©capitulatif:\")\n",
        "print(df_results.to_string(index=False))\n",
        "\n",
        "# Meilleur k selon silhouette\n",
        "best_k = df_results.loc[df_results['Silhouette'].idxmax(), 'k']\n",
        "print(f\"\\n‚≠ê Meilleur k selon Silhouette Score: {best_k}\")\n",
        "\n",
        "# 4. Visualisation des clusters pour chaque k\n",
        "print(f\"\\n4. Visualisation des clusters:\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, result in enumerate(results):\n",
        "    k = result['k']\n",
        "    labels = result['labels']\n",
        "    centroids = result['centroids']\n",
        "    ax = axes[idx]\n",
        "    # Scatter plot des points\n",
        "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], \n",
        "                        c=labels, cmap='tab10', \n",
        "                        alpha=0.6, edgecolors='w', s=50)\n",
        "    \n",
        "    # Centro√Ødes\n",
        "    ax.scatter(centroids[:, 0], centroids[:, 1], \n",
        "              c='red', marker='X', s=200, \n",
        "              edgecolors='black', linewidths=2,\n",
        "              label='Centro√Ødes')\n",
        "    \n",
        "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
        "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
        "    ax.set_title(f'k={k} (Silhouette={result[\"Silhouette\"]:.3f})')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Colorbar\n",
        "    plt.colorbar(scatter, ax=ax, label='Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Analyse approfondie du meilleur k\n",
        "print(f\"\\n5. Analyse d√©taill√©e pour k={best_k}:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "best_result = [r for r in results if r['k'] == best_k][0]\n",
        "best_labels = best_result['labels']\n",
        "\n",
        "# Profilage des clusters\n",
        "print(f\"\\nProfilage des clusters (features originales):\")\n",
        "\n",
        "df_analysis = pd.DataFrame(X, columns=feature_names)\n",
        "df_analysis['Cluster'] = best_labels\n",
        "\n",
        "cluster_profiles = df_analysis.groupby('Cluster').agg(['mean', 'std'])\n",
        "print(cluster_profiles)\n",
        "\n",
        "# Heatmap des caract√©ristiques par cluster\n",
        "cluster_means = df_analysis.groupby('Cluster').mean()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(cluster_means.T, annot=True, fmt='.2f', cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'Valeur moyenne'})\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Feature')\n",
        "plt.title(f'Profil des {best_k} clusters (valeurs moyennes)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comparaison avec les vraies classes (curiosit√© acad√©mique)\n",
        "print(f\"\\nüìà Comparaison avec les vraies esp√®ces d'Iris:\")\n",
        "print(\"(Note: Le clustering est NON SUPERVIS√â, cette comparaison est\")\n",
        "print(\" juste pour comprendre ce que l'algorithme a trouv√©)\")\n",
        "\n",
        "confusion_unsupervised = pd.crosstab(\n",
        "    pd.Series(true_labels, name='Vraie esp√®ce'),\n",
        "    pd.Series(best_labels, name='Cluster trouv√©')\n",
        ")\n",
        "print(confusion_unsupervised)\n",
        "\n",
        "# Conclusion\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"CONCLUSION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚úì PCA a r√©duit les donn√©es de 4D √† 2D\")\n",
        "print(f\"‚úì {sum(pca.explained_variance_ratio_):.1%} de variance pr√©serv√©e\")\n",
        "print(f\"‚úì K optimal selon silhouette: {best_k}\")\n",
        "print(f\"‚úì Silhouette score: {best_result['Silhouette']:.3f}\")\n",
        "print(f\"‚úì Les clusters correspondent {'assez bien' if best_result['ARI (vs vrai)'] > 0.7 else 'partiellement'} aux vraies esp√®ces\")\n",
        "print(f\"  (ARI = {best_result['ARI (vs vrai)']:.3f})\")"
      ],
      "id": "e63f1de5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**R√©sultat attendu:**\n",
        "\n",
        "Le pipeline devrait r√©v√©ler que:\n",
        "\n",
        "- **k=3** est optimal (correspond aux 3 esp√®ces d'Iris)\n",
        "- Le silhouette score sera autour de 0.5-0.6\n",
        "- PCA capture environ 95% de la variance en 2D\n",
        "- Les clusters trouv√©s correspondent assez bien aux vraies esp√®ces\n",
        "- Une esp√®ce (Setosa) sera bien s√©par√©e, les deux autres se chevaucheront un peu\n",
        ":::\n",
        "\n",
        "## R√©sum√© de la S√©ance\n",
        "\n",
        "::: {.callout-important icon=false}\n",
        "## Points cl√©s √† retenir\n",
        "\n",
        "1. **Apprentissage non supervis√©** = d√©couvrir des patterns dans des donn√©es non √©tiquet√©es\n",
        "2. **Clustering** = regrouper des donn√©es similaires (k-means, DBSCAN, hi√©rarchique)\n",
        "3. **Mesures de qualit√©** : silhouette score, inertie, Davies-Bouldin\n",
        "4. **R√©duction de dimension** : PCA (lin√©aire), t-SNE (non-lin√©aire)\n",
        "5. **Applications** : segmentation client, analyse de documents, traitement d'image\n",
        "6. **D√©fis** : choix du nombre de clusters, qualit√© sans v√©rit√© terrain\n",
        ":::\n",
        "\n",
        "## Lectures Compl√©mentaires\n",
        "\n",
        "1. G√©ron, A. (2019) - Chapitre 9: Unsupervised Learning Techniques\n",
        "2. [Scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html)\n",
        "3. [Visualizing Data using t-SNE](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)</parameter>"
      ],
      "id": "e9777bc5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "test_env",
      "language": "python",
      "display_name": "Python (test_env)",
      "path": "C:\\Users\\abdal\\AppData\\Roaming\\jupyter\\kernels\\test_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}