{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Séance 2: Apprentissage Supervisé - Classification\n",
        "\n",
        "::: {.callout-note icon=false}\n",
        "## Informations de la séance\n",
        "- **Type**: Cours\n",
        "- **Durée**: 2h\n",
        "- **Objectifs**: Obj5, Obj6\n",
        ":::\n",
        "\n",
        "## 1. Introduction à la Classification\n",
        "\n",
        "### 1.1 Définition\n",
        "\n",
        "La **classification** est une tâche d'apprentissage supervisé où l'objectif est de prédire une **classe** ou **catégorie** discrète à partir de caractéristiques d'entrée.\n",
        "\n",
        "::: {.callout-note}\n",
        "## Exemple\n",
        "**Entrée**: Caractéristiques d'un email (mots, expéditeur, longueur, etc.)  \n",
        "**Sortie**: Classe = \"Spam\" ou \"Non Spam\"\n",
        ":::\n",
        "\n",
        "### 1.2 Différence Classification vs Régression\n",
        "\n",
        "| Caractéristique | Classification | Régression |\n",
        "|----------------|----------------|------------|\n",
        "| **Sortie** | Catégorie discrète | Valeur continue |\n",
        "| **Exemple** | Spam/Non spam | Prix d'une maison |\n",
        "| **Métrique** | Accuracy, F1-score | MAE, RMSE |\n",
        "| **Fonction** | Probabilité → Classe | Valeur numérique |\n",
        "\n",
        "## 2. Types de Classification\n",
        "\n",
        "### 2.1 Classification Binaire\n",
        "\n",
        "Deux classes possibles: 0 ou 1, Vrai ou Faux, Positif ou Négatif\n",
        "\n",
        "**Exemples**:\n",
        "\n",
        "- Détection de spam (spam/non spam)\n",
        "- Diagnostic médical (malade/sain)\n",
        "- Détection de fraude (fraude/légitime)\n",
        "- Approbation de crédit (approuvé/rejeté)"
      ],
      "id": "ffdb7ae8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Exemple: Classification binaire\n",
        "y_binary = [0, 1, 1, 0, 1, 0, 0, 1]  # 0 = négatif, 1 = positif"
      ],
      "id": "cf643cb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Classification Multi-classes\n",
        "\n",
        "Plus de deux classes **mutuellement exclusives** (une seule classe par instance)\n",
        "\n",
        "**Exemples**:\n",
        "\n",
        "- Reconnaissance de chiffres manuscrits (0-9 = 10 classes)\n",
        "- Classification de fleurs Iris (Setosa, Versicolor, Virginica)\n",
        "- Catégorisation d'articles (Sport, Politique, Économie, Culture)"
      ],
      "id": "598b71c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Exemple: Classification multi-classes\n",
        "y_multiclass = [0, 1, 2, 1, 0, 2, 1]  # 3 classes: 0, 1, 2"
      ],
      "id": "55613cd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Classification Multi-label\n",
        "\n",
        "Plusieurs classes **simultanées** possibles pour une instance\n",
        "\n",
        "**Exemples**:\n",
        "\n",
        "- Étiquetage de photos (peut contenir: personne, chien, extérieur)\n",
        "- Catégorisation de films (peut être: Action, Comédie, Drame)\n",
        "- Analyse de sentiments multiple (joie + surprise)"
      ],
      "id": "c5f3e51b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Exemple: Classification multi-label\n",
        "y_multilabel = [\n",
        "    [1, 0, 1],  # instance a les labels 0 et 2\n",
        "    [0, 1, 1],  # instance a les labels 1 et 2\n",
        "    [1, 1, 0]   # instance a les labels 0 et 1\n",
        "]"
      ],
      "id": "139388f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Algorithmes de Classification\n",
        "\n",
        "### 3.1 Arbre de Décision (Decision Tree)\n",
        "\n",
        "Modèle qui prend des décisions basées sur des questions successives.\n",
        "\n",
        "#### Principe\n",
        "\n",
        "L'arbre divise l'espace des caractéristiques en régions par des questions binaires.\n",
        "\n",
        "\n",
        "**Diagramme mermaid (conversion échouée):**\n",
        "```{mermaid}\n",
        "graph TD\n",
        "    A[Age > 30?] -->|Oui| B[Revenu > 50k?]\n",
        "    A -->|Non| C[Étudiant?]\n",
        "    B -->|Oui| D[Approuvé ]\n",
        "    B -->|Non| E[Rejeté ]\n",
        "    C -->|Oui| F[Rejeté ]\n",
        "    C -->|Non| G[Approuvé ]\n",
        "```\n",
        "\n",
        "\n",
        "#### Avantages\n",
        "- Facile à interpréter et visualiser\n",
        "- Pas besoin de normalisation des données\n",
        "- Gère les données non linéaires\n",
        "- Gère les variables catégorielles et numériques\n",
        "\n",
        "#### Inconvénients\n",
        "- Tendance à l'overfitting\n",
        "- Instable (petits changements de données → arbre différent)\n",
        "- Biais vers les classes majoritaires"
      ],
      "id": "09c8d48e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Chargement des données\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Entraînement\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Prédiction\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy: {clf.score(X_test, y_test):.2f}\")"
      ],
      "id": "04bb4c87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Random Forest\n",
        "Ensemble d'arbres de décision qui votent ensemble.\n",
        "\n",
        "#### Principe\n",
        "1. Créer N arbres sur des sous-ensembles aléatoires de données\n",
        "2. Chaque arbre vote pour une classe\n",
        "3. Prédiction finale = vote majoritaire\n",
        "\n",
        "#### Avantages\n",
        "- Très performant et robuste\n",
        "- Réduit l'overfitting par rapport à un arbre unique\n",
        "- Gère bien les grandes dimensions\n",
        "- Donne l'importance des features\n",
        "\n",
        "#### Inconvénients\n",
        "- Moins interprétable qu'un arbre unique\n",
        "- Plus lent à entraîner et prédire\n",
        "- Mémoire importante"
      ],
      "id": "23f4cb2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Entraînement\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Prédiction\n",
        "y_pred = rf.predict(X_test)\n",
        "print(f\"Accuracy: {rf.score(X_test, y_test):.2f}\")\n",
        "\n",
        "# Importance des features\n",
        "importances = rf.feature_importances_\n",
        "for i, imp in enumerate(importances):\n",
        "    print(f\"Feature {iris.feature_names[i]}: {imp:.3f}\")"
      ],
      "id": "b5cf4fd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Support Vector Machine (SVM)\n",
        "\n",
        "Trouve l'hyperplan optimal qui sépare les classes avec la marge maximale.\n",
        "\n",
        "#### Principe\n",
        "\n",
        "- **Marge**: distance entre l'hyperplan et les points les plus proches (vecteurs de support)\n",
        "- **Objectif**: Maximiser cette marge\n",
        "- **Kernel trick**: Permet de gérer des données non linéairement séparables"
      ],
      "id": "7ca0f608"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# SVM linéaire\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# SVM avec kernel RBF (pour données non linéaires)\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"SVM Linear Accuracy: {svm_linear.score(X_test, y_test):.2f}\")\n",
        "print(f\"SVM RBF Accuracy: {svm_rbf.score(X_test, y_test):.2f}\")"
      ],
      "id": "7986a2d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avantages\n",
        "- Très efficace en haute dimension\n",
        "- Robuste aux outliers\n",
        "- Versatile (différents kernels)\n",
        "\n",
        "#### Inconvénients\n",
        "- Lent sur de grandes données\n",
        "- Difficile à interpréter\n",
        "- Sensible au choix des hyperparamètres\n",
        "\n",
        "### 3.4 Naïve Bayes\n",
        "\n",
        "Basé sur le théorème de Bayes avec hypothèse d'indépendance des features.\n",
        "\n",
        "#### Principe - Théorème de Bayes\n",
        "\n",
        "$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n",
        "\n",
        "Où:\n",
        "\n",
        "- $P(y|X)$ = probabilité de la classe $y$ sachant les features $X$ (posterior)\n",
        "- $P(X|y)$ = vraisemblance\n",
        "- $P(y)$ = probabilité a priori de la classe\n",
        "- $P(X)$ = évidence (constante)\n",
        "\n",
        "#### Hypothèse \"Naïve\"\n",
        "\n",
        "Les features sont **indépendantes** conditionnellement à la classe:\n",
        "\n",
        "$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y)$$"
      ],
      "id": "cd5c2d8f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "\n",
        "# Gaussian Naive Bayes (pour features continues)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Gaussian NB Accuracy: {gnb.score(X_test, y_test):.2f}\")\n",
        "\n",
        "# Multinomial NB (pour comptages, ex: mots dans un texte)\n",
        "# mnb = MultinomialNB()\n",
        "# mnb.fit(X_train_counts, y_train)"
      ],
      "id": "e3fa6250",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avantages\n",
        "- Très rapide (entraînement et prédiction)\n",
        "- Fonctionne bien avec peu de données\n",
        "- Excellent pour la classification de texte\n",
        "\n",
        "#### Inconvénients\n",
        "- Hypothèse d'indépendance rarement vraie\n",
        "- Performance limitée si hypothèse violée\n",
        "\n",
        "### 3.5 Régression Logistique\n",
        "\n",
        "**Attention**: Malgré son nom, c'est un algorithme de **classification** !\n",
        "\n",
        "#### Principe\n",
        "\n",
        "Modèle linéaire qui utilise la fonction sigmoïde pour produire des probabilités.\n",
        "\n",
        "**Fonction sigmoïde**:\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "**Modèle**:\n",
        "$$P(y=1|X) = \\sigma(w^T X + b) = \\frac{1}{1 + e^{-(w^T X + b)}}$$\n",
        "\n",
        "#### Interprétation Probabiliste\n",
        "\n",
        "- Sortie $\\in [0, 1]$ : probabilité d'appartenance à la classe positive\n",
        "- Si $P(y=1|X) \\geq 0.5$ → prédiction = classe 1\n",
        "- Si $P(y=1|X) < 0.5$ → prédiction = classe 0"
      ],
      "id": "f1b6da67"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Entraînement\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Prédiction de classes\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Prédiction de probabilités\n",
        "y_proba = log_reg.predict_proba(X_test)\n",
        "\n",
        "print(f\"Accuracy: {log_reg.score(X_test, y_test):.2f}\")\n",
        "print(f\"\\nPremière prédiction:\")\n",
        "print(f\"  Probabilités: {y_proba[0]}\")\n",
        "print(f\"  Classe prédite: {y_pred[0]}\")"
      ],
      "id": "31a1703b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avantages\n",
        "- Simple et interprétable\n",
        "- Donne des probabilités (utile pour la prise de décision)\n",
        "- Peu de paramètres à ajuster\n",
        "- Fonctionne bien sur données linéairement séparables\n",
        "\n",
        "#### Inconvénients\n",
        "- Assume une relation linéaire\n",
        "- Sensible aux outliers\n",
        "- Nécessite feature engineering pour les relations non linéaires\n",
        "\n",
        "### 3.6 k-Nearest Neighbors (k-NN)\n",
        "\n",
        "Classification basée sur la proximité avec les voisins.\n",
        "\n",
        "#### Principe\n",
        "1. Calculer la distance entre le nouveau point et tous les points d'entraînement\n",
        "2. Sélectionner les k points les plus proches\n",
        "3. Vote majoritaire parmi ces k voisins"
      ],
      "id": "4a086693"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# k=5 voisins\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "print(f\"k-NN Accuracy: {knn.score(X_test, y_test):.2f}\")"
      ],
      "id": "5879827f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avantages\n",
        "- Simple et intuitif\n",
        "- Pas d'entraînement (lazy learning)\n",
        "- Fonctionne bien pour des frontières complexes\n",
        "\n",
        "#### Inconvénients\n",
        "- Lent pour la prédiction (calcule toutes les distances)\n",
        "- Sensible à l'échelle des features (nécessite normalisation)\n",
        "- Curse of dimensionality (mauvais en haute dimension)\n",
        "\n",
        "## 4. Critères d'Évaluation (Aperçu)\n",
        "\n",
        "### 4.1 Métriques Principales\n",
        "\n",
        "- **Accuracy**: Proportion de prédictions correctes\n",
        "- **Precision**: Proportion de vrais positifs parmi les prédictions positives\n",
        "- **Recall**: Proportion de vrais positifs parmi les cas réellement positifs\n",
        "- **F1-Score**: Moyenne harmonique de Precision et Recall\n",
        "\n",
        "::: {.callout-important}\n",
        "Ces métriques seront détaillées en profondeur dans la **Séance 5 - TD2**\n",
        ":::\n",
        "\n",
        "### 4.2 Exemple Simple"
      ],
      "id": "272a31c8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calcul de l'accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Rapport complet\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "id": "c21d568b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Exemple Complet: Comparaison d'Algorithmes"
      ],
      "id": "8bf9d566"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Création d'un dataset synthétique\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, \n",
        "    n_features=2, \n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split et normalisation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Entraînement de plusieurs modèles\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "models = {\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'k-NN': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Comparaison des performances\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    score = model.score(X_test_scaled, y_test)\n",
        "    results[name] = score\n",
        "    print(f\"{name:20s}: {score:.4f}\")\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(list(results.keys()), list(results.values()))\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('Comparaison des Algorithmes de Classification')\n",
        "plt.xlim([0, 1])\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fbdf1b8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Résumé de la Séance\n",
        "\n",
        "::: {.callout-important icon=false}\n",
        "## Points clés à retenir\n",
        "\n",
        "1. **Classification** = prédire une catégorie discrète\n",
        "2. **Types**: Binaire, Multi-classes, Multi-label\n",
        "3. **Algorithmes principaux**:\n",
        "   - Decision Tree: interprétable mais tendance à l'overfitting\n",
        "   - Random Forest: robuste et performant\n",
        "   - SVM: excellent en haute dimension\n",
        "   - Naïve Bayes: rapide, bon pour le texte\n",
        "   - Régression Logistique: simple, interprétable, probabiliste\n",
        "   - k-NN: simple mais coûteux en prédiction\n",
        "4. **Choix du modèle** dépend de: taille des données, interprétabilité, performance, ressources\n",
        "5. **Évaluation** avec métriques appropriées (détails en TD2)\n",
        ":::\n",
        "\n",
        "## Exercices\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Exercice 1\n",
        "Implémentez une régression logistique sur le dataset Iris et analysez les coefficients appris. Que représentent-ils?\n",
        ":::\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Exercice 2\n",
        "Comparez les performances de Decision Tree vs Random Forest sur le dataset digits de sklearn. Expliquez les différences observées.\n",
        ":::\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Exercice 3\n",
        "Pour un problème de détection de fraude bancaire, quel algorithme recommanderiez-vous et pourquoi? Considérez les aspects: interprétabilité, temps réel, déséquilibre des classes.\n",
        ":::\n",
        "\n",
        "## Lectures Complémentaires\n",
        "\n",
        "1. Géron, A. (2019) - Chapitre 3: Classification\n",
        "2. Scikit-learn Documentation: [Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html)\n",
        "3. [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)"
      ],
      "id": "e57f31a2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "test_env",
      "language": "python",
      "display_name": "Python (test_env)",
      "path": "C:\\Users\\abdal\\AppData\\Roaming\\jupyter\\kernels\\test_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}