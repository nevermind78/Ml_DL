[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning et Deep Learning",
    "section": "",
    "text": "Pr√©sentation du Cours",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "index.html#informations-g√©n√©rales",
    "href": "index.html#informations-g√©n√©rales",
    "title": "Machine Learning et Deep Learning",
    "section": "Informations G√©n√©rales",
    "text": "Informations G√©n√©rales\n\n\n\n\n\n\nNoteD√©tails du cours\n\n\n\n\nIntitul√©: Machine Learning et Deep Learning\nD√©partement: G√©nie Informatique\nNiveau: 4√®me ann√©e DS\nSemestre: S2\nR√©gime: Mixte\nVolume Horaire Total: 42h\n\nCours / TD: 21h\nTP / Projet: 21h",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "index.html#pr√©requis",
    "href": "index.html#pr√©requis",
    "title": "Machine Learning et Deep Learning",
    "section": "Pr√©requis",
    "text": "Pr√©requis\n\nMath√©matiques\n\nAlg√®bre lin√©aire: vecteurs, matrices, op√©rations matricielles\nCalcul diff√©rentiel et int√©gral: d√©riv√©es, gradients, int√©grales\nProbabilit√©s et statistiques: distributions de probabilit√©, estimations de param√®tres, tests d‚Äôhypoth√®ses\n\n\n\nProgrammation\n\nPython: structures de contr√¥le, fonctions, classes\nBiblioth√®ques: NumPy, Pandas, Matplotlib",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "index.html#objectifs-dapprentissage",
    "href": "index.html#objectifs-dapprentissage",
    "title": "Machine Learning et Deep Learning",
    "section": "Objectifs d‚ÄôApprentissage",
    "text": "Objectifs d‚ÄôApprentissage\nAu terme de ce cours, vous serez capable de:\n\nComprendre les concepts fondamentaux du Machine Learning\nIdentifier diff√©rents types de probl√®mes ML et leurs solutions\nAppliquer des techniques de r√©gression et classification\nMa√Ætriser les m√©thodes d‚Äô√©valuation et d‚Äôoptimisation\nComprendre et impl√©menter des r√©seaux de neurones\nUtiliser des techniques avanc√©es (CNN, Transfer Learning, GAN)",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "index.html#structure-du-cours",
    "href": "index.html#structure-du-cours",
    "title": "Machine Learning et Deep Learning",
    "section": "Structure du Cours",
    "text": "Structure du Cours\nLe cours est divis√© en 3 parties principales:\n\nPartie 1: Machine Learning Fondamental (S1-S10)\n\nIntroduction √† l‚ÄôIA et au ML\nApprentissage supervis√© (classification et r√©gression)\nApprentissage non supervis√© (clustering)\n\n\n\nPartie 2: R√©seaux de Neurones & Deep Learning (S11-S20)\n\nR√©seaux de neurones artificiels\nCNN et Transfer Learning\nRNN et s√©ries temporelles\nIA g√©n√©rative et GAN\n\n\n\nPartie 3: Projet & √âvaluation (S21-S24)\n\nMini-projet en groupe\nPr√©sentations et √©valuations",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "index.html#√©valuation",
    "href": "index.html#√©valuation",
    "title": "Machine Learning et Deep Learning",
    "section": "√âvaluation",
    "text": "√âvaluation\n\n\n\n\n\n\n\n\nType\nPourcentage\nD√©tails\n\n\n\n\nMini-Projet\n60%\nRapport (20%) + Code (20%) + Pr√©sentation (20%)\n\n\nDevoir Surveill√©\n40%\nTh√©orie (20%) + Pratique (20%)",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "index.html#ressources",
    "href": "index.html#ressources",
    "title": "Machine Learning et Deep Learning",
    "section": "Ressources",
    "text": "Ressources\n\nOuvrages de r√©f√©rence\n\nG√©ron, A. (2019) - Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\nGoodfellow, Bengio & Courville (2016) - Deep Learning, MIT Press\nChollet, F. (2021) - Deep Learning with Python, 2nd Edition\n\n\n\nOutils\n\nLangages: Python\nBiblioth√®ques: Scikit-learn, TensorFlow, Keras, PyTorch (optionnel)\nEnvironnement: Jupyter Notebook, Google Colab, VS Code",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Machine Learning et Deep Learning",
    "section": "Contact",
    "text": "Contact\n\nEnseignants: Dr.¬†Fatma Sbiaa, Abdallah Khemais\nDirecteur du D√©partement: Mr.¬†Ramzi Mahmoudi\nDirecteur des √âtudes: Pr. Moncef Bouzidi\n\n\n\n\n\n\n\nTipPlateforme d‚Äôapprentissage\n\n\n\nTous les supports de cours sont disponibles sur Moodle/Teams",
    "crumbs": [
      "Pr√©sentation du Cours"
    ]
  },
  {
    "objectID": "seance1.html",
    "href": "seance1.html",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "",
    "text": "1. D√©finitions et Concepts de Base",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#d√©finitions-et-concepts-de-base",
    "href": "seance1.html#d√©finitions-et-concepts-de-base",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "",
    "text": "1.1 Intelligence Artificielle (IA)\nL‚ÄôIntelligence Artificielle est un domaine de l‚Äôinformatique qui vise √† cr√©er des syst√®mes capables d‚Äôeffectuer des t√¢ches n√©cessitant normalement l‚Äôintelligence humaine.\n\n\n\n\n\n\nTipExemples d‚ÄôIA au quotidien\n\n\n\n\nAssistants vocaux (Siri, Alexa, Google Assistant)\nRecommandations Netflix/Spotify\nFiltres anti-spam des emails\nReconnaissance faciale sur smartphones\nTraduction automatique\n\n\n\n\n\n1.2 Machine Learning (Apprentissage Automatique)\nLe Machine Learning est une sous-discipline de l‚ÄôIA qui permet aux ordinateurs d‚Äôapprendre √† partir de donn√©es sans √™tre explicitement programm√©s.\nDiff√©rence cl√©:\n\nProgrammation traditionnelle: Humain √©crit les r√®gles ‚Üí Ordinateur applique\nMachine Learning: Ordinateur apprend les r√®gles √† partir des donn√©es\n\n# Approche traditionnelle\ndef classifier_email(email):\n    if \"viagra\" in email or \"lottery\" in email:\n        return \"spam\"\n    else:\n        return \"not spam\"\n\n# Approche Machine Learning\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)  # Apprend des exemples\nprediction = model.predict(new_email)\n\n\n1.3 Deep Learning\nLe Deep Learning est une sous-cat√©gorie du ML utilisant des r√©seaux de neurones artificiels profonds (plusieurs couches).\nDiagramme mermaid:\n\n\n\n\n\ngraph TD\n    A[Intelligence Artificielle] --&gt; B[Machine Learning]\n    B --&gt; C[Deep Learning]\n    A --&gt; D[Syst√®mes experts]\n    A --&gt; E[Robotique]\n    B --&gt; F[Apprentissage supervis√©]\n    B --&gt; G[Apprentissage non supervis√©]\n    B --&gt; H[Apprentissage par renforcement]",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#applications-et-cas-dutilisation",
    "href": "seance1.html#applications-et-cas-dutilisation",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "2. Applications et Cas d‚ÄôUtilisation",
    "text": "2. Applications et Cas d‚ÄôUtilisation\n\n2.1 Vision par Ordinateur\n\nD√©tection d‚Äôobjets\nReconnaissance faciale\nDiagnostic m√©dical (imagerie)\nVoitures autonomes\n\n\n\n2.2 Traitement du Langage Naturel (NLP)\n\nChatbots et assistants virtuels\nTraduction automatique\nAnalyse de sentiments\nR√©sum√© automatique de textes\n\n\n\n2.3 Syst√®mes de Recommandation\n\nE-commerce (Amazon, Alibaba)\nStreaming (Netflix, YouTube)\nR√©seaux sociaux (Facebook, Instagram)\n\n\n\n2.4 Finance\n\nD√©tection de fraude\nTrading algorithmique\n√âvaluation de risque de cr√©dit\n\n\n\n2.5 Sant√©\n\nDiagnostic de maladies\nD√©couverte de m√©dicaments\nAnalyse d‚Äôimagerie m√©dicale",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#types-dapprentissage",
    "href": "seance1.html#types-dapprentissage",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "3. Types d‚ÄôApprentissage",
    "text": "3. Types d‚ÄôApprentissage\n\n3.1 Apprentissage Supervis√©\nLe mod√®le apprend √† partir de donn√©es √©tiquet√©es (avec r√©ponses connues).\n\n\n\n\n\n\nNoteExemple\n\n\n\nDonn√©es d‚Äôentra√Ænement: emails avec labels ‚Äúspam‚Äù ou ‚Äúnon spam‚Äù Objectif: Pr√©dire si un nouveau email est spam\n\n\nT√¢ches principales:\n\nClassification: pr√©dire une cat√©gorie (spam/non spam, chat/chien)\nR√©gression: pr√©dire une valeur continue (prix maison, temp√©rature)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Classification\nclf = LogisticRegression()\nclf.fit(X_train, y_train)  # y_train contient les cat√©gories\npred_class = clf.predict(X_test)\n\n# R√©gression\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)  # y_train contient les valeurs continues\npred_value = reg.predict(X_test)\n\n\n3.2 Apprentissage Non Supervis√©\nLe mod√®le apprend √† partir de donn√©es non √©tiquet√©es (sans r√©ponses).\n\n\n\n\n\n\nNoteExemple\n\n\n\nDonn√©es: comportements d‚Äôachat de clients Objectif: Identifier des groupes de clients similaires (segmentation)\n\n\nT√¢ches principales:\n\nClustering: regrouper des donn√©es similaires\nR√©duction de dimension: simplifier les donn√©es\nD√©tection d‚Äôanomalies: identifier des points inhabituels\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Clustering\nkmeans = KMeans(n_clusters=3)\nclusters = kmeans.fit_predict(X)\n\n# R√©duction de dimension\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n\n\n3.3 Apprentissage Semi-Supervis√©\nCombine donn√©es √©tiquet√©es (peu) et non √©tiquet√©es (beaucoup).\nCas d‚Äôusage: Lorsque l‚Äô√©tiquetage est co√ªteux (imagerie m√©dicale, reconnaissance vocale)\n\n\n3.4 Apprentissage par Renforcement\nL‚Äôagent apprend par essai-erreur en interagissant avec un environnement.\n\n\n\n\n\n\nNoteExemple\n\n\n\n\nJeux vid√©o (AlphaGo, Chess AI)\nRobotique\nContr√¥le de syst√®mes complexes\n\n\n\nComposants:\n\nAgent: celui qui apprend\nEnvironnement: le monde dans lequel l‚Äôagent √©volue\nActions: ce que l‚Äôagent peut faire\nR√©compenses: feedback positif/n√©gatif",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#√©tapes-de-conception-dun-mod√®le-ia",
    "href": "seance1.html#√©tapes-de-conception-dun-mod√®le-ia",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "4. √âtapes de Conception d‚Äôun Mod√®le IA",
    "text": "4. √âtapes de Conception d‚Äôun Mod√®le IA\n\n4.1 Pipeline ML Standard\nDiagramme mermaid:\n\n\n\n\n\ngraph TB\n    A[1 D√©finir le probl√®me] --&gt; B[2 Collecter les donn√©es]\n    B --&gt; C[3 Explorer les donn√©es]\n    C --&gt; D[4 Pr√©parer les donn√©es]\n    D --&gt; E[5 Choisir un mod√®le]\n    E --&gt; F[6 Entra√Æner le mod√®le]\n    F --&gt; G[7 √âvaluer le mod√®le]\n    G --&gt; H{Performance OK?}\n    H --&gt;|Non| E\n    H --&gt;|Oui| I[8 D√©ployer]\n    I --&gt; J[9 Monitorer]\n\n\n\n\n\n\n\n\n4.2 D√©tails des √âtapes\n\n√âtape 1: D√©finir le Probl√®me\n\nQuel type de probl√®me? (classification, r√©gression, clustering)\nQuelles sont les m√©triques de succ√®s?\nQuelles sont les contraintes?\n\n\n\n√âtape 2: Collecter les Donn√©es\n\nSources de donn√©es\nQuantit√© n√©cessaire\nQualit√© des donn√©es\n\n\n\n√âtape 3: Explorer les Donn√©es (EDA)\n\nStatistiques descriptives\nVisualisations\nIdentifier les patterns, outliers, donn√©es manquantes\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Exemple EDA simple\ndf = pd.read_csv('data.csv')\nprint(df.info())\nprint(df.describe())\n\n# Visualisation\nsns.pairplot(df)\nplt.show()\n\n\n√âtape 4: Pr√©parer les Donn√©es\n\nNettoyage (valeurs manquantes, doublons)\nTransformation (normalisation, encodage)\nFeature engineering\nSplit train/test\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Split des donn√©es\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Normalisation\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Gestion des valeurs manquantes\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n\n\n√âtape 5: Choisir un Mod√®le\n\nBas√© sur le type de probl√®me\nComplexit√© vs interpr√©tabilit√©\nRessources disponibles\n\n\n\n√âtape 6: Entra√Æner le Mod√®le\n\nAjuster les param√®tres\nOptimisation\n\n\n\n√âtape 7: √âvaluer le Mod√®le\n\nM√©triques appropri√©es\nValidation crois√©e\nAnalyse des erreurs\n\n\n\n√âtape 8: D√©ployer\n\nMise en production\nAPI, application web, etc.\n\n\n\n√âtape 9: Monitorer\n\nPerformances en production\nD√©rive des donn√©es (data drift)\nMise √† jour du mod√®le",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#concepts-cl√©s",
    "href": "seance1.html#concepts-cl√©s",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "5. Concepts Cl√©s",
    "text": "5. Concepts Cl√©s\n\n5.1 Overfitting vs Underfitting\n\nUnderfittingOverfittingJuste bien (Good fit)\n\n\n\nMod√®le trop simple\nNe capture pas les patterns dans les donn√©es\nBiais √©lev√©, variance faible\nMauvaise performance train ET test\n\n\n\n\nMod√®le trop complexe\nM√©morise les donn√©es d‚Äôentra√Ænement (bruit inclus)\nBiais faible, variance √©lev√©e\nBonne performance train, mauvaise performance test\n\n\n\n\nMod√®le √©quilibr√©\nCapture les vrais patterns\nBiais et variance faibles\nBonne g√©n√©ralisation\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# G√©n√©ration de donn√©es\nnp.random.seed(42)\nX = np.linspace(0, 10, 50)\ny = 2*X + 1 + np.random.randn(50)*2\n\n# Sous-ajustement (linear)\nunderfit_model = LinearRegression()\nunderfit_model.fit(X.reshape(-1, 1), y)\ny_underfit = underfit_model.predict(X.reshape(-1, 1))\n\n# Bon ajustement (polynomial degree 2)\ngoodfit_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\ngoodfit_model.fit(X.reshape(-1, 1), y)\ny_goodfit = goodfit_model.predict(X.reshape(-1, 1))\n\n# Surajustement (polynomial degree 15)\noverfit_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=15)),\n    ('linear', LinearRegression())\n])\noverfit_model.fit(X.reshape(-1, 1), y)\ny_overfit = overfit_model.predict(X.reshape(-1, 1))\n\n# Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].scatter(X, y, alpha=0.5)\naxes[0].plot(X, y_underfit, 'r-', linewidth=2)\naxes[0].set_title('Underfitting (lin√©aire)')\n\naxes[1].scatter(X, y, alpha=0.5)\naxes[1].plot(X, y_goodfit, 'g-', linewidth=2)\naxes[1].set_title('Good Fit (polynomial deg 2)')\n\naxes[2].scatter(X, y, alpha=0.5)\naxes[2].plot(X, y_overfit, 'b-', linewidth=2)\naxes[2].set_title('Overfitting (polynomial deg 15)')\n\nplt.tight_layout()\nplt.show()\n\n\n5.2 Compromis Biais-Variance\n\nL‚Äô√©quilibre fondamental du Machine Learning\nLe compromis biais-variance est un concept essentiel qui explique pourquoi certains mod√®les ne g√©n√©ralisent pas bien. Imaginez apprendre pour un examen :\n\nBiais √©lev√© = Vous survolez trop le cours (sous-apprentissage)\nVariance √©lev√©e = Vous m√©morisez par c≈ìur sans comprendre (sur-apprentissage)\n\n\n\nFormules Math√©matiques Cl√©s\nErreur totale du mod√®le : \\[E_{\\text{total}} = \\underbrace{\\text{Biais}^2}_{\\text{simplicit√©}} + \\underbrace{\\text{Variance}}_{\\text{complexit√©}} + \\epsilon\\]\nO√π :\n\n\\(\\text{Biais} = E[\\hat{f}(x)] - f(x)\\) (diff√©rence entre pr√©diction moyenne et v√©rit√©)\n\\(\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]\\) (variabilit√© des pr√©dictions)\n\\(\\epsilon\\) = Bruit irr√©ductible des donn√©es\n\n\n\nExemple Illustratif avec Python\nDans cet exemple ,nous allons explorer le compromis biais-variance √† travers un cas concret :\n\nLe Sc√©nario\nImaginons que nous voulons pr√©dire une variable y √† partir d‚Äôune variable X. Nos donn√©es suivent une tendance sinuso√Ødale (comme une vague) avec du bruit al√©atoire ajout√© pour simuler des mesures r√©elles imparfaites.\n\n\nLes Trois Types de Mod√®les Test√©s\nNous allons ajuster trois mod√®les polynomiaux de complexit√© croissante :\n\nDegr√© 1 (Lin√©aire) :\n\nMod√®le le plus simple : une ligne droite\nProbl√®me attendu : Trop simple pour capturer la forme sinuso√Ødale ‚Üí Biais √©lev√© (sous-ajustement)\nLa ligne droite ne peut pas suivre les courbes des donn√©es\n\nDegr√© 3 (Cubique) :\n\nComplexit√© mod√©r√©e : peut faire des courbes douces\nR√©sultat attendu : Bon √©quilibre entre simplicit√© et flexibilit√©\nCapture la tendance g√©n√©rale sans trop coller au bruit\n\nDegr√© 9 (Polyn√¥me de haut degr√©) :\n\nMod√®le tr√®s complexe : peut faire des courbes tr√®s compliqu√©es\nProbl√®me attendu : Trop flexible, suit le bruit ‚Üí Variance √©lev√©e (sur-ajustement)\nPasse par presque tous les points d‚Äôentra√Ænement mais pr√©dit mal sur de nouvelles donn√©es\n\n\n\n\n\nCe que Vous Allez Observer\nDans les onglets suivants, vous verrez :\n\nPoints bleus = Donn√©es d‚Äôentra√Ænement (le mod√®le ‚Äúvoit‚Äù ces points)\nPoints rouges = Donn√©es de test (le mod√®le ne ‚Äúvoit‚Äù PAS ces points)\nLigne verte = Pr√©dictions du mod√®le\n\nQuestion cl√© √† observer : Quel mod√®le pr√©dit le mieux les points rouges (test) qu‚Äôil n‚Äôa jamais vus ?\n\n\n\n\n\n\n\n\n\nComposition de l‚ÄôErreur Totale\nExplication : L‚Äôerreur totale d‚Äôun mod√®le se d√©compose en trois parties :\n\nBiais¬≤ : Erreur syst√©matique due √† la simplicit√© du mod√®le\nVariance : Sensibilit√© du mod√®le aux variations dans les donn√©es\nBruit irr√©ductible : Erreur al√©atoire inh√©rente aux donn√©es\n\n\n\n\n\n\ngraph TD\n    A[\"Erreur Totale\"] --&gt; B[\"Biais au carre&lt;br/&gt;Erreur due a la simplicite\"]\n    A --&gt; C[\"Variance&lt;br/&gt;Erreur due a la complexite\"]\n    A --&gt; D[\"Bruit irreductible&lt;br/&gt;Non controlable\"]\n    \n    style A fill:#e1f5ff\n    style B fill:#ffe1e1\n    style C fill:#fff4e1\n    style D fill:#f0f0f0\n\n\n\n\n\n\n\n\nImpact de la Complexit√© sur le Mod√®le\nExplication : Lorsqu‚Äôon augmente la complexit√© d‚Äôun mod√®le :\n\nLe biais diminue : le mod√®le peut mieux capturer les patterns complexes\nLa variance augmente : le mod√®le devient plus sensible au bruit dans les donn√©es\n\nC‚Äôest le c≈ìur du compromis biais-variance !\n\n\n\n\n\ngraph TD\n    E1[\"Complexite croissante\"] --&gt; F1[\"Impact sur Biais\"]\n    E1 --&gt; G1[\"Impact sur Variance\"]\n    \n    F1 --&gt; H1[\"Modele simple: Biais eleve\"]\n    F1 --&gt; I1[\"Modele complexe: Biais faible\"]\n    \n    G1 --&gt; J1[\"Modele simple: Variance faible\"]\n    G1 --&gt; K1[\"Modele complexe: Variance elevee\"]\n    \n    style E1 fill:#e1f5ff\n    style F1 fill:#fff4e1\n    style G1 fill:#fff4e1\n    style H1 fill:#f8d7da\n    style I1 fill:#d4edda\n    style J1 fill:#d4edda\n    style K1 fill:#f8d7da\n\n\n\n\n\n\n\n\n\nRecherche de la Zone Optimale\nExplication : L‚Äôobjectif est de trouver le point d‚Äô√©quilibre o√π :\n\nLe biais n‚Äôest pas trop √©lev√© (mod√®le pas trop simple)\nLa variance n‚Äôest pas trop √©lev√©e (mod√®le pas trop complexe)\nLe mod√®le g√©n√©ralise bien sur de nouvelles donn√©es\n\n\n\n\n\n\ngraph TD\n    L2[\"Recherche de l equilibre\"] --&gt; M2[\"Zone optimale&lt;br/&gt;Biais carre proche Variance\"]\n    M2 --&gt; N2[\"Modele generalise bien\"]\n    \n    style L2 fill:#fff3cd\n    style M2 fill:#d1ecf1\n    style N2 fill:#d4edda\n\n\n\n\n\n\n\n\n\n\nA Retenir\n\nBiais √©lev√© = Mod√®le trop simple = Sous-ajustement (underfitting)\nVariance √©lev√©e = Mod√®le trop complexe = Sur-ajustement (overfitting)\nObjectif = Trouver le juste milieu pour une bonne g√©n√©ralisation\nErreur de test = Indicateur principal de la performance r√©elle du mod√®le\n\n\n\nComment Trouver l‚Äô√âquilibre ?\n\nCommencez simple (r√©gression lin√©aire comme baseline)\nAugmentez progressivement la complexit√©\nSurveillez l‚Äô√©cart entre performance d‚Äôentra√Ænement et de test\nArr√™tez quand l‚Äôerreur de test commence √† augmenter\n\nFormule √† retenir : \\[E_{\\text{test}} = \\text{Biais}^2 + \\text{Variance} + \\epsilon\\]\nLe succ√®s = trouver le point o√π cette somme est minimale !\n\nR√®gle d‚Äôor : Visez l‚Äô√©quilibre o√π votre mod√®le est assez complexe pour apprendre les patterns importants, mais assez simple pour ignorer le bruit al√©atoire.\n\nCette compr√©hension est cruciale pour choisir et ajuster vos mod√®les. L‚Äôobjectif n‚Äôest pas d‚Äô√©liminer le biais ou la variance, mais de trouver l‚Äô√©quilibre optimal pour votre probl√®me sp√©cifique !\n\n\nExercice Pratique : Diagnostic et Correction\nfrom sklearn.datasets import make_moons\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# Donn√©es non-lin√©aires\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Test de diff√©rents mod√®les\nmodels = {\n    'Arbre Profond (Variance)': DecisionTreeClassifier(max_depth=20),\n    'Arbre Simple (Biais)': DecisionTreeClassifier(max_depth=2),\n    'Arbre Optimis√©': DecisionTreeClassifier(max_depth=5, min_samples_split=10),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5),\n    'SVM Lin√©aire (Biais)': SVC(kernel='linear', C=1),\n    'SVM RBF (Variance)': SVC(kernel='rbf', C=10, gamma=10)\n}\n\nprint(\"üß™ TEST DU COMPROMIS BIAS-VARIANCE\")\nprint(\"=\" * 50)\n\nfor name, model in models.items():\n    diagnose_bias_variance(model, X_train, X_test, y_train, y_test)\n    print(\"-\" * 40)\n\n\n\n\n\n\nTipCliquez ici pour r√©v√©ler/masquer la solution interactive\n\n\n\n\n\n\n\n \n\n\nAfficher/Masquer l‚Äôexercice interactif\n\n\n\n\nConclusion et Bonnes Pratiques\nChecklist de Validation\n\nBiais √©lev√© suspect√© ‚Üí Essayer mod√®les plus complexes\nVariance √©lev√©e suspect√©e ‚Üí Ajouter r√©gularisation\nDonn√©es limit√©es ‚Üí Privil√©gier mod√®les simples\nDonn√©es abondantes ‚Üí Mod√®les complexes possibles\nToujours utiliser validation crois√©e\n\nR√®gles Empiriques\n\nCommencez simple : Lin√©aire/logistique comme baseline\nAugmentez progressivement la complexit√©\nSurveillez l‚Äô√©cart entre train et validation\nUtilisez l‚Äôensemble de test UNE SEULE FOIS √† la fin\nDocumentez vos choix d‚Äôhyperparam√®tres\n\nFormule √† Retenir\n\nMod√®le Id√©al = Biais¬≤ + Variance + Bruit\n‚Üí Minimiser la somme, pas individuellement\n\nLe compromis biais-variance n‚Äôest pas un probl√®me √† √©liminer mais un √©quilibre √† ma√Ætriser. La cl√© r√©side dans la compr√©hension des besoins de votre probl√®me sp√©cifique et l‚Äôajustement continu de votre approche.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#exercices-de-r√©flexion",
    "href": "seance1.html#exercices-de-r√©flexion",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "6. Exercices de R√©flexion",
    "text": "6. Exercices de R√©flexion\n\n\n\n\n\n\nWarningQuestion 1\n\n\n\nPour chacun des probl√®mes suivants, identifiez le type d‚Äôapprentissage appropri√© (supervis√©, non supervis√©, renforcement):\n\nPr√©dire si un patient a une maladie cardiaque\nRegrouper des articles de presse par th√®me\nApprendre √† un robot √† marcher\nPr√©dire le prix d‚Äôune maison\nD√©tecter des transactions frauduleuses inhabituelles\n\n\n\n\n\n\n\n\n\nNoteR√©ponse 1\n\n\n\n\n\n\nApprentissage supervis√© (Classification) : On pr√©dit une √©tiquette binaire (malade ou non).\nApprentissage non supervis√© (Clustering) : On regroupe des donn√©es sans √©tiquettes pr√©alables.\nApprentissage par renforcement : Le robot apprend par essais et erreurs avec un syst√®me de r√©compenses.\nApprentissage supervis√© (R√©gression) : On pr√©dit une valeur num√©rique continue.\nApprentissage non supervis√© (D√©tection d‚Äôanomalies) : On cherche des comportements qui s‚Äô√©cartent de la norme.\n\n\n\n\n\n\n\n\n\n\nWarningQuestion 2\n\n\n\nExpliquez pourquoi un mod√®le avec 100% de pr√©cision sur les donn√©es d‚Äôentra√Ænement peut √™tre probl√©matique.\n\n\n\n\n\n\n\n\nNoteR√©ponse 2\n\n\n\n\n\nUne pr√©cision de 100 % sur les donn√©es d‚Äôentra√Ænement est souvent le signe d‚Äôun surapprentissage (overfitting). Le mod√®le a ‚Äúm√©moris√©‚Äù le bruit et les particularit√©s des donn√©es d‚Äôentra√Ænement au lieu d‚Äôapprendre les tendances g√©n√©rales. Par cons√©quent, il risque d‚Äôavoir de tr√®s mauvaises performances sur de nouvelles donn√©es (faible capacit√© de g√©n√©ralisation).\n\n\n\n\n\n\n\n\n\nWarningQuestion 3\n\n\n\nDonnez 3 exemples d‚Äôapplications ML dans votre domaine d‚Äôint√©r√™t et identifiez le type de probl√®me (classification, r√©gression, clustering).\n\n\n\n\n\n\n\n\nNoteR√©ponse 3\n\n\n\n\n\nExemples dans le domaine du commerce √©lectronique :\n\nSyst√®me de recommandation de produits : Identifier des groupes de clients aux comportements similaires (Clustering).\nPr√©vision de la demande (stocks) : Pr√©dire le nombre d‚Äôunit√©s qui seront vendues le mois prochain (R√©gression).\nFiltrage de commentaires abusifs : Identifier si un avis client est conforme ou non aux r√®gles de la plateforme (Classification).",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#r√©sum√©-de-la-s√©ance",
    "href": "seance1.html#r√©sum√©-de-la-s√©ance",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "R√©sum√© de la S√©ance",
    "text": "R√©sum√© de la S√©ance\n\n\n\n\n\n\nImportantPoints cl√©s √† retenir\n\n\n\n\nML = apprentissage √† partir de donn√©es sans programmation explicite\nTrois types principaux: supervis√©, non supervis√©, renforcement\nPipeline ML: Probl√®me ‚Üí Donn√©es ‚Üí Exploration ‚Üí Pr√©paration ‚Üí Mod√®le ‚Üí √âvaluation ‚Üí D√©ploiement\nOverfitting vs Underfitting: √©quilibre crucial pour la g√©n√©ralisation\nApplications diverses: vision, NLP, recommandations, finance, sant√©",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance1.html#lectures-compl√©mentaires",
    "href": "seance1.html#lectures-compl√©mentaires",
    "title": "S√©ance 1: Introduction IA et Machine Learning",
    "section": "Lectures Compl√©mentaires",
    "text": "Lectures Compl√©mentaires\n\nG√©ron, A. (2019) - Chapitre 1: The Machine Learning Landscape\nGoogle‚Äôs Machine Learning Crash Course\nAndrew Ng - What is Machine Learning?",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>S√©ance 1: Introduction IA et Machine Learning</span>"
    ]
  },
  {
    "objectID": "seance2.html",
    "href": "seance2.html",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "",
    "text": "1. Introduction √† la Classification",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#introduction-√†-la-classification",
    "href": "seance2.html#introduction-√†-la-classification",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "",
    "text": "1.1 D√©finition\nLa classification est une t√¢che d‚Äôapprentissage supervis√© o√π l‚Äôobjectif est de pr√©dire une classe ou cat√©gorie discr√®te √† partir de caract√©ristiques d‚Äôentr√©e.\n\n\n\n\n\n\nNoteExemple\n\n\n\nEntr√©e: Caract√©ristiques d‚Äôun email (mots, exp√©diteur, longueur, etc.)\nSortie: Classe = ‚ÄúSpam‚Äù ou ‚ÄúNon Spam‚Äù\n\n\n\n\n1.2 Diff√©rence Classification vs R√©gression\n\n\n\nCaract√©ristique\nClassification\nR√©gression\n\n\n\n\nSortie\nCat√©gorie discr√®te\nValeur continue\n\n\nExemple\nSpam/Non spam\nPrix d‚Äôune maison\n\n\nM√©trique\nAccuracy, F1-score\nMAE, RMSE\n\n\nFonction\nProbabilit√© ‚Üí Classe\nValeur num√©rique",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#types-de-classification",
    "href": "seance2.html#types-de-classification",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "2. Types de Classification",
    "text": "2. Types de Classification\n\n2.1 Classification Binaire\nDeux classes possibles: 0 ou 1, Vrai ou Faux, Positif ou N√©gatif\nExemples:\n\nD√©tection de spam (spam/non spam)\nDiagnostic m√©dical (malade/sain)\nD√©tection de fraude (fraude/l√©gitime)\nApprobation de cr√©dit (approuv√©/rejet√©)\n\n\n# Exemple: Classification binaire\ny_binary = [0, 1, 1, 0, 1, 0, 0, 1]  # 0 = n√©gatif, 1 = positif\n\n\n\n2.2 Classification Multi-classes\nPlus de deux classes mutuellement exclusives (une seule classe par instance)\nExemples:\n\nReconnaissance de chiffres manuscrits (0-9 = 10 classes)\nClassification de fleurs Iris (Setosa, Versicolor, Virginica)\nCat√©gorisation d‚Äôarticles (Sport, Politique, √âconomie, Culture)\n\n\n# Exemple: Classification multi-classes\ny_multiclass = [0, 1, 2, 1, 0, 2, 1]  # 3 classes: 0, 1, 2\n\n\n\n2.3 Classification Multi-label\nPlusieurs classes simultan√©es possibles pour une instance\nExemples:\n\n√âtiquetage de photos (peut contenir: personne, chien, ext√©rieur)\nCat√©gorisation de films (peut √™tre: Action, Com√©die, Drame)\nAnalyse de sentiments multiple (joie + surprise)\n\n\n# Exemple: Classification multi-label\ny_multilabel = [\n    [1, 0, 1],  # instance a les labels 0 et 2\n    [0, 1, 1],  # instance a les labels 1 et 2\n    [1, 1, 0]   # instance a les labels 0 et 1\n]",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#algorithmes-de-classification",
    "href": "seance2.html#algorithmes-de-classification",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "3. Algorithmes de Classification",
    "text": "3. Algorithmes de Classification\n\n3.1 Arbre de D√©cision (Decision Tree)\nMod√®le qui prend des d√©cisions bas√©es sur des questions successives.\n\nPrincipe\nL‚Äôarbre divise l‚Äôespace des caract√©ristiques en r√©gions par des questions binaires.\nDiagramme :\n\n\n\n\n\ngraph TD\n    A[Age &gt; 30?] --&gt;|Oui| B[Revenu &gt; 50k?]\n    A --&gt;|Non| C[√âtudiant?]\n    B --&gt;|Oui| D[Approuv√© ]\n    B --&gt;|Non| E[Rejet√© ]\n    C --&gt;|Oui| F[Rejet√© ]\n    C --&gt;|Non| G[Approuv√© ]\n\n\n\n\n\n\n\n\nAvantages\n\nFacile √† interpr√©ter et visualiser\nPas besoin de normalisation des donn√©es\nG√®re les donn√©es non lin√©aires\nG√®re les variables cat√©gorielles et num√©riques\n\n\n\nInconv√©nients\n\nTendance √† l‚Äôoverfitting\nInstable (petits changements de donn√©es ‚Üí arbre diff√©rent)\nBiais vers les classes majoritaires\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des donn√©es\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Entra√Ænement\nclf = DecisionTreeClassifier(max_depth=3, random_state=42)\nclf.fit(X_train, y_train)\n\n# Pr√©diction\ny_pred = clf.predict(X_test)\nprint(f\"Accuracy: {clf.score(X_test, y_test):.2f}\")\n\n\n\n\n3.2 Random Forest\nEnsemble d‚Äôarbres de d√©cision qui votent ensemble.\n\nPrincipe\n\nCr√©er N arbres sur des sous-ensembles al√©atoires de donn√©es\nChaque arbre vote pour une classe\nPr√©diction finale = vote majoritaire\n\n\n\nAvantages\n\nTr√®s performant et robuste\nR√©duit l‚Äôoverfitting par rapport √† un arbre unique\nG√®re bien les grandes dimensions\nDonne l‚Äôimportance des features\n\n\n\nInconv√©nients\n\nMoins interpr√©table qu‚Äôun arbre unique\nPlus lent √† entra√Æner et pr√©dire\nM√©moire importante\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Entra√Ænement\nrf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\nrf.fit(X_train, y_train)\n\n# Pr√©diction\ny_pred = rf.predict(X_test)\nprint(f\"Accuracy: {rf.score(X_test, y_test):.2f}\")\n\n# Importance des features\nimportances = rf.feature_importances_\nfor i, imp in enumerate(importances):\n    print(f\"Feature {iris.feature_names[i]}: {imp:.3f}\")\n\n\n\n\n3.3 Support Vector Machine (SVM)\nTrouve l‚Äôhyperplan optimal qui s√©pare les classes avec la marge maximale.\n\nPrincipe\n\nMarge: distance entre l‚Äôhyperplan et les points les plus proches (vecteurs de support)\nObjectif: Maximiser cette marge\nKernel trick: Permet de g√©rer des donn√©es non lin√©airement s√©parables\n\n\nfrom sklearn.svm import SVC\n\n# SVM lin√©aire\nsvm_linear = SVC(kernel='linear', C=1.0)\nsvm_linear.fit(X_train, y_train)\n\n# SVM avec kernel RBF (pour donn√©es non lin√©aires)\nsvm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\nsvm_rbf.fit(X_train, y_train)\n\nprint(f\"SVM Linear Accuracy: {svm_linear.score(X_test, y_test):.2f}\")\nprint(f\"SVM RBF Accuracy: {svm_rbf.score(X_test, y_test):.2f}\")\n\n\n\nAvantages\n\nTr√®s efficace en haute dimension\nRobuste aux outliers\nVersatile (diff√©rents kernels)\n\n\n\nInconv√©nients\n\nLent sur de grandes donn√©es\nDifficile √† interpr√©ter\nSensible au choix des hyperparam√®tres\n\n\n\n\n3.4 Na√Øve Bayes\nBas√© sur le th√©or√®me de Bayes avec hypoth√®se d‚Äôind√©pendance des features.\n\nPrincipe - Th√©or√®me de Bayes\n\\[P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\\]\nO√π:\n\n\\(P(y|X)\\) = probabilit√© de la classe \\(y\\) sachant les features \\(X\\) (posterior)\n\\(P(X|y)\\) = vraisemblance\n\\(P(y)\\) = probabilit√© a priori de la classe\n\\(P(X)\\) = √©vidence (constante)\n\n\n\nHypoth√®se ‚ÄúNa√Øve‚Äù\nLes features sont ind√©pendantes conditionnellement √† la classe:\n\\[P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y)\\]\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\n# Gaussian Naive Bayes (pour features continues)\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nprint(f\"Gaussian NB Accuracy: {gnb.score(X_test, y_test):.2f}\")\n\n# Multinomial NB (pour comptages, ex: mots dans un texte)\n# mnb = MultinomialNB()\n# mnb.fit(X_train_counts, y_train)\n\n\n\nAvantages\n\nTr√®s rapide (entra√Ænement et pr√©diction)\nFonctionne bien avec peu de donn√©es\nExcellent pour la classification de texte\n\n\n\nInconv√©nients\n\nHypoth√®se d‚Äôind√©pendance rarement vraie\nPerformance limit√©e si hypoth√®se viol√©e\n\n\n\n\n3.5 R√©gression Logistique\nAttention: Malgr√© son nom, c‚Äôest un algorithme de classification !\n\nPrincipe\nMod√®le lin√©aire qui utilise la fonction sigmo√Øde pour produire des probabilit√©s.\nFonction sigmo√Øde: \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\nMod√®le: \\[P(y=1|X) = \\sigma(w^T X + b) = \\frac{1}{1 + e^{-(w^T X + b)}}\\]\n\n\nInterpr√©tation Probabiliste\n\nSortie \\(\\in [0, 1]\\) : probabilit√© d‚Äôappartenance √† la classe positive\nSi \\(P(y=1|X) \\geq 0.5\\) ‚Üí pr√©diction = classe 1\nSi \\(P(y=1|X) &lt; 0.5\\) ‚Üí pr√©diction = classe 0\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Entra√Ænement\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\n\n# Pr√©diction de classes\ny_pred = log_reg.predict(X_test)\n\n# Pr√©diction de probabilit√©s\ny_proba = log_reg.predict_proba(X_test)\n\nprint(f\"Accuracy: {log_reg.score(X_test, y_test):.2f}\")\nprint(f\"\\nPremi√®re pr√©diction:\")\nprint(f\"  Probabilit√©s: {y_proba[0]}\")\nprint(f\"  Classe pr√©dite: {y_pred[0]}\")\n\n\n\nAvantages\n\nSimple et interpr√©table\nDonne des probabilit√©s (utile pour la prise de d√©cision)\nPeu de param√®tres √† ajuster\nFonctionne bien sur donn√©es lin√©airement s√©parables\n\n\n\nInconv√©nients\n\nAssume une relation lin√©aire\nSensible aux outliers\nN√©cessite feature engineering pour les relations non lin√©aires\n\n\n\n\n3.6 k-Nearest Neighbors (k-NN)\nClassification bas√©e sur la proximit√© avec les voisins.\n\nPrincipe\n\nCalculer la distance entre le nouveau point et tous les points d‚Äôentra√Ænement\nS√©lectionner les k points les plus proches\nVote majoritaire parmi ces k voisins\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# k=5 voisins\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\nprint(f\"k-NN Accuracy: {knn.score(X_test, y_test):.2f}\")\n\n\n\nAvantages\n\nSimple et intuitif\nPas d‚Äôentra√Ænement (lazy learning)\nFonctionne bien pour des fronti√®res complexes\n\n\n\nInconv√©nients\n\nLent pour la pr√©diction (calcule toutes les distances)\nSensible √† l‚Äô√©chelle des features (n√©cessite normalisation)\nCurse of dimensionality (mauvais en haute dimension)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#crit√®res-d√©valuation-aper√ßu",
    "href": "seance2.html#crit√®res-d√©valuation-aper√ßu",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "4. Crit√®res d‚Äô√âvaluation (Aper√ßu)",
    "text": "4. Crit√®res d‚Äô√âvaluation (Aper√ßu)\n\n4.1 M√©triques Principales\n\nAccuracy: Proportion de pr√©dictions correctes\nPrecision: Proportion de vrais positifs parmi les pr√©dictions positives\nRecall: Proportion de vrais positifs parmi les cas r√©ellement positifs\nF1-Score: Moyenne harmonique de Precision et Recall\n\n\n\n\n\n\n\nImportant\n\n\n\nCes m√©triques seront d√©taill√©es en profondeur dans la S√©ance 5 - TD2\n\n\n\n\n4.2 Exemple Simple\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Calcul de l'accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Rapport complet\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#exemple-complet-comparaison-dalgorithmes",
    "href": "seance2.html#exemple-complet-comparaison-dalgorithmes",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "5. Exemple Complet: Comparaison d‚ÄôAlgorithmes",
    "text": "5. Exemple Complet: Comparaison d‚ÄôAlgorithmes\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Cr√©ation d'un dataset synth√©tique\nX, y = make_classification(\n    n_samples=1000, \n    n_features=2, \n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=42\n)\n\n# Split et normalisation\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Entra√Ænement de plusieurs mod√®les\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodels = {\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(kernel='rbf', random_state=42),\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Naive Bayes': GaussianNB(),\n    'k-NN': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Comparaison des performances\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    score = model.score(X_test_scaled, y_test)\n    results[name] = score\n    print(f\"{name:20s}: {score:.4f}\")\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nplt.barh(list(results.keys()), list(results.values()))\nplt.xlabel('Accuracy')\nplt.title('Comparaison des Algorithmes de Classification')\nplt.xlim([0, 1])\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#r√©sum√©-de-la-s√©ance",
    "href": "seance2.html#r√©sum√©-de-la-s√©ance",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "R√©sum√© de la S√©ance",
    "text": "R√©sum√© de la S√©ance\n\n\n\n\n\n\nImportantPoints cl√©s √† retenir\n\n\n\n\nClassification = pr√©dire une cat√©gorie discr√®te\nTypes: Binaire, Multi-classes, Multi-label\nAlgorithmes principaux:\n\nDecision Tree: interpr√©table mais tendance √† l‚Äôoverfitting\nRandom Forest: robuste et performant\nSVM: excellent en haute dimension\nNa√Øve Bayes: rapide, bon pour le texte\nR√©gression Logistique: simple, interpr√©table, probabiliste\nk-NN: simple mais co√ªteux en pr√©diction\n\nChoix du mod√®le d√©pend de: taille des donn√©es, interpr√©tabilit√©, performance, ressources\n√âvaluation avec m√©triques appropri√©es (d√©tails en TD2)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#exercices",
    "href": "seance2.html#exercices",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "Exercices",
    "text": "Exercices\n\n\n\n\n\n\nWarningExercice 1\n\n\n\nImpl√©mentez une r√©gression logistique sur le dataset Iris et analysez les coefficients appris. Que repr√©sentent-ils?\n\n\n\n\n\n\n\n\nNoteR√©ponse 1\n\n\n\n\n\nCode d‚Äôimpl√©mentation :\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Chargement et entra√Ænement\niris = load_iris()\nX, y = iris.data, iris.target\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X, y)\n\n# Analyse des coefficients\nprint(f\"Coefficients (W): \\n{log_reg.coef_}\")\nprint(f\"Intercept (b): {log_reg.intercept_}\")\nExplication des coefficients :\n\nSignification : Chaque coefficient repr√©sente l‚Äôimportance d‚Äôune caract√©ristique (feature) pour pr√©dire une classe donn√©e.\nDirection : Un coefficient positif augmente la probabilit√© que l‚Äôinstance appartienne √† la classe, tandis qu‚Äôun coefficient n√©gatif la diminue.\nAmpleur : Plus la valeur absolue du coefficient est √©lev√©e, plus la caract√©ristique a un impact d√©terminant sur la d√©cision du mod√®le.\n\n\n\n\n\n\n\n\n\n\nWarningExercice 2\n\n\n\nComparez les performances de Decision Tree vs Random Forest sur le dataset digits de sklearn. Expliquez les diff√©rences observ√©es.\n\n\n\n\n\n\n\n\nNoteR√©ponse 2\n\n\n\n\n\nComparaison th√©orique et pratique :\n\nPerformance : Le Random Forest obtient g√©n√©ralement une meilleure pr√©cision (accuracy) que l‚Äôarbre seul car il combine les pr√©dictions de nombreux arbres, r√©duisant ainsi les erreurs al√©atoires.\nStabilit√© : Un Decision Tree est tr√®s sensible aux petites variations des donn√©es (variance √©lev√©e). Le Random Forest stabilise cela par √©chantillonnage (bagging).\nSur-apprentissage (Overfitting) : L‚Äôarbre de d√©cision a tendance √† m√©moriser le bruit des donn√©es s‚Äôil n‚Äôest pas limit√© en profondeur. Le Random Forest limite ce risque en moyennant les r√©sultats de plusieurs arbres entra√Æn√©s sur des sous-ensembles diff√©rents.\n\n\n\n\n\n\n\n\n\n\nWarningExercice 3\n\n\n\nPour un probl√®me de d√©tection de fraude bancaire, quel algorithme recommanderiez-vous et pourquoi? Consid√©rez les aspects: interpr√©tabilit√©, temps r√©el, d√©s√©quilibre des classes.\n\n\n\n\n\n\n\n\nNoteR√©ponse 3\n\n\n\n\n\nPour la d√©tection de fraude, je recommanderais le Random Forest ou le XGBoost (m√©thodes d‚Äôensemble), pour les raisons suivantes :\n\nInterpr√©tabilit√© : Bien que moins direct qu‚Äôun arbre unique, le Random Forest permet d‚Äôextraire l‚Äôimportance des variables, ce qui est crucial pour comprendre quels facteurs (montant, lieu, heure) d√©clenchent une alerte de fraude.\nD√©s√©quilibre des classes : La fraude est rare (classe minoritaire). Ces algorithmes g√®rent mieux les donn√©es d√©s√©quilibr√©es gr√¢ce √† des techniques de pond√©ration des classes.\nTemps r√©el : Une fois entra√Æn√©, la pr√©diction d‚Äôun Random Forest est tr√®s rapide, permettant de valider ou bloquer une transaction en quelques millisecondes.\nRobustesse : Ces mod√®les sont moins sensibles aux valeurs aberrantes (outliers) souvent pr√©sentes dans les donn√©es financi√®res.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance2.html#lectures-compl√©mentaires",
    "href": "seance2.html#lectures-compl√©mentaires",
    "title": "S√©ance 2: Apprentissage Supervis√© - Classification",
    "section": "Lectures Compl√©mentaires",
    "text": "Lectures Compl√©mentaires\n\nG√©ron, A. (2019) - Chapitre 3: Classification\nScikit-learn Documentation: Supervised Learning\nStatQuest: Logistic Regression",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>S√©ance 2: Apprentissage Supervis√© - Classification</span>"
    ]
  },
  {
    "objectID": "seance3.html",
    "href": "seance3.html",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "",
    "text": "Objectifs du TP\n√Ä la fin de ce TP, vous serez capable de:\nüìé Lien du TP : TP1 ‚Äî Pipeline Classification",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#objectifs-du-tp",
    "href": "seance3.html#objectifs-du-tp",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "",
    "text": "Charger et explorer un dataset\nPr√©parer les donn√©es pour l‚Äôapprentissage\nCr√©er un pipeline de pr√©traitement avec Scikit-learn\nEntra√Æner un mod√®le de classification binaire\n√âvaluer les performances du mod√®le",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#configuration-de-lenvironnement",
    "href": "seance3.html#configuration-de-lenvironnement",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "1. Configuration de l‚ÄôEnvironnement",
    "text": "1. Configuration de l‚ÄôEnvironnement\n\n# Installation des biblioth√®ques (si n√©cessaire)\n# !pip install scikit-learn pandas numpy matplotlib seaborn\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\n# Configuration\nplt.style.use('default')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n\nprint(\"‚úì Biblioth√®ques import√©es avec succ√®s\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#chargement-et-exploration-des-donn√©es",
    "href": "seance3.html#chargement-et-exploration-des-donn√©es",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "2. Chargement et Exploration des Donn√©es",
    "text": "2. Chargement et Exploration des Donn√©es\n\n2.1 Chargement du Dataset Titanic\n\n# Chargement depuis seaborn\ntitanic = sns.load_dataset('titanic')\n\n# Affichage des premi√®res lignes\nprint(\"Aper√ßu des donn√©es:\")\nprint(titanic.head())\n\nprint(f\"\\nDimensions: {titanic.shape}\")\nprint(f\"Colonnes: {titanic.columns.tolist()}\")\n\n\n\n2.2 Exploration Initiale\n\n# Informations g√©n√©rales\nprint(\"Informations sur le dataset:\")\nprint(titanic.info())\n\nprint(\"\\nStatistiques descriptives:\")\nprint(titanic.describe())\n\n# V√©rification des valeurs manquantes\nprint(\"\\nValeurs manquantes:\")\nprint(titanic.isnull().sum())\n\n# Distribution de la variable cible\nprint(\"\\nDistribution de la survie:\")\nprint(titanic['survived'].value_counts())\nprint(f\"\\nTaux de survie: {titanic['survived'].mean():.2%}\")\n\n\n\n2.3 Visualisations Exploratoires\n\n# Figure 1: Distribution de la survie\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Survie globale\naxes[0, 0].pie(\n    titanic['survived'].value_counts(), \n    labels=['D√©c√©d√©', 'Survivant'],\n    autopct='%1.1f%%',\n    startangle=90,\n    colors=['#ff6b6b', '#51cf66']\n)\naxes[0, 0].set_title('Distribution de la Survie')\n\n# Survie par sexe\nsurvival_by_sex = titanic.groupby(['sex', 'survived']).size().unstack()\nsurvival_by_sex.plot(kind='bar', ax=axes[0, 1], color=['#ff6b6b', '#51cf66'])\naxes[0, 1].set_title('Survie par Sexe')\naxes[0, 1].set_xlabel('Sexe')\naxes[0, 1].set_ylabel('Nombre de passagers')\naxes[0, 1].legend(['D√©c√©d√©', 'Survivant'])\naxes[0, 1].tick_params(axis='x', rotation=0)\n\n# Survie par classe\nsurvival_by_class = titanic.groupby(['pclass', 'survived']).size().unstack()\nsurvival_by_class.plot(kind='bar', ax=axes[1, 0], color=['#ff6b6b', '#51cf66'])\naxes[1, 0].set_title('Survie par Classe')\naxes[1, 0].set_xlabel('Classe')\naxes[1, 0].set_ylabel('Nombre de passagers')\naxes[1, 0].legend(['D√©c√©d√©', 'Survivant'])\n\n# Distribution de l'√¢ge\naxes[1, 1].hist(titanic[titanic['survived']==0]['age'].dropna(), \n                alpha=0.5, label='D√©c√©d√©', bins=30, color='#ff6b6b')\naxes[1, 1].hist(titanic[titanic['survived']==1]['age'].dropna(), \n                alpha=0.5, label='Survivant', bins=30, color='#51cf66')\naxes[1, 1].set_title('Distribution de l\\'√¢ge par survie')\naxes[1, 1].set_xlabel('√Çge')\naxes[1, 1].set_ylabel('Fr√©quence')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#pr√©paration-des-donn√©es",
    "href": "seance3.html#pr√©paration-des-donn√©es",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "3. Pr√©paration des Donn√©es",
    "text": "3. Pr√©paration des Donn√©es\n\n3.1 S√©lection des Features\n\n# S√©lection des colonnes pertinentes\nfeatures = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\ntarget = 'survived'\n\n# Cr√©ation du dataset de travail\ndf = titanic[features + [target]].copy()\n\nprint(f\"Dataset de travail: {df.shape}\")\nprint(f\"\\nValeurs manquantes:\")\nprint(df.isnull().sum())\n\n\n\n3.2 Traitement des Valeurs Manquantes\n\n# Strat√©gies de traitement\n# 1. Age: remplir avec la m√©diane\ndf['age'].fillna(df['age'].median(), inplace=True)\n\n# 2. Embarked: remplir avec le mode (valeur la plus fr√©quente)\ndf['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n\n# 3. Fare: remplir avec la m√©diane (si manquant)\ndf['fare'].fillna(df['fare'].median(), inplace=True)\n\n# V√©rification\nprint(\"Apr√®s traitement:\")\nprint(df.isnull().sum())\n\n\n\n3.3 Encodage des Variables Cat√©gorielles\n\n# Encodage de 'sex'\ndf['sex'] = df['sex'].map({'male': 0, 'female': 1})\n\n# Encodage de 'embarked' (One-Hot Encoding)\ndf = pd.get_dummies(df, columns=['embarked'], prefix='embarked', drop_first=True)\n\nprint(\"Dataset apr√®s encodage:\")\nprint(df.head())\nprint(f\"\\nNouvelles dimensions: {df.shape}\")\n\n\n\n3.4 S√©paration Features / Target\n\n# S√©paration X (features) et y (target)\nX = df.drop('survived', axis=1)\ny = df['survived']\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"\\nFeatures utilis√©es:\\n{X.columns.tolist()}\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#split-trainvalidationtest",
    "href": "seance3.html#split-trainvalidationtest",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "4. Split Train/Validation/Test",
    "text": "4. Split Train/Validation/Test\n\n4.1 Split Train/Test\n\n# Split 80/20\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2,      # 20% pour le test\n    random_state=42,    # reproductibilit√©\n    stratify=y          # pr√©server la distribution des classes\n)\n\nprint(f\"Train set: {X_train.shape}\")\nprint(f\"Test set:  {X_test.shape}\")\n\n# V√©rification de la distribution\nprint(f\"\\nDistribution train: {y_train.value_counts(normalize=True)}\")\nprint(f\"Distribution test:  {y_test.value_counts(normalize=True)}\")\n\n\n\n4.2 Split Train/Validation (optionnel)\n\n# Optionnel: cr√©er un ensemble de validation\nX_train_full, X_val, y_train_full, y_val = train_test_split(\n    X_train, y_train,\n    test_size=0.2,  # 20% du train pour validation\n    random_state=42,\n    stratify=y_train\n)\n\nprint(f\"Train full: {X_train_full.shape}\")\nprint(f\"Validation: {X_val.shape}\")\nprint(f\"Test:       {X_test.shape}\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#pipeline-de-pr√©traitement-et-entra√Ænement",
    "href": "seance3.html#pipeline-de-pr√©traitement-et-entra√Ænement",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "5. Pipeline de Pr√©traitement et Entra√Ænement",
    "text": "5. Pipeline de Pr√©traitement et Entra√Ænement\n\n5.1 Cr√©ation du Pipeline\n\n# Pipeline: Standardisation + Mod√®le\npipeline = Pipeline([\n    ('scaler', StandardScaler()),  # √âtape 1: Standardisation\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))  # √âtape 2: Mod√®le\n])\n\nprint(\"Pipeline cr√©√©:\")\nprint(pipeline)\n\n\n\n5.2 Entra√Ænement du Mod√®le\n\n# Entra√Ænement\nprint(\"Entra√Ænement en cours...\")\npipeline.fit(X_train, y_train)\nprint(\"‚úì Entra√Ænement termin√©\")\n\n# Pr√©dictions\ny_train_pred = pipeline.predict(X_train)\ny_test_pred = pipeline.predict(X_test)\n\nprint(\"‚úì Pr√©dictions effectu√©es\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#√©valuation-initiale",
    "href": "seance3.html#√©valuation-initiale",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "6. √âvaluation Initiale",
    "text": "6. √âvaluation Initiale\n\n6.1 Accuracy\n\n# Calcul de l'accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\nprint(f\"Accuracy Train: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\nprint(f\"Accuracy Test:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n\n# Analyse de l'overfitting\ndiff = train_accuracy - test_accuracy\nprint(f\"\\nDiff√©rence Train-Test: {diff:.4f}\")\nif diff &lt; 0.05:\n    print(\"‚Üí Bon √©quilibre biais-variance\")\nelif diff &lt; 0.10:\n    print(\"‚Üí L√©ger overfitting\")\nelse:\n    print(\"‚Üí Overfitting significatif\")\n\n\n\n6.2 Matrice de Confusion\n\n# Calcul de la matrice de confusion\ncm = confusion_matrix(y_test, y_test_pred)\n\n# Visualisation\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['D√©c√©d√©', 'Survivant'],\n            yticklabels=['D√©c√©d√©', 'Survivant'])\nplt.title('Matrice de Confusion - Test Set')\nplt.ylabel('Vraie Classe')\nplt.xlabel('Classe Pr√©dite')\nplt.tight_layout()\nplt.show()\n\n# Interpr√©tation\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\nVrais N√©gatifs (TN):  {tn}\")\nprint(f\"Faux Positifs (FP):   {fp}\")\nprint(f\"Faux N√©gatifs (FN):   {fn}\")\nprint(f\"Vrais Positifs (TP):  {tp}\")\n\n\n\n6.3 Rapport de Classification\n\n# Rapport d√©taill√©\nprint(\"\\nRapport de Classification:\")\nprint(classification_report(y_test, y_test_pred, \n                          target_names=['D√©c√©d√©', 'Survivant']))",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#comparaison-de-plusieurs-mod√®les",
    "href": "seance3.html#comparaison-de-plusieurs-mod√®les",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "7. Comparaison de Plusieurs Mod√®les",
    "text": "7. Comparaison de Plusieurs Mod√®les\n\n# D√©finition des mod√®les\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n}\n\n# Entra√Ænement et √©valuation\nresults = {}\nfor name, model in models.items():\n    # Pipeline pour chaque mod√®le\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', model)\n    ])\n    \n    # Entra√Ænement\n    pipe.fit(X_train, y_train)\n    \n    # √âvaluation\n    train_score = pipe.score(X_train, y_train)\n    test_score = pipe.score(X_test, y_test)\n    \n    results[name] = {\n        'train': train_score,\n        'test': test_score,\n        'diff': train_score - test_score\n    }\n    \n    print(f\"\\n{name}:\")\n    print(f\"  Train Accuracy: {train_score:.4f}\")\n    print(f\"  Test Accuracy:  {test_score:.4f}\")\n    print(f\"  Diff√©rence:     {train_score - test_score:.4f}\")\n\n# Visualisation comparative\ndf_results = pd.DataFrame(results).T\ndf_results[['train', 'test']].plot(kind='bar', figsize=(10, 6))\nplt.title('Comparaison des Performances des Mod√®les')\nplt.xlabel('Mod√®le')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'])\nplt.xticks(rotation=45, ha='right')\nplt.ylim([0, 1])\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#analyse-des-pr√©dictions",
    "href": "seance3.html#analyse-des-pr√©dictions",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "8. Analyse des Pr√©dictions",
    "text": "8. Analyse des Pr√©dictions\n\n8.1 Exemples de Pr√©dictions\n\n# Pr√©dictions avec probabilit√©s\ny_proba = pipeline.predict_proba(X_test)\n\n# Affichage de quelques exemples\nn_samples = 5\nindices = np.random.choice(len(X_test), n_samples, replace=False)\n\nprint(\"Exemples de pr√©dictions:\\n\")\nfor idx in indices:\n    actual = y_test.iloc[idx]\n    predicted = y_test_pred[idx]\n    proba = y_proba[idx]\n    \n    print(f\"Passager {idx}:\")\n    print(f\"  Vraie classe:     {'Survivant' if actual == 1 else 'D√©c√©d√©'}\")\n    print(f\"  Pr√©diction:       {'Survivant' if predicted == 1 else 'D√©c√©d√©'}\")\n    print(f\"  Probabilit√©s:     D√©c√©d√©={proba[0]:.2%}, Survivant={proba[1]:.2%}\")\n    print(f\"  Correct:          {'+' if actual == predicted else '+'}\")\n    print()\n\n\n\n8.2 Analyse des Erreurs\n\n# Identification des erreurs\nerrors = X_test[y_test != y_test_pred].copy()\nerrors['actual'] = y_test[y_test != y_test_pred]\nerrors['predicted'] = y_test_pred[y_test != y_test_pred]\n\nprint(f\"Nombre d'erreurs: {len(errors)}\")\nprint(f\"Taux d'erreur: {len(errors)/len(X_test):.2%}\")\n\nprint(\"\\nQuelques erreurs:\")\nprint(errors.head())\n\n# Analyse des caract√©ristiques des erreurs\nprint(\"\\nCaract√©ristiques moyennes des erreurs vs correctes:\")\ncorrect = X_test[y_test == y_test_pred]\n\ncomparison = pd.DataFrame({\n    'Erreurs': errors.drop(['actual', 'predicted'], axis=1).mean(),\n    'Correctes': correct.mean()\n})\nprint(comparison)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#exercices-pratiques",
    "href": "seance3.html#exercices-pratiques",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "Exercices Pratiques",
    "text": "Exercices Pratiques\n\n\n\n\n\n\nWarningExercice 1: Feature Engineering\n\n\n\n\n\nCr√©ez une nouvelle feature family_size = sibsp + parch + 1, puis r√©-entra√Ænez le mod√®le. La performance s‚Äôam√©liore-t-elle ?\n\nSolution\n\n\nCode\n# Cr√©ation de la nouvelle feature\ndf['family_size'] = df['sibsp'] + df['parch'] + 1\n\n# Refaire le split et l'entra√Ænement\nX_new = df.drop('survived', axis=1)\ny_new = df['survived']\n\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n)\n\npipeline_new = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\n\npipeline_new.fit(X_train_new, y_train_new)\nnew_score = pipeline_new.score(X_test_new, y_test_new)\n\nprint(f\"Accuracy avec family_size: {new_score:.4f}\")\nprint(f\"Accuracy sans family_size: {test_accuracy:.4f}\")\nprint(f\"Am√©lioration: {new_score - test_accuracy:.4f}\")\n\n\n\n\n\n\n\n\n\n\n\n\nWarningExercice 2: Optimisation des Hyperparam√®tres\n\n\n\n\n\nTestez diff√©rentes valeurs de max_depth pour le Decision Tree (3, 5, 7, 10, None). Quelle valeur donne les meilleures performances sur le test set ?\n\nSolution\n\n\nCode\ndepths = [3, 5, 7, 10, None]\nresults_depth = []\n\nfor depth in depths:\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', DecisionTreeClassifier(max_depth=depth, random_state=42))\n    ])\n    \n    pipe.fit(X_train, y_train)\n    train_score = pipe.score(X_train, y_train)\n    test_score = pipe.score(X_test, y_test)\n    \n    results_depth.append({\n        'max_depth': depth,\n        'train': train_score,\n        'test': test_score,\n        'diff': train_score - test_score\n    })\n    \ndf_depth = pd.DataFrame(results_depth)\nprint(df_depth)\n\n# Meilleure valeur\nbest_depth = df_depth.loc[df_depth['test'].idxmax(), 'max_depth']\nprint(f\"\\nMeilleur max_depth: {best_depth}\")\n\n\n\n\n\n\n\n\n\n\n\n\nWarningExercice 3: Analyse d‚ÄôImportance\n\n\n\n\n\nPour le Random Forest, affichez l‚Äôimportance des features. Quelles sont les 3 features les plus importantes ?\n\nSolution\n\n\nCode\n# Entra√Æner Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Importance des features\nimportances = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': rf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Importance des features:\")\nprint(importances)\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nplt.barh(importances['feature'], importances['importance'])\nplt.xlabel('Importance')\nplt.title('Importance des Features - Random Forest')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTop 3 features:\")\nprint(importances.head(3))",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#r√©sum√©-du-tp",
    "href": "seance3.html#r√©sum√©-du-tp",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "R√©sum√© du TP",
    "text": "R√©sum√© du TP\n\n\n\n\n\n\nImportantCe que vous avez appris\n\n\n\n\nChargement et exploration de donn√©es avec pandas\nPr√©traitement des donn√©es:\n\nTraitement des valeurs manquantes\nEncodage des variables cat√©gorielles\nStandardisation\n\nPipeline Scikit-learn pour automatiser le workflow\nSplit Train/Test avec stratification\nEntra√Ænement et √©valuation de mod√®les de classification\nComparaison de plusieurs algorithmes\nAnalyse des r√©sultats et des erreurs",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#checklist-de-validation",
    "href": "seance3.html#checklist-de-validation",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "Checklist de Validation",
    "text": "Checklist de Validation\n\nDataset charg√© et explor√©\nValeurs manquantes trait√©es\nVariables cat√©gorielles encod√©es\nPipeline cr√©√© avec StandardScaler\nMod√®le entra√Æn√© avec succ√®s\nAccuracy calcul√©e (train et test)\nMatrice de confusion g√©n√©r√©e\nComparaison de plusieurs mod√®les effectu√©e\nAnalyse des erreurs r√©alis√©e",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance3.html#pour-aller-plus-loin",
    "href": "seance3.html#pour-aller-plus-loin",
    "title": "S√©ance 3: TP1 - Pipeline de Classification Binaire",
    "section": "Pour Aller Plus Loin",
    "text": "Pour Aller Plus Loin\n\nTestez d‚Äôautres features (titre extrait du nom, cabine, etc.)\nExp√©rimentez avec le seuil de d√©cision (au lieu de 0.5)\nUtilisez la validation crois√©e (voir TP2)\nEssayez d‚Äôautres algorithmes (SVM, Gradient Boosting)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>S√©ance 3: TP1 - Pipeline de Classification Binaire</span>"
    ]
  },
  {
    "objectID": "seance4.html",
    "href": "seance4.html",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "",
    "text": "Introduction\nCe TD vous permet d‚Äôapprofondir votre compr√©hension th√©orique et pratique des principaux algorithmes de classification. Vous allez travailler sur des exercices conceptuels et des probl√®mes appliqu√©s.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#partie-1-arbres-de-d√©cision",
    "href": "seance4.html#partie-1-arbres-de-d√©cision",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Partie 1: Arbres de D√©cision",
    "text": "Partie 1: Arbres de D√©cision\n\nExercice 1.1: Construction d‚Äôun Arbre\nConsid√©rez le dataset suivant pour pr√©dire si un client va acheter un ordinateur:\n\n\n\nAge\nRevenu\n√âtudiant\nCr√©dit\nAch√®te\n\n\n\n\nJeune\n√âlev√©\nNon\nExcellent\nNon\n\n\nJeune\n√âlev√©\nNon\nExcellent\nNon\n\n\nMoyen\n√âlev√©\nNon\nExcellent\nOui\n\n\nSenior\nMoyen\nNon\nExcellent\nOui\n\n\nSenior\nFaible\nOui\nExcellent\nOui\n\n\nSenior\nFaible\nOui\nBon\nNon\n\n\nMoyen\nFaible\nOui\nBon\nOui\n\n\nJeune\nMoyen\nNon\nExcellent\nNon\n\n\nJeune\nFaible\nOui\nExcellent\nOui\n\n\nSenior\nMoyen\nOui\nExcellent\nOui\n\n\nJeune\nMoyen\nOui\nBon\nOui\n\n\nMoyen\nMoyen\nNon\nBon\nOui\n\n\nMoyen\n√âlev√©\nOui\nExcellent\nOui\n\n\nSenior\nMoyen\nNon\nBon\nNon\n\n\n\nQuestions:\n\nCalculez l‚Äôentropie initiale du dataset\nCalculez le gain d‚Äôinformation pour chaque attribut (Age, Revenu, √âtudiant, Cr√©dit)\nQuel attribut sera choisi comme racine de l‚Äôarbre ?\nDessinez l‚Äôarbre de d√©cision complet\n\n\n\n\n\n\n\nTipRappels - Formules et Algorithme\n\n\n\n\n\nEntropie: \\[H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\]\nGain d‚ÄôInformation: \\[IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\\]\no√π:\n\n\\(S\\) = ensemble de donn√©es\n\\(A\\) = attribut\n\\(c\\) = nombre de classes\n\\(p_i\\) = proportion de la classe \\(i\\)\n\\(S_v\\) = sous-ensemble o√π \\(A = v\\)\n\nAlgorithme ID3 (construction de l‚Äôarbre) :\n\nSi tous les exemples appartiennent √† la m√™me classe ‚Üí cr√©er une feuille avec cette classe\nSi plus aucun attribut √† tester ‚Üí cr√©er une feuille avec la classe majoritaire\nSinon :\n\nCalculer \\(IG(S, A)\\) pour chaque attribut \\(A\\)\nChoisir l‚Äôattribut \\(A^*\\) avec le gain d‚Äôinformation maximal\nCr√©er un n≈ìud de d√©cision sur \\(A^*\\)\nPour chaque valeur \\(v\\) de \\(A^*\\) :\n\nCr√©er une branche pour \\(S_v = \\{x \\in S \\mid A^*(x) = v\\}\\)\nAppliquer r√©cursivement l‚Äôalgorithme sur \\(S_v\\) sans l‚Äôattribut \\(A^*\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution Exercice 1.1\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\n# Donn√©es\ndata = [\n    ('Jeune', '√âlev√©', 'Non', 'Excellent', 'Non'),\n    ('Jeune', '√âlev√©', 'Non', 'Excellent', 'Non'),\n    ('Moyen', '√âlev√©', 'Non', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Non', 'Excellent', 'Oui'),\n    ('Senior', 'Faible', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Faible', 'Oui', 'Bon', 'Non'),\n    ('Moyen', 'Faible', 'Oui', 'Bon', 'Oui'),\n    ('Jeune', 'Moyen', 'Non', 'Excellent', 'Non'),\n    ('Jeune', 'Faible', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Oui', 'Excellent', 'Oui'),\n    ('Jeune', 'Moyen', 'Oui', 'Bon', 'Oui'),\n    ('Moyen', 'Moyen', 'Non', 'Bon', 'Oui'),\n    ('Moyen', '√âlev√©', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Non', 'Bon', 'Non')\n]\n\ndef entropy(labels):\n    \"\"\"Calcule l'entropie\"\"\"\n    counter = Counter(labels)\n    total = len(labels)\n    ent = 0\n    for count in counter.values():\n        p = count / total\n        if p &gt; 0:\n            ent -= p * np.log2(p)\n    return ent\n\ndef information_gain(data, attr_idx):\n    \"\"\"Calcule le gain d'information\"\"\"\n    # Entropie initiale\n    labels = [row[-1] for row in data]\n    h_s = entropy(labels)\n    \n    # Partition par attribut\n    partitions = {}\n    for row in data:\n        attr_value = row[attr_idx]\n        if attr_value not in partitions:\n            partitions[attr_value] = []\n        partitions[attr_value].append(row[-1])\n    \n    # Entropie pond√©r√©e\n    h_s_a = 0\n    total = len(data)\n    for partition_labels in partitions.values():\n        p = len(partition_labels) / total\n        h_s_a += p * entropy(partition_labels)\n    \n    return h_s - h_s_a\n\n# 1. Entropie initiale\nlabels = [row[-1] for row in data]\nprint(f\"1. Entropie initiale: {entropy(labels):.4f}\")\n\n# 2. Gain d'information pour chaque attribut\nattributes = ['Age', 'Revenu', '√âtudiant', 'Cr√©dit']\nprint(\"\\n2. Gain d'information:\")\ngains = {}\nfor idx, attr in enumerate(attributes):\n    ig = information_gain(data, idx)\n    gains[attr] = ig\n    print(f\"   {attr}: {ig:.4f}\")\n\n# 3. Meilleur attribut\nbest_attr = max(gains, key=gains.get)\nprint(f\"\\n3. Attribut racine: {best_attr} (IG = {gains[best_attr]:.4f})\")\n\n# 4. L'arbre complet n√©cessiterait une impl√©mentation r√©cursive\nprint(\"\\n4. Arbre de d√©cision (structure simplifi√©e):\")\nprint(\"\"\"\n         Age?\n        /    |    \\\\\n    Jeune  Moyen  Senior\n      |      |      |\n    [Classes selon donn√©es]\n\"\"\")\n\n\nR√©ponses:\n\nEntropie initiale \\(\\approx\\) 0.940\nGains d‚Äôinformation:\n\nAge: ~0.246\nRevenu: ~0.029\n√âtudiant: ~0.151\nCr√©dit: ~0.048\n\nAge sera choisi comme racine (gain le plus √©lev√©)\n\n\n\n\n\n\nExercice 1.2: Overfitting dans les Arbres\nQuestion: Expliquez pourquoi un arbre de d√©cision sans contraintes (profondeur illimit√©e) tend √† faire de l‚Äôoverfitting.\nProposez 3 m√©thodes pour limiter l‚Äôoverfitting dans les arbres de d√©cision.\n\n\n\n\n\n\nNoteSolution Exercice 1.2\n\n\n\n\n\nPourquoi l‚Äôoverfitting ?\nUn arbre sans contraintes va cr√©er des branches jusqu‚Äô√† ce que chaque feuille soit ‚Äúpure‚Äù (contient une seule classe). Cela signifie: - L‚Äôarbre m√©morise les donn√©es d‚Äôentra√Ænement, y compris le bruit - Il cr√©e des r√®gles tr√®s sp√©cifiques qui ne g√©n√©ralisent pas - La complexit√© du mod√®le est trop √©lev√©e par rapport aux donn√©es\n3 m√©thodes pour limiter l‚Äôoverfitting:\n\nPr√©-√©lagage (Pre-pruning):\n\nmax_depth: Limiter la profondeur maximale\nmin_samples_split: Nombre minimum d‚Äô√©chantillons pour diviser un n≈ìud\nmin_samples_leaf: Nombre minimum d‚Äô√©chantillons dans une feuille\nmax_leaf_nodes: Nombre maximum de feuilles\n\nPost-√©lagage (Post-pruning):\n\nConstruire l‚Äôarbre complet\n√âlaguer les branches qui n‚Äôapportent pas assez d‚Äôam√©lioration\nUtiliser un ensemble de validation pour guider l‚Äô√©lagage\n\nEnsemble Methods:\n\nRandom Forest: moyenne de plusieurs arbres\nGradient Boosting: construction it√©rative d‚Äôarbres\nBagging: bootstrap + agr√©gation\n\n\n\n# Exemple pratique\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Arbre avec overfitting\ntree_overfit = DecisionTreeClassifier()  # Pas de contraintes\n\n# Arbre r√©gularis√©\ntree_regularized = DecisionTreeClassifier(\n    max_depth=5,              # Profondeur max\n    min_samples_split=10,     # Min √©chantillons pour split\n    min_samples_leaf=5,       # Min √©chantillons par feuille\n    max_leaf_nodes=20         # Max feuilles\n)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#partie-2-r√©gression-logistique",
    "href": "seance4.html#partie-2-r√©gression-logistique",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Partie 2: R√©gression Logistique",
    "text": "Partie 2: R√©gression Logistique\n\nExercice 2.1: Intuition Probabiliste\nConsid√©rez le mod√®le de r√©gression logistique suivant pour pr√©dire l‚Äôadmission √† l‚Äôuniversit√©:\n\\[P(admission=1|score) = \\frac{1}{1 + e^{-(0.05 \\times score - 3)}}\\]\nQuestions:\n\nQuelle est la probabilit√© d‚Äôadmission pour un score de 60?\nQuelle est la probabilit√© d‚Äôadmission pour un score de 80?\nQuel score donne une probabilit√© d‚Äôadmission de 50%?\nInterpr√©tez le coefficient 0.05\n\n\n\n\n\n\n\nNoteSolution Exercice 2.1\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    \"\"\"Fonction sigmo√Øde\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef prob_admission(score):\n    \"\"\"Probabilit√© d'admission\"\"\"\n    z = 0.05 * score - 3\n    return sigmoid(z)\n\n# 1. Probabilit√© pour score = 60\np_60 = prob_admission(60)\nprint(f\"1. P(admission|score=60) = {p_60:.4f} = {p_60*100:.2f}%\")\n\n# 2. Probabilit√© pour score = 80\np_80 = prob_admission(80)\nprint(f\"2. P(admission|score=80) = {p_80:.4f} = {p_80*100:.2f}%\")\n\n# 3. Score pour P = 0.5\n# 0.5 = 1/(1 + e^(-(0.05*score - 3)))\n# =&gt; 0.05*score - 3 = 0\n# =&gt; score = 60\nscore_50 = 3 / 0.05\nprint(f\"3. Score pour P=50%: {score_50}\")\n\n# 4. Interpr√©tation du coefficient\nprint(f\"\\n4. Coefficient 0.05:\")\nprint(f\"   - Augmenter le score de 1 point augmente z de 0.05\")\nprint(f\"   - Cela augmente les log-odds de 0.05\")\nprint(f\"   - Odds ratio = e^0.05 = {np.exp(0.05):.4f}\")\n\n# Visualisation\nscores = np.linspace(0, 100, 1000)\nprobs = [prob_admission(s) for s in scores]\n\nplt.figure(figsize=(10, 6))\nplt.plot(scores, probs, linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='P = 0.5')\nplt.axvline(x=60, color='r', linestyle='--', alpha=0.7, label='Score = 60')\nplt.scatter([60, 80], [p_60, p_80], color='red', s=100, zorder=5)\nplt.xlabel('Score')\nplt.ylabel('Probabilit√© d\\'admission')\nplt.title('R√©gression Logistique - Admission √† l\\'universit√©')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nR√©ponses:\n\nP(admission|score=60) = 0.50 = 50%\nP(admission|score=80) = 0.73 = 73%\nScore pour P=50%: 60\nLe coefficient 0.05 indique qu‚Äôaugmenter le score de 1 point multiplie les odds d‚Äôadmission par e^0.05 \\(\\approx\\) 1.051 (5.1% d‚Äôaugmentation)\n\n\n\n\n\n\nExercice 2.2: R√©gression Logistique Multiclasse\nExpliquez comment adapter la r√©gression logistique pour un probl√®me multiclasse (ex: 3 classes). Quelles sont les deux approches principales?\n\n\n\n\n\n\nNoteSolution Exercice 2.2\n\n\n\n\n\nDeux approches pour la classification multiclasse:\n\n1. One-vs-Rest (OvR) ou One-vs-All (OvA)\nPrincipe:\n\nEntra√Æner K mod√®les binaires (K = nombre de classes)\nChaque mod√®le s√©pare une classe vs toutes les autres\nPr√©diction: choisir la classe avec la probabilit√© la plus √©lev√©e\n\nExemple avec 3 classes:\n\nMod√®le 1: Classe A vs (B, C)\nMod√®le 2: Classe B vs (A, C)\nMod√®le 3: Classe C vs (A, B)\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# One-vs-Rest (par d√©faut dans scikit-learn)\novr_model = LogisticRegression(multi_class='ovr')\novr_model.fit(X_train, y_train)\n\n\n\n2. Softmax (ou Multinomial)\nPrincipe:\n\nUn seul mod√®le qui produit K probabilit√©s (une par classe)\nUtilise la fonction softmax au lieu de sigmoid\nLes probabilit√©s somment √† 1\n\nFonction Softmax: \\[P(y=k|X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\\]\no√π \\(z_k = w_k^T X + b_k\\)\n\n# Softmax / Multinomial\nsoftmax_model = LogisticRegression(multi_class='multinomial')\nsoftmax_model.fit(X_train, y_train)\n\nComparaison:\n\n\n\nCrit√®re\nOne-vs-Rest\nSoftmax\n\n\n\n\nNombre de mod√®les\nK mod√®les\n1 mod√®le\n\n\nProbabilit√©s\nPeuvent d√©passer 1 (total)\nSomment √† 1\n\n\nEntra√Ænement\nPlus rapide\nPlus lent\n\n\nPerformance\nG√©n√©ralement similaire\nL√©g√®rement meilleur\n\n\nCalibration\nMoins bonne\nMeilleure",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#partie-3-k-nearest-neighbors",
    "href": "seance4.html#partie-3-k-nearest-neighbors",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Partie 3: k-Nearest Neighbors",
    "text": "Partie 3: k-Nearest Neighbors\n\nExercice 3.1: Distance et Voisinage\nConsid√©rez les points suivants dans un espace 2D:\n\n\n\nPoint\nx1\nx2\nClasse\n\n\n\n\nA\n2\n3\nRouge\n\n\nB\n3\n4\nRouge\n\n\nC\n5\n6\nBleu\n\n\nD\n5\n4\nBleu\n\n\nE\n7\n8\nBleu\n\n\n\nNouveau point: P (4, 5)\nQuestions:\n\nCalculez la distance euclidienne entre P et chaque point\nAvec k=3, quelle classe sera pr√©dite pour P?\nQue se passerait-il avec k=5?\nPourquoi est-il important de normaliser les donn√©es avant d‚Äôutiliser k-NN?\n\n\n\n\n\n\n\nNoteSolution Exercice 3.1\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom collections import Counter\n\n# Points\npoints = {\n    'A': ([2, 3], 'Rouge'),\n    'B': ([3, 4], 'Rouge'),\n    'C': ([5, 6], 'Bleu'),\n    'D': ([5, 4], 'Bleu'),\n    'E': ([7, 8], 'Bleu')\n}\n\n# Nouveau point\nP = np.array([4, 5])\n\n# 1. Distances euclidiennes\nprint(\"1. Distances euclidiennes:\")\ndistances = {}\nfor name, (coords, classe) in points.items():\n    dist = np.sqrt(np.sum((np.array(coords) - P)**2))\n    distances[name] = (dist, classe)\n    print(f\"   P ‚Üí {name}: {dist:.4f} (classe: {classe})\")\n\n# 2. k=3\nprint(\"\\n2. k=3:\")\nsorted_distances = sorted(distances.items(), key=lambda x: x[1][0])\nk3_neighbors = sorted_distances[:3]\nk3_classes = [classe for _, (_, classe) in k3_neighbors]\nk3_prediction = Counter(k3_classes).most_common(1)[0][0]\nprint(f\"   3 voisins les plus proches: {[n[0] for n in k3_neighbors]}\")\nprint(f\"   Classes: {k3_classes}\")\nprint(f\"   Pr√©diction: {k3_prediction}\")\n\n# 3. k=5\nprint(\"\\n3. k=5:\")\nk5_neighbors = sorted_distances[:5]\nk5_classes = [classe for _, (_, classe) in k5_neighbors]\nk5_prediction = Counter(k5_classes).most_common(1)[0][0]\nprint(f\"   5 voisins les plus proches: {[n[0] for n in k5_neighbors]}\")\nprint(f\"   Classes: {k5_classes}\")\nprint(f\"   Pr√©diction: {k5_prediction}\")\n\n# 4. Importance de la normalisation\nprint(\"\\n4. Importance de la normalisation:\")\nprint(\"   Sans normalisation, une feature avec une grande √©chelle\")\nprint(\"   dominera le calcul de distance.\")\nprint(\"\\n   Exemple:\")\nprint(\"   - Feature 1 (√¢ge): 20-80 ‚Üí √©chelle ~60\")\nprint(\"   - Feature 2 (revenu): 20000-100000 ‚Üí √©chelle ~80000\")\nprint(\"   ‚Üí La distance sera domin√©e par le revenu!\")\n\n\nR√©ponses:\n\nDistances:\n\nP ‚Üí A: 2.828\nP ‚Üí B: 1.414 (plus proche)\nP ‚Üí C: 1.414 (plus proche)\nP ‚Üí D: 1.414 (plus proche)\nP ‚Üí E: 4.243\n\nAvec k=3: Les 3 voisins sont B (Rouge), C (Bleu), D (Bleu)\n\nVote: 1 Rouge, 2 Bleus\nPr√©diction: Bleu\n\nAvec k=5: Tous les points\n\nVote: 2 Rouges, 3 Bleus\nPr√©diction: Bleu (m√™me r√©sultat)\n\nNormalisation importante car:\n\nLes features avec de grandes valeurs dominent le calcul de distance\nExemple: Si une feature est en milliers et l‚Äôautre en dizaines, la premi√®re √©crasera la seconde\nSolution: StandardScaler ou MinMaxScaler",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#partie-4-naive-bayes",
    "href": "seance4.html#partie-4-naive-bayes",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Partie 4: Naive Bayes",
    "text": "Partie 4: Naive Bayes\n\nExercice 4.1: Application du Th√©or√®me de Bayes\nDataset pour classification de courriels:\n\n\n\nCourriel\n‚Äúgratuit‚Äù\n‚Äúargent‚Äù\n‚Äúviagra‚Äù\nClasse\n\n\n\n\n1\nOui\nNon\nNon\nSpam\n\n\n2\nOui\nOui\nOui\nSpam\n\n\n3\nNon\nNon\nNon\nHam\n\n\n4\nNon\nNon\nNon\nHam\n\n\n5\nOui\nOui\nNon\nSpam\n\n\n\nNouveau courriel contient: ‚Äúgratuit‚Äù et ‚Äúargent‚Äù\nCalculez P(Spam | gratuit, argent) et P(Ham | gratuit, argent)\n\n\n\n\n\n\nNoteSolution Exercice 4.1\n\n\n\n\n\n\n\nCode\n# Donn√©es\nemails = [\n    {'gratuit': 1, 'argent': 0, 'viagra': 0, 'classe': 'Spam'},\n    {'gratuit': 1, 'argent': 1, 'viagra': 1, 'classe': 'Spam'},\n    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n    {'gratuit': 1, 'argent': 1, 'viagra': 0, 'classe': 'Spam'},\n]\n\n# Probabilit√©s a priori\nn_spam = sum(1 for e in emails if e['classe'] == 'Spam')\nn_ham = sum(1 for e in emails if e['classe'] == 'Ham')\ntotal = len(emails)\n\np_spam = n_spam / total\np_ham = n_ham / total\n\nprint(\"Probabilit√©s a priori:\")\nprint(f\"P(Spam) = {n_spam}/{total} = {p_spam}\")\nprint(f\"P(Ham) = {n_ham}/{total} = {p_ham}\")\n\n# Probabilit√©s conditionnelles pour Spam\nspam_emails = [e for e in emails if e['classe'] == 'Spam']\np_gratuit_spam = sum(e['gratuit'] for e in spam_emails) / n_spam\np_argent_spam = sum(e['argent'] for e in spam_emails) / n_spam\n\nprint(f\"\\nP(gratuit|Spam) = {p_gratuit_spam}\")\nprint(f\"P(argent|Spam) = {p_argent_spam}\")\n\n# Probabilit√©s conditionnelles pour Ham\nham_emails = [e for e in emails if e['classe'] == 'Ham']\np_gratuit_ham = sum(e['gratuit'] for e in ham_emails) / n_ham\np_argent_ham = sum(e['argent'] for e in ham_emails) / n_ham\n\nprint(f\"\\nP(gratuit|Ham) = {p_gratuit_ham}\")\nprint(f\"P(argent|Ham) = {p_argent_ham}\")\n\n# Calcul Naive Bayes (hypoth√®se d'ind√©pendance)\n# P(Spam | gratuit, argent) $\\propto$ P(gratuit|Spam) * P(argent|Spam) * P(Spam)\nnumerator_spam = p_gratuit_spam * p_argent_spam * p_spam\nnumerator_ham = p_gratuit_ham * p_argent_ham * p_ham\n\n# Normalisation\np_spam_given_words = numerator_spam / (numerator_spam + numerator_ham)\np_ham_given_words = numerator_ham / (numerator_spam + numerator_ham)\n\nprint(f\"\\nR√©sultats:\")\nprint(f\"P(Spam | gratuit, argent) = {p_spam_given_words:.4f}\")\nprint(f\"P(Ham | gratuit, argent) = {p_ham_given_words:.4f}\")\nprint(f\"\\nPr√©diction: {'Spam' if p_spam_given_words &gt; p_ham_given_words else 'Ham'}\")\n\n\nR√©solution manuelle:\n√âtape 1: Probabilit√©s a priori\n\nP(Spam) = 3/5 = 0.6\nP(Ham) = 2/5 = 0.4\n\n√âtape 2: Probabilit√©s conditionnelles\nPour Spam:\n\nP(gratuit|Spam) = 3/3 = 1.0\nP(argent|Spam) = 2/3 \\(\\approx\\) 0.67\n\nPour Ham:\n\nP(gratuit|Ham) = 0/2 = 0\nP(argent|Ham) = 0/2 = 0\n\n√âtape 3: Application de Bayes\nP(Spam | gratuit, argent) \\(\\propto\\) 1.0 √ó 0.67 √ó 0.6 = 0.4\nP(Ham | gratuit, argent) \\(\\propto\\) 0 √ó 0 √ó 0.4 = 0\nPr√©diction: Spam (avec 100% de confiance)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#partie-5-gradient-boosting-xgboostlightgbm",
    "href": "seance4.html#partie-5-gradient-boosting-xgboostlightgbm",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Partie 5: Gradient Boosting (XGBoost/LightGBM)",
    "text": "Partie 5: Gradient Boosting (XGBoost/LightGBM)\n\nExercice 5.1: Comprendre le Boosting\nQuestions conceptuelles:\n\nQuelle est la diff√©rence fondamentale entre Random Forest (Bagging) et Gradient Boosting?\nPourquoi le Gradient Boosting est-il plus sensible √† l‚Äôoverfitting que Random Forest?\nQuels sont les 3 hyperparam√®tres les plus importants √† ajuster pour XGBoost/LightGBM?\n\n\n\n\n\n\n\nNoteSolution Exercice 5.1\n\n\n\n\n\nüìé Lien du Slide : Gradient Boosting ‚Äî Classification\n1. Diff√©rence Bagging vs Boosting:\n\n\n\n\n\n\n\n\nAspect\nRandom Forest (Bagging)\nGradient Boosting\n\n\n\n\nConstruction\nParall√®le (arbres ind√©pendants)\nS√©quentielle (arbres d√©pendants)\n\n\nObjectif\nR√©duire la variance\nR√©duire le biais\n\n\nDonn√©es\nBootstrap (√©chantillonnage)\nTotalit√© des donn√©es\n\n\nPoids\nTous arbres √©gaux\nArbres pond√©r√©s\n\n\nPr√©diction\nMoyenne simple\nSomme pond√©r√©e\n\n\nFocus\nErreurs al√©atoires\nErreurs r√©siduelles\n\n\n\nDiagramme :\n\n\n\n\n\ngraph LR\n    A[Random Forest] --&gt; B[Arbre 1]\n    A --&gt; C[Arbre 2]\n    A --&gt; D[Arbre N]\n    B --&gt; E[Vote]\n\n\n\n\n\n\n2. Sensibilit√© √† l‚Äôoverfitting:\nGradient Boosting est plus sensible car: - Chaque arbre se concentre sur les erreurs pr√©c√©dentes - Risque d‚Äôapprendre le bruit si trop d‚Äôit√©rations - Peut ‚Äúm√©moriser‚Äù les cas difficiles du train set - Pas de randomisation par d√©faut (contrairement √† RF)\nSolutions: - Limiter le nombre d‚Äôarbres (n_estimators) - R√©duire le taux d‚Äôapprentissage (learning_rate) - Limiter la profondeur (max_depth) - Early stopping avec validation set\n3. Hyperparam√®tres cl√©s:\n\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier(\n    # 1. Nombre d'arbres\n    n_estimators=100,  # Plus = meilleur mais risque overfitting\n    \n    # 2. Taux d'apprentissage\n    learning_rate=0.1,  # Plus faible = besoin de plus d'arbres\n                        # Typage: 0.01-0.3\n    \n    # 3. Profondeur maximale\n    max_depth=6,  # Plus profond = plus complexe\n                  # Typique: 3-10\n    \n    # Bonus importants:\n    subsample=0.8,      # √âchantillonnage des donn√©es (0.5-1.0)\n    colsample_bytree=0.8,  # √âchantillonnage des features\n    min_child_weight=1,    # R√©gularisation\n    \n    random_state=42\n)\n\nRecommandations de tuning:\n\nCommencer avec:\n\nlearning_rate=0.1\nmax_depth=6\nn_estimators=100\n\nPuis optimiser:\n\nAugmenter n_estimators + r√©duire learning_rate\nAjuster max_depth (3-10)\nAjouter r√©gularisation (subsample, colsample_bytree)\n\nUtiliser early stopping:\n\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=10,  # Stop si pas d'am√©lioration\n    verbose=False\n)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#exercices-r√©capitulatifs",
    "href": "seance4.html#exercices-r√©capitulatifs",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Exercices R√©capitulatifs",
    "text": "Exercices R√©capitulatifs\n\n\n\n\n\n\nWarningExercice Final: Choix d‚ÄôAlgorithme\n\n\n\nPour chacun des sc√©narios suivants, recommandez un algorithme et justifiez:\n\nDiagnostic m√©dical (interpr√©tabilit√© cruciale, 1000 patients, 20 features)\nD√©tection de fraude (millions de transactions, temps r√©el, d√©s√©quilibre 99/1)\nClassification d‚Äôimages (50000 images, haute dimension, GPU disponible)\nPr√©diction de churn (10000 clients, features mixtes, besoin de probabilit√©s calibr√©es)\nClassification de textes (emails spam, 100000 emails, features = mots)\n\n\n\n\n\n\n\n\n\nNoteSolution Exercice Final\n\n\n\n\n\n1. Diagnostic m√©dical:\n\nRecommandation: Decision Tree ou R√©gression Logistique\nJustification:\n\nInterpr√©tabilit√© essentielle pour les m√©decins\nDataset de taille mod√©r√©e\nBesoin de comprendre les r√®gles de d√©cision\nAlternative: Random Forest + feature importance\n\n\n2. D√©tection de fraude:\n\nRecommandation: XGBoost/LightGBM\nJustification:\n\nExcellent avec classes d√©s√©quilibr√©es (param√®tre scale_pos_weight)\nTr√®s rapide en pr√©diction (important pour temps r√©el)\nG√®re bien les grandes donn√©es\nRobuste et performant\nPeut utiliser early stopping\n\n\n3. Classification d‚Äôimages:\n\nRecommandation: CNN (Deep Learning) - hors scope pour l‚Äôinstant\nJustification actuelle avec ML classique:\n\nRandom Forest avec features extraites (HOG, SIFT)\nSVM avec kernel RBF\nMais performances limit√©es vs Deep Learning\n\n\n4. Pr√©diction de churn:\n\nRecommandation: R√©gression Logistique ou Gradient Boosting\nJustification:\n\nLogistic Regression: probabilit√©s bien calibr√©es, interpr√©table\nGradient Boosting: meilleures performances, feature importance\nDataset de taille moyenne\nFeatures mixtes g√©r√©es par les deux\n\n\n5. Classification de textes:\n\nRecommandation: Naive Bayes (Multinomial)\nJustification:\n\nTr√®s performant pour la classification de texte\nRapide √† entra√Æner et pr√©dire\nG√®re bien les grandes dimensions (nombreux mots)\nProbabilit√©s natives\nAlternative: R√©gression Logistique",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#r√©sum√©-du-td",
    "href": "seance4.html#r√©sum√©-du-td",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "R√©sum√© du TD",
    "text": "R√©sum√© du TD\n\n\n\n\n\n\nImportantPoints cl√©s √† retenir\n\n\n\nAlgorithmes et leurs forces\n\nDecision Tree\n\n\\(\\checkmark\\) Interpr√©table, visuel\nX Overfitting, instable\n\nRandom Forest\n\n\\(\\checkmark\\) Robuste, performant\nX Moins interpr√©table, m√©moire\n\nSVM\n\n\\(\\checkmark\\) Excellent en haute dimension\nX Lent, difficile √† interpr√©ter\n\nNaive Bayes\n\n\\(\\checkmark\\) Rapide, bon pour texte\nX Hypoth√®se d‚Äôind√©pendance forte\n\nR√©gression Logistique\n\n\\(\\checkmark\\) Probabilit√©s calibr√©es, interpr√©table\nX Assume lin√©arit√©\n\nk-NN\n\n\\(\\checkmark\\) Simple, pas de training\nX Lent en pr√©diction, besoin normalisation\n\nGradient Boosting\n\n\\(\\checkmark\\) Tr√®s performant, g√®re d√©s√©quilibre\nX Sensible overfitting, plus complexe",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#pour-le-prochain-cours",
    "href": "seance4.html#pour-le-prochain-cours",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Pour le Prochain Cours",
    "text": "Pour le Prochain Cours\nPr√©parez-vous pour le TD2 sur les Crit√®res d‚Äô√âvaluation o√π nous approfondirons: - Matrice de confusion - Precision, Recall, F1-score - Courbe ROC et AUC - Choix de m√©triques selon le contexte",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#ressources-compl√©mentaires",
    "href": "seance4.html#ressources-compl√©mentaires",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Ressources Compl√©mentaires",
    "text": "Ressources Compl√©mentaires\n\nScikit-learn: Choosing the right estimator\nStatQuest: Decision Trees\nXGBoost Documentation",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance4.html#correction",
    "href": "seance4.html#correction",
    "title": "S√©ance 4: TD1 - Mod√®les de Classification de Base",
    "section": "Correction",
    "text": "Correction\nüìé Lien de la correction : TD1 ‚Äî Mod√®les de Classification de Base",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>S√©ance 4: TD1 - Mod√®les de Classification de Base</span>"
    ]
  },
  {
    "objectID": "seance5.html",
    "href": "seance5.html",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "",
    "text": "Introduction\nL‚Äô√©valuation correcte d‚Äôun mod√®le de classification est cruciale. Choisir la mauvaise m√©trique peut conduire √† des conclusions erron√©es et √† des mod√®les inadapt√©s en production. Dans ce TD, nous allons explorer en profondeur les diff√©rentes m√©triques d‚Äô√©valuation.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#la-matrice-de-confusion",
    "href": "seance5.html#la-matrice-de-confusion",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "1. La Matrice de Confusion",
    "text": "1. La Matrice de Confusion\n\n1.1 D√©finition et Structure\nLa matrice de confusion est un tableau qui visualise les performances d‚Äôun mod√®le de classification en comparant les pr√©dictions aux vraies valeurs.\nPour un probl√®me binaire:\n                    Pr√©diction\n                 N√©gatif  Positif\nR√©alit√©  N√©gatif    TN       FP\n         Positif    FN       TP\nO√π:\n\nTP (True Positive): Vrais Positifs - correctement classifi√©s comme positifs\nTN (True Negative): Vrais N√©gatifs - correctement classifi√©s comme n√©gatifs\nFP (False Positive): Faux Positifs - incorrectement classifi√©s comme positifs (Erreur Type I)\nFN (False Negative): Faux N√©gatifs - incorrectement classifi√©s comme n√©gatifs (Erreur Type II)\n\n\n\n\n\n\n\nTipMn√©motechnique\n\n\n\n\nType I (FP): Fausse alarme - ‚ÄúOn crie au loup alors qu‚Äôil n‚Äôy a pas de loup‚Äù\nType II (FN): Manque - ‚ÄúOn ne voit pas le loup alors qu‚Äôil est l√†‚Äù\n\n\n\n\n\n1.2 Exemple Concret: D√©tection de Maladie\nConsid√©rez un test m√©dical pour une maladie:\n\n\n\nPatient\nVraie Classe\nPr√©diction\nR√©sultat\n\n\n\n\n1\nMalade\nMalade\nTP \\(\\checkmark\\)\n\n\n2\nMalade\nSain\nFN X (Dangereux!)\n\n\n3\nSain\nMalade\nFP X (Fausse alarme)\n\n\n4\nSain\nSain\nTN \\(\\checkmark\\)\n\n\n5\nMalade\nMalade\nTP \\(\\checkmark\\)\n\n\n6\nSain\nSain\nTN \\(\\checkmark\\)\n\n\n7\nMalade\nSain\nFN X (Dangereux!)\n\n\n8\nSain\nMalade\nFP X (Fausse alarme)\n\n\n\nMatrice de confusion:\n              Pr√©diction\n           Sain  Malade\nR√©alit√© Sain   2      2      (4 sains)\n      Malade   2      2      (4 malades)\n\nTP = 2: Malades correctement d√©tect√©s\nTN = 2: Sains correctement identifi√©s\nFP = 2: Sains diagnostiqu√©s malades (traitement inutile)\nFN = 2: Malades non d√©tect√©s (tr√®s dangereux!)\n\n\n\nExercice 1.1: Construction de Matrice de Confusion\nSoit les pr√©dictions suivantes pour un d√©tecteur de spam (1 = Spam, 0 = Ham):\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\nQuestions: 1. Construisez la matrice de confusion 2. Calculez TP, TN, FP, FN 3. Interpr√©tez chaque type d‚Äôerreur dans ce contexte\n\n\n\n\n\n\nNoteSolution Exercice 1.1\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0])\n\n# 1. Matrice de confusion\ncm = confusion_matrix(y_true, y_pred)\nprint(\"1. Matrice de Confusion:\")\nprint(cm)\n\n# Visualisation\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Ham (0)', 'Spam (1)'],\n            yticklabels=['Ham (0)', 'Spam (1)'])\nplt.ylabel('Vraie Classe')\nplt.xlabel('Pr√©diction')\nplt.title('Matrice de Confusion - D√©tection de Spam')\nplt.tight_layout()\nplt.show()\n\n# 2. Calcul manuel\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\n2. Valeurs:\")\nprint(f\"TP (Vrais Positifs) = {tp}\")\nprint(f\"TN (Vrais N√©gatifs) = {tn}\")\nprint(f\"FP (Faux Positifs) = {fp}\")\nprint(f\"FN (Faux N√©gatifs) = {fn}\")\n\n# V√©rification manuelle\nprint(\"\\n3. Interpr√©tation:\")\nprint(f\"TP = {tp}: Spams correctement d√©tect√©s\")\nprint(f\"TN = {tn}: Hams correctement identifi√©s\")\nprint(f\"FP = {fp}: Hams class√©s comme spam (vont en ind√©sirables)\")\nprint(f\"FN = {fn}: Spams non d√©tect√©s (arrivent en bo√Æte de r√©ception)\")\n\n# Analyse d√©taill√©e\nprint(\"\\nAnalyse d√©taill√©e des pr√©dictions:\")\nfor i, (true, pred) in enumerate(zip(y_true, y_pred)):\n    status = \"\"\n    if true == 1 and pred == 1:\n        status = \"TP $\\checkmark$\"\n    elif true == 0 and pred == 0:\n        status = \"TN $\\checkmark$\"\n    elif true == 0 and pred == 1:\n        status = \"FP X\"\n    elif true == 1 and pred == 0:\n        status = \"FN X\"\n    \n    true_label = \"Spam\" if true == 1 else \"Ham\"\n    pred_label = \"Spam\" if pred == 1 else \"Ham\"\n    print(f\"Email {i+1}: Vrai={true_label}, Pr√©dit={pred_label} ‚Üí {status}\")\n\n\nR√©ponses:\n\nMatrice: [[4, 1], [1, 4]]\nTP=4, TN=4, FP=1, FN=1\nInterpr√©tation:\n\nFP (1 email): Email l√©gitime envoy√© dans spam (utilisateur peut manquer info importante)\nFN (1 email): Spam non d√©tect√© dans bo√Æte r√©ception (nuisance mineure)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#m√©triques-d√©riv√©es",
    "href": "seance5.html#m√©triques-d√©riv√©es",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "2. M√©triques D√©riv√©es",
    "text": "2. M√©triques D√©riv√©es\n\n2.1 Accuracy (Exactitude)\nD√©finition: Proportion de pr√©dictions correctes\n\\[\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\\]\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(y_true, y_pred)\n# ou manuellement:\naccuracy = (tp + tn) / (tp + tn + fp + fn)\n\n\n\n\n\n\n\nWarningPi√®ge de l‚ÄôAccuracy!\n\n\n\nL‚Äôaccuracy peut √™tre trompeuse avec des classes d√©s√©quilibr√©es!\nExemple: D√©tection de fraude\n\n990 transactions l√©gitimes, 10 frauduleuses\nMod√®le na√Øf qui pr√©dit toujours ‚Äúl√©gitime‚Äù\nAccuracy = 990/1000 = 99%\nMais 0% de fraudes d√©tect√©es!\n\n\n\n\n\n2.2 Precision (Pr√©cision)\nD√©finition: Proportion de pr√©dictions positives qui sont correctes\n\\[\\text{Precision} = \\frac{TP}{TP + FP}\\]\nQuestion r√©pondue: ‚ÄúParmi tous les cas pr√©dits positifs, combien le sont vraiment?‚Äù\nInterpr√©tation:\n\nHaute pr√©cision ‚Üí Peu de faux positifs\nImportant quand le co√ªt d‚Äôun FP est √©lev√©\n\n\nfrom sklearn.metrics import precision_score\n\nprecision = precision_score(y_true, y_pred)\n# ou manuellement:\nprecision = tp / (tp + fp)\n\nExemples o√π la Precision est cruciale:\n\nRecommandation de produits: Ne pas recommander des produits non pertinents\nFiltrage spam: Ne pas mettre d‚Äôemails importants dans spam\nD√©tection de visages: Ne pas identifier de faux visages\n\n\n\n2.3 Recall (Rappel) ou Sensitivity (Sensibilit√©)\nD√©finition: Proportion de vrais positifs correctement identifi√©s\n\\[\\text{Recall} = \\frac{TP}{TP + FN}\\]\nQuestion r√©pondue: ‚ÄúParmi tous les cas r√©ellement positifs, combien ai-je d√©tect√©s?‚Äù\nInterpr√©tation:\n\nHaut recall ‚Üí Peu de faux n√©gatifs\nImportant quand le co√ªt d‚Äôun FN est √©lev√©\n\n\nfrom sklearn.metrics import recall_score\n\nrecall = recall_score(y_true, y_pred)\n# ou manuellement:\nrecall = tp / (tp + fn)\n\nExemples o√π le Recall est crucial:\n\nD√©tection de cancer: Ne manquer aucun malade\nD√©tection de fraude: D√©tecter toutes les fraudes\nSyst√®mes de s√©curit√©: Ne rater aucune menace\n\n\n\n2.4 Compromis Precision-Recall\nIl existe g√©n√©ralement un compromis entre Precision et Recall:\nDiagramme mermaid :\n\n\n\n\n\ngraph LR\n    A[Seuil bas0.3] --&gt; B[Haute RecallBasse Precision]\n    C[Seuil moyen0.5] --&gt; D[√âquilibre]\n    E[Seuil haut0.7] --&gt; F[Haute PrecisionBasse Recall]\n\n\n\n\n\n\nExemple concret:\n\n# Mod√®le de d√©tection de maladie\n# Proba patient malade: 0.6\n\n# Seuil = 0.5: Pr√©dit malade ‚Üí Haute recall\n# Seuil = 0.8: Pr√©dit sain ‚Üí Haute precision (mais manque des cas)\n\n\n\n2.5 F1-Score\nD√©finition: Moyenne harmonique de Precision et Recall\n\\[F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\]\nPourquoi moyenne harmonique?\n\nP√©nalise les d√©s√©quilibres\nSi Precision=100% et Recall=1%, F1 \\(\\approx\\) 2% (pas 50.5%)\n\n\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(y_true, y_pred)\n# ou manuellement:\nf1 = 2 * (precision * recall) / (precision + recall)\n\nQuand utiliser F1? - Besoin d‚Äô√©quilibre entre Precision et Recall - Classes d√©s√©quilibr√©es - Vouloir une seule m√©trique r√©sum√©e\n\n\n2.6 Autres M√©triques\nSpecificity (Sp√©cificit√©):\n\\[\\text{Specificity} = \\frac{TN}{TN + FP}\\] Proportion de vrais n√©gatifs correctement identifi√©s\nF-Beta Score:\n\\[F_\\beta = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}\\]\n\n\\(\\beta &gt; 1\\): Favorise le Recall\n\\(\\beta &lt; 1\\): Favorise la Precision\n\n\n\nExercice 2.1: Calcul de M√©triques\nSoit la matrice de confusion suivante pour un d√©tecteur de tumeurs malignes:\n              Pr√©diction\n           B√©nigne  Maligne\nR√©alit√© B√©nigne    850      50\n       Maligne     20       80\nQuestions:\n\nCalculez Accuracy, Precision, Recall, F1-Score\nQuelle m√©trique est la plus importante dans ce contexte? Pourquoi?\nLe mod√®le est-il satisfaisant? Justifiez.\n\n\n\n\n\n\n\nNoteSolution Exercice 2.1\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Matrice de confusion\n#          B√©nigne  Maligne\n# B√©nigne    850      50\n# Maligne     20      80\n\ntn = 850  # B√©nigne correctement identifi√©\nfp = 50   # B√©nigne pr√©dit Maligne\nfn = 20   # Maligne pr√©dit B√©nigne\ntp = 80   # Maligne correctement identifi√©\n\ntotal = tn + fp + fn + tp\n\n# 1. Calcul des m√©triques\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nf1 = 2 * (precision * recall) / (precision + recall)\nspecificity = tn / (tn + fp)\n\nprint(\"1. M√©triques calcul√©es:\")\nprint(f\"Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"Precision:   {precision:.4f} ({precision*100:.2f}%)\")\nprint(f\"Recall:      {recall:.4f} ({recall*100:.2f}%)\")\nprint(f\"F1-Score:    {f1:.4f} ({f1*100:.2f}%)\")\nprint(f\"Specificity: {specificity:.4f} ({specificity*100:.2f}%)\")\n\n# 2. M√©trique la plus importante\nprint(\"\\n2. M√©trique la plus importante: RECALL\")\nprint(\"   Raison: En m√©dical, ne pas d√©tecter un cancer (FN) est\")\nprint(\"   beaucoup plus grave qu'une fausse alarme (FP).\")\nprint(f\"   Actuellement, Recall = {recall:.2%} signifie que\")\nprint(f\"   {fn} cancers sur {tp+fn} ne sont pas d√©tect√©s!\")\n\n# 3. √âvaluation\nprint(\"\\n3. √âvaluation du mod√®le:\")\nprint(f\"   X Recall de {recall:.2%} est INSUFFISANT\")\nprint(f\"   X 20% de cancers manqu√©s = inacceptable\")\nprint(f\"   $\\checkmark$ Precision de {precision:.2%} est correcte\")\nprint(f\"   $\\checkmark$ Specificity de {specificity:.2%} est bonne\")\nprint(\"\\n   CONCLUSION: Mod√®le √† am√©liorer!\")\nprint(\"   Recommandation: Abaisser le seuil de d√©cision pour\")\nprint(\"   augmenter le Recall, m√™me au prix de la Precision.\")\n\n# Impact en nombre de patients\nprint(f\"\\nImpact sur 1000 patients:\")\nprint(f\"   - 20 cancers NON D√âTECT√âS (FN) ‚Üê CRITIQUE\")\nprint(f\"   - 50 fausses alarmes (FP) ‚Üê Acceptable (examens compl√©mentaires)\")\n\n\nR√©ponses:\n\nM√©triques:\n\nAccuracy: 93.00%\nPrecision: 61.54%\nRecall: 80.00%\nF1-Score: 69.57%\nSpecificity: 94.44%\n\nM√©trique importante: RECALL\n\nUn cancer non d√©tect√© (FN) peut √™tre mortel\nUne fausse alarme (FP) ‚Üí examens suppl√©mentaires (acceptable)\nDonc: mieux vaut trop d√©tecter que pas assez\n\n√âvaluation:\n\nX Recall de 80% insuffisant (20% de cancers manqu√©s)\n\\(\\checkmark\\) Specificity correcte\nConclusion: Mod√®le dangereux en l‚Äô√©tat\nAction: R√©duire le seuil pour augmenter Recall",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#courbe-roc-et-auc",
    "href": "seance5.html#courbe-roc-et-auc",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "3. Courbe ROC et AUC",
    "text": "3. Courbe ROC et AUC\n\n3.1 Courbe ROC (Receiver Operating Characteristic)\nLa courbe ROC visualise les performances d‚Äôun classificateur binaire en variant le seuil de d√©cision.\nAxes:\n\nX: False Positive Rate (FPR) = \\(\\frac{FP}{FP + TN}\\) = 1 - Specificity\nY: True Positive Rate (TPR) = \\(\\frac{TP}{TP + FN}\\) = Recall\n\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Obtenir les probabilit√©s (pas les classes)\ny_proba = model.predict_proba(X_test)[:, 1]\n\n# Calculer les points de la courbe ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\n\n# Tracer la courbe\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, linewidth=2, label='ROC Curve')\nplt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.5)')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Courbe ROC')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nInterpr√©tation des points:\n\nPoint (0, 0): Tout pr√©dit n√©gatif (seuil = 1.0)\nPoint (1, 1): Tout pr√©dit positif (seuil = 0.0)\nPoint (0, 1): Classificateur parfait\nDiagonale: Classificateur al√©atoire\n\n\n\n3.2 AUC (Area Under Curve)\nL‚ÄôAUC est l‚Äôaire sous la courbe ROC.\nInterpr√©tation:\n\nAUC = 1.0: Classificateur parfait\nAUC = 0.9 - 1.0: Excellent\nAUC = 0.8 - 0.9: Tr√®s bon\nAUC = 0.7 - 0.8: Bon\nAUC = 0.6 - 0.7: M√©diocre\nAUC = 0.5: Al√©atoire (inutile)\nAUC &lt; 0.5: Pire qu‚Äôal√©atoire (inverser les pr√©dictions!)\n\n\n# Calcul de l'AUC\nauc = roc_auc_score(y_test, y_proba)\nprint(f\"AUC Score: {auc:.4f}\")\n\nAvantages de l‚ÄôAUC:\n\nInd√©pendant du seuil de d√©cision\nRobuste aux classes d√©s√©quilibr√©es\nFacile √† interpr√©ter (une seule valeur)\n\nSignification probabiliste:\nAUC = probabilit√© qu‚Äôun exemple positif al√©atoire ait un score plus √©lev√© qu‚Äôun exemple n√©gatif al√©atoire\n\n\nExercice 3.1: Analyse de Courbe ROC\nVous avez trois mod√®les avec les AUC suivants:\n\nMod√®le A: AUC = 0.95\nMod√®le B: AUC = 0.75\nMod√®le C: AUC = 0.52\n\nQuestions:\n\nClassez les mod√®les par performance\nQuel mod√®le choisiriez-vous pour d√©tecter des fraudes bancaires?\nDans quel cas le Mod√®le C pourrait-il √™tre utile?\n\n\n\n\n\n\n\nNoteSolution Exercice 3.1\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulation de courbes ROC\nnp.random.seed(42)\n\n# Mod√®le A (excellent)\nfpr_a = np.linspace(0, 1, 100)\ntpr_a = np.power(fpr_a, 0.2)  # Courbe tr√®s au-dessus de la diagonale\n\n# Mod√®le B (bon)\nfpr_b = np.linspace(0, 1, 100)\ntpr_b = np.power(fpr_b, 0.6)\n\n# Mod√®le C (quasi-al√©atoire)\nfpr_c = np.linspace(0, 1, 100)\ntpr_c = fpr_c + np.random.normal(0, 0.05, 100)\ntpr_c = np.clip(tpr_c, 0, 1)\n\n# Visualisation\nplt.figure(figsize=(10, 8))\nplt.plot(fpr_a, tpr_a, linewidth=2, label=f'Mod√®le A (AUC=0.95)')\nplt.plot(fpr_b, tpr_b, linewidth=2, label=f'Mod√®le B (AUC=0.75)')\nplt.plot(fpr_c, tpr_c, linewidth=2, label=f'Mod√®le C (AUC=0.52)')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Al√©atoire (AUC=0.5)')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Comparaison des Courbes ROC')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"1. Classement par performance:\")\nprint(\"   1er: Mod√®le A (AUC=0.95) - Excellent\")\nprint(\"   2e:  Mod√®le B (AUC=0.75) - Bon\")\nprint(\"   3e:  Mod√®le C (AUC=0.52) - Quasi-al√©atoire\")\n\nprint(\"\\n2. Pour d√©tecter des fraudes:\")\nprint(\"   ‚Üí Mod√®le A (AUC=0.95)\")\nprint(\"   Raison: Meilleure capacit√© √† distinguer fraude/l√©gitime\")\nprint(\"   Permet d'ajuster le seuil selon co√ªt FP vs FN\")\n\nprint(\"\\n3. Utilit√© du Mod√®le C:\")\nprint(\"   ‚Üí Quasiment aucune!\")\nprint(\"   AUC=0.52 $\\approx$ tirage √† pile ou face\")\nprint(\"   SAUF: Si on inverse ses pr√©dictions ‚Üí AUC=0.48\")\nprint(\"   ‚Üí Peut indiquer un bug dans le code/labels\")\n\n\nR√©ponses:\n\nClassement: A &gt; B &gt; C\n\nA est excellent\nB est bon mais moins performant\nC est pratiquement inutile\n\nFraude bancaire: Mod√®le A\n\nAUC √©lev√© = meilleure discrimination\nImportant car co√ªts √©lev√©s (fraudes manqu√©es + fausses alarmes)\nPermet de choisir le seuil optimal selon les co√ªts m√©tier\n\nUtilit√© de C:\n\nPratiquement aucune (performance al√©atoire)\nPourrait indiquer un probl√®me (bug, labels invers√©s, features non pertinentes)\nSi AUC &lt; 0.5: inverser les pr√©dictions pourrait aider!",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#choix-de-m√©triques-selon-le-contexte",
    "href": "seance5.html#choix-de-m√©triques-selon-le-contexte",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "4. Choix de M√©triques selon le Contexte",
    "text": "4. Choix de M√©triques selon le Contexte\n\n4.1 Tableau de D√©cision\n\n\n\n\n\n\n\n\nContexte\nM√©trique Principale\nRaison\n\n\n\n\nD√©tection de spam\nPrecision\n√âviter de perdre emails importants (FP co√ªteux)\n\n\nD√©tection de cancer\nRecall\nNe manquer aucun malade (FN critique)\n\n\nMoteur de recherche\nPrecision@K\nPremiers r√©sultats doivent √™tre pertinents\n\n\nFraude bancaire\nF1 ou AUC\n√âquilibre entre d√©tecter fraudes et √©viter fausses alarmes\n\n\nSyst√®me de recommandation\nPrecision\nRecommandations doivent √™tre pertinentes\n\n\nD√©tection d‚Äôintrusion r√©seau\nRecall\nNe rater aucune attaque (FN dangereux)\n\n\nDiagnostic automatis√©\nRecall + Precision\nLes deux sont importants (vies humaines)\n\n\n\n\n\n4.2 Classes D√©s√©quilibr√©es\nLorsque les classes sont tr√®s d√©s√©quilibr√©es (ex: 1% positifs, 99% n√©gatifs):\n‚ùå √Ä √âVITER:\n\nAccuracy (trompeuse)\n\n‚úÖ √Ä UTILISER:\n\nPrecision, Recall, F1-Score\nAUC-ROC\nPrecision-Recall curve\nBalanced Accuracy = \\(\\frac{\\text{Recall} + \\text{Specificity}}{2}\\)\n\n\nfrom sklearn.metrics import balanced_accuracy_score\n\n# Accuracy normale (biais√©e si d√©s√©quilibre)\nacc = accuracy_score(y_true, y_pred)\n\n# Balanced accuracy (moyenne de recall et specificity)\nbalanced_acc = balanced_accuracy_score(y_true, y_pred)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n\n\n\nExercice 4.1: Choix de M√©trique\nPour chaque sc√©nario, choisissez la m√©trique la plus appropri√©e et justifiez:\n\nSyst√®me de reconnaissance faciale pour d√©verrouillage de t√©l√©phone\nFiltre anti-spam Gmail\nD√©tection de COVID-19 par test rapide\nSyst√®me de recommandation Netflix\nD√©tection de transactions frauduleuses (0.1% de fraudes)\n\n\n\n\n\n\n\nNoteSolution Exercice 4.1\n\n\n\n\n\n1. Reconnaissance faciale pour t√©l√©phone: - M√©trique: Precision (prioritaire) + FPR faible - Justification:\n\nFP = Personne non autoris√©e acc√®de au t√©l√©phone (GRAVE - s√©curit√©)\nFN = Propri√©taire doit retenter (MINEUR - inconv√©nient)\nMieux vaut bloquer le propri√©taire parfois que laisser entrer un intrus\n\n2. Filtre anti-spam Gmail:\n\nM√©trique: Precision (prioritaire)\nJustification:\n\nFP = Email important dans spam (GRAVE - peut manquer info critique)\nFN = Spam en bo√Æte de r√©ception (MINEUR - simple nuisance)\nGmail pr√©f√®re laisser passer du spam que bloquer des emails l√©gitimes\n\n\n3. D√©tection COVID-19:\n\nM√©trique: Recall (prioritaire)\nJustification:\n\nFN = Malade non d√©tect√© ‚Üí propage le virus (GRAVE - sant√© publique)\nFP = Personne saine isol√©e inutilement (MINEUR - test confirmatoire possible)\nMieux vaut trop d√©tecter que pas assez\n\n\n4. Recommandation Netflix:\n\nM√©trique: Precision@K (K = nombre de recommandations affich√©es)\nJustification:\n\nUtilisateur voit seulement les K premi√®res recommandations\nCelles-ci doivent √™tre pertinentes pour satisfaction client\nRecall moins important (on ne peut pas tout recommander)\n\n\n5. Fraude bancaire (0.1% de fraudes):\n\nM√©triques: F1-Score + AUC-ROC + Precision-Recall curve\nJustification:\n\nClasses tr√®s d√©s√©quilibr√©es ‚Üí Accuracy inutile\nFP = Client bloqu√© √† tort (co√ªteux - insatisfaction)\nFN = Fraude non d√©tect√©e (tr√®s co√ªteux - pertes financi√®res)\nBesoin d‚Äô√©quilibre ‚Üí F1 ou optimiser selon co√ªts m√©tier\nAUC pour comparer mod√®les ind√©pendamment du seuil",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#exercice-r√©capitulatif-complet",
    "href": "seance5.html#exercice-r√©capitulatif-complet",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "5. Exercice R√©capitulatif Complet",
    "text": "5. Exercice R√©capitulatif Complet\n\nSc√©nario: D√©tecteur de D√©fauts en Usine\nUne usine produit 10,000 pi√®ces par jour. En moyenne, 100 pi√®ces (1%) sont d√©fectueuses.\nVous avez d√©velopp√© un mod√®le de d√©tection automatique qui donne les r√©sultats suivants sur 1000 pi√®ces de test:\n              Pr√©diction\n           OK    D√©fectueux\nR√©alit√© OK    970      20\n     D√©fect.   3       7\nCo√ªts:\n\nPi√®ce d√©fectueuse non d√©tect√©e (FN): 500‚Ç¨ (vendue puis retourn√©e par client)\nPi√®ce OK rejet√©e √† tort (FP): 10‚Ç¨ (inspection manuelle inutile)\nInspection manuelle d‚Äôune pi√®ce: 5‚Ç¨\n\nQuestions:\n\nCalculez toutes les m√©triques (Accuracy, Precision, Recall, F1, Specificity)\nLe mod√®le est-il acceptable d‚Äôun point de vue m√©tier?\nCalculez le co√ªt total journalier des erreurs\nQuelle m√©trique devriez-vous optimiser en priorit√©?\nProposez une am√©lioration du mod√®le\n\n\n\n\n\n\n\nNoteSolution Exercice R√©capitulatif\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Matrice de confusion\n#           OK    D√©fect\n# OK       970      20\n# D√©fect     3       7\n\ntn, fp = 970, 20\nfn, tp = 3, 7\n\ntotal = tn + fp + fn + tp\n\nprint(\"=\" * 60)\nprint(\"ANALYSE DU D√âTECTEUR DE D√âFAUTS\")\nprint(\"=\" * 60)\n\n# 1. M√©triques\nprint(\"\\n1. M√âTRIQUES DE PERFORMANCE:\")\nprint(\"-\" * 60)\n\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nf1 = 2 * (precision * recall) / (precision + recall)\nspecificity = tn / (tn + fp)\nfpr = fp / (fp + tn)\nfnr = fn / (fn + tp)\n\nmetrics = {\n    'Accuracy': accuracy,\n    'Precision': precision,\n    'Recall': recall,\n    'F1-Score': f1,\n    'Specificity': specificity,\n    'False Positive Rate': fpr,\n    'False Negative Rate': fnr\n}\n\nfor metric, value in metrics.items():\n    print(f\"{metric:25s}: {value:.4f} ({value*100:6.2f}%)\")\n\n# 2. Acceptabilit√© m√©tier\nprint(\"\\n2. ACCEPTABILIT√â M√âTIER:\")\nprint(\"-\" * 60)\nprint(f\"X Recall = {recall:.2%} est TR√àS FAIBLE\")\nprint(f\"  ‚Üí Sur 10 d√©fauts, seulement {tp} d√©tect√©s, {fn} manqu√©s!\")\nprint(f\"  ‚Üí {fnr:.2%} de d√©fauts passent inaper√ßus\")\nprint(f\"\\n$\\checkmark$ Precision = {precision:.2%} est acceptable\")\nprint(f\"  ‚Üí Peu de fausses alarmes\")\nprint(f\"\\nX CONCLUSION: Mod√®le INACCEPTABLE\")\nprint(f\"  ‚Üí Trop de d√©fauts non d√©tect√©s atteignent les clients\")\n\n# 3. Co√ªt journalier\nprint(\"\\n3. CO√õT TOTAL JOURNALIER:\")\nprint(\"-\" * 60)\n\n# Extrapolation √† 10,000 pi√®ces\ntotal_pieces = 10000\ndefect_rate = 0.01\ntotal_defects = int(total_pieces * defect_rate)  # 100 d√©fauts\ntotal_ok = total_pieces - total_defects  # 9900 OK\n\n# Calcul des erreurs attendues\nexpected_fn = int(total_defects * fnr)  # D√©fauts non d√©tect√©s\nexpected_fp = int(total_ok * fpr)  # OK rejet√©s √† tort\n\ncout_fn = expected_fn * 500  # D√©fauts non d√©tect√©s\ncout_fp = expected_fp * 10   # OK rejet√©s\ncout_inspection = (expected_fn + expected_fp) * 0  # Pas d'inspection pour erreurs\n\ncout_total = cout_fn + cout_fp\n\nprint(f\"Production journali√®re:     {total_pieces:,} pi√®ces\")\nprint(f\"D√©fauts r√©els:              {total_defects} pi√®ces ({defect_rate:.1%})\")\nprint(f\"\\nErreurs attendues:\")\nprint(f\"  - FN (d√©fauts manqu√©s):   {expected_fn} √ó 500‚Ç¨ = {cout_fn:,}‚Ç¨\")\nprint(f\"  - FP (OK rejet√©s):        {expected_fp} √ó 10‚Ç¨  = {cout_fp:,}‚Ç¨\")\nprint(f\"\\nCo√ªt total des erreurs:     {cout_total:,}‚Ç¨/jour\")\nprint(f\"Co√ªt mensuel (22 jours):    {cout_total*22:,}‚Ç¨/mois\")\nprint(f\"Co√ªt annuel (250 jours):    {cout_total*250:,}‚Ç¨/an\")\n\n# 4. M√©trique √† optimiser\nprint(\"\\n4. M√âTRIQUE √Ä OPTIMISER:\")\nprint(\"-\" * 60)\nprint(\"‚Üí RECALL (priorit√© absolue)\")\nprint(\"\\nRaison: Le co√ªt d'un FN (500‚Ç¨) est 50√ó plus √©lev√©\")\nprint(\"        que le co√ªt d'un FP (10‚Ç¨)\")\nprint(f\"\\nRatio co√ªt FN/FP: {500/10:.0f}:1\")\nprint(\"\\nStrat√©gie: Maximiser le Recall, m√™me au prix de\")\nprint(\"           la Precision (plus de fausses alarmes acceptable)\")\n\n# 5. Am√©lioration\nprint(\"\\n5. PROPOSITIONS D'AM√âLIORATION:\")\nprint(\"-\" * 60)\nprint(\"\\nA. Court terme (ajustement du seuil):\")\nprint(\"   1. Abaisser le seuil de d√©cision (ex: 0.5 ‚Üí 0.2)\")\nprint(\"   2. Objectif: Recall &gt; 90%\")\nprint(\"   3. Cons√©quence: Plus de FP (mais co√ªt acceptable)\")\n\n# Simulation avec recall am√©lior√©\nrecall_target = 0.90\nfn_new = int(total_defects * (1 - recall_target))  # 10 d√©fauts manqu√©s\n# Supposons que Precision baisse √† 20%\nprecision_new = 0.20\nfp_new = int((total_defects * recall_target) / precision_new * (1 - precision_new))\n\ncout_fn_new = fn_new * 500\ncout_fp_new = fp_new * 10\ncout_total_new = cout_fn_new + cout_fp_new\n\nprint(f\"\\n   Simulation avec Recall=90%, Precision=20%:\")\nprint(f\"   - FN: {fn_new} √ó 500‚Ç¨ = {cout_fn_new:,}‚Ç¨\")\nprint(f\"   - FP: {fp_new} √ó 10‚Ç¨ = {cout_fp_new:,}‚Ç¨\")\nprint(f\"   - Co√ªt total: {cout_total_new:,}‚Ç¨/jour\")\nprint(f\"   - √âconomie: {cout_total - cout_total_new:,}‚Ç¨/jour\")\nprint(f\"   - ROI annuel: {(cout_total - cout_total_new)*250:,}‚Ç¨\")\n\nprint(\"\\nB. Moyen terme (am√©lioration du mod√®le):\")\nprint(\"   1. Collecter plus de donn√©es (surtout d√©fauts)\")\nprint(\"   2. Feature engineering (nouvelles caract√©ristiques)\")\nprint(\"   3. Essayer d'autres algorithmes (XGBoost, CNN)\")\nprint(\"   4. Data augmentation pour la classe minoritaire\")\n\nprint(\"\\nC. Long terme (approche hybride):\")\nprint(\"   1. Mod√®le ML comme premier filtre (Recall √©lev√©)\")\nprint(\"   2. Inspection manuelle des cas suspects\")\nprint(\"   3. Double v√©rification pour cas limites\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Matrice de confusion\ncm = np.array([[tn, fp], [fn, tp]])\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n            xticklabels=['OK', 'D√©fectueux'],\n            yticklabels=['OK', 'D√©fectueux'])\naxes[0].set_ylabel('Vraie Classe')\naxes[0].set_xlabel('Pr√©diction')\naxes[0].set_title('Matrice de Confusion')\n\n# Comparaison co√ªts\nscenarios = ['Mod√®le Actuel\\n(Recall=70%)', \n             'Mod√®le Am√©lior√©\\n(Recall=90%)']\ncouts = [cout_total, cout_total_new]\ncolors = ['#ff6b6b', '#51cf66']\n\naxes[1].bar(scenarios, couts, color=colors, alpha=0.7)\naxes[1].set_ylabel('Co√ªt journalier (‚Ç¨)')\naxes[1].set_title('Comparaison des Co√ªts')\naxes[1].grid(axis='y', alpha=0.3)\n\nfor i, (scenario, cout) in enumerate(zip(scenarios, couts)):\n    axes[1].text(i, cout + 100, f'{cout:,}‚Ç¨', \n                ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Tableau r√©capitulatif\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TABLEAU R√âCAPITULATIF\")\nprint(\"=\" * 60)\n\ncomparison = pd.DataFrame({\n    'M√©trique': ['Recall', 'Precision', 'FN/jour', 'FP/jour', 'Co√ªt/jour'],\n    'Actuel': [f'{recall:.1%}', f'{precision:.1%}', expected_fn, expected_fp, f'{cout_total:,}‚Ç¨'],\n    'Cible': ['90%', '20%', fn_new, fp_new, f'{cout_total_new:,}‚Ç¨'],\n    'Am√©lioration': ['‚Üë +20pp', '‚Üì -6pp', f'‚Üì -{expected_fn-fn_new}', f'‚Üë +{fp_new-expected_fp}', \n                     f'‚Üì -{cout_total-cout_total_new:,}‚Ç¨']\n})\n\nprint(comparison.to_string(index=False))\nprint(\"=\" * 60)\n\n\nSynth√®se des r√©ponses:\n1. M√©triques:\n\nAccuracy: 97.70%\nPrecision: 25.93% (7 sur 27 pr√©dictions de d√©fauts sont correctes)\nRecall: 70.00% (7 sur 10 vrais d√©fauts d√©tect√©s)\nF1-Score: 38.89%\nSpecificity: 97.98%\n\n2. Acceptabilit√© m√©tier:\n\n‚ùå NON ACCEPTABLE\nRecall de 70% signifie 30% de d√©fauts non d√©tect√©s\nCes d√©fauts atteignent les clients ‚Üí r√©putation + co√ªts\n\n3. Co√ªt journalier:\n\nFN: 30 d√©fauts √ó 500‚Ç¨ = 15,000‚Ç¨/jour\nFP: 200 pi√®ces √ó 10‚Ç¨ = 2,000‚Ç¨/jour\nTotal: 17,000‚Ç¨/jour (\\(\\approx\\) 4.25M‚Ç¨/an!)\n\n4. M√©trique √† optimiser:\n\nRECALL (priorit√© absolue)\nRatio co√ªt 50:1 en faveur de l‚Äôaugmentation du Recall\nAccepter plus de FP pour r√©duire FN\n\n5. Am√©lioration propos√©e:\n\nAbaisser seuil ‚Üí Recall 90%+\n√âconomie potentielle: ~10,000‚Ç¨/jour\nROI annuel: ~2.5M‚Ç¨",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#comparaison-critique-des-r√©sultats",
    "href": "seance5.html#comparaison-critique-des-r√©sultats",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "6. Comparaison Critique des R√©sultats",
    "text": "6. Comparaison Critique des R√©sultats\n\nExercice 6.1: Analyse Comparative\nTrois data scientists ont entra√Æn√© des mod√®les pour d√©tecter des d√©faillances machines:\n\n\n\nMod√®le\nAccuracy\nPrecision\nRecall\nF1\nAUC\n\n\n\n\nA\n98%\n40%\n95%\n56%\n0.92\n\n\nB\n95%\n80%\n70%\n75%\n0.88\n\n\nC\n99%\n20%\n60%\n30%\n0.75\n\n\n\nContexte: 2% de d√©faillances, co√ªt FN = 10,000‚Ç¨, co√ªt FP = 100‚Ç¨\nQuestions:\n\nQuel mod√®le recommandez-vous?\nJustifiez votre choix avec calculs de co√ªts\nQue r√©v√®le l‚ÄôAccuracy √©lev√©e du mod√®le C?\n\n\n\n\n\n\n\nNoteSolution Exercice 6.1\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Donn√©es\nmodels_data = {\n    'Mod√®le': ['A', 'B', 'C'],\n    'Accuracy': [0.98, 0.95, 0.99],\n    'Precision': [0.40, 0.80, 0.20],\n    'Recall': [0.95, 0.70, 0.60],\n    'F1': [0.56, 0.75, 0.30],\n    'AUC': [0.92, 0.88, 0.75]\n}\n\ndf_models = pd.DataFrame(models_data)\n\n# Contexte\ndefect_rate = 0.02\ntotal_samples = 10000\ntotal_defects = int(total_samples * defect_rate)  # 200\ntotal_normal = total_samples - total_defects  # 9800\n\ncost_fn = 10000  # Co√ªt d√©faillance non d√©tect√©e\ncost_fp = 100    # Co√ªt fausse alarme\n\nprint(\"ANALYSE COMPARATIVE DES MOD√àLES\")\nprint(\"=\" * 70)\n\n# Calcul des co√ªts pour chaque mod√®le\nresults = []\n\nfor idx, row in df_models.iterrows():\n    model = row['Mod√®le']\n    recall = row['Recall']\n    precision = row['Precision']\n    \n    # Calcul des erreurs\n    fn = int(total_defects * (1 - recall))  # D√©fauts manqu√©s\n    tp = total_defects - fn  # D√©fauts d√©tect√©s\n    \n    # De precision = TP / (TP + FP), on d√©duit FP\n    if precision &gt; 0:\n        fp = int(tp / precision - tp)  # Fausses alarmes\n    else:\n        fp = 0\n    \n    # Co√ªts\n    cost_fn_total = fn * cost_fn\n    cost_fp_total = fp * cost_fp\n    cost_total = cost_fn_total + cost_fp_total\n    \n    results.append({\n        'Mod√®le': model,\n        'FN': fn,\n        'FP': fp,\n        'Co√ªt FN': cost_fn_total,\n        'Co√ªt FP': cost_fp_total,\n        'Co√ªt Total': cost_total\n    })\n    \n    print(f\"\\nMod√®le {model}:\")\n    print(f\"  Recall: {recall:.0%} ‚Üí FN = {fn} d√©faillances manqu√©es\")\n    print(f\"  Precision: {precision:.0%} ‚Üí FP = {fp} fausses alarmes\")\n    print(f\"  Co√ªt FN: {fn} √ó {cost_fn:,}‚Ç¨ = {cost_fn_total:,}‚Ç¨\")\n    print(f\"  Co√ªt FP: {fp} √ó {cost_fp:,}‚Ç¨ = {cost_fp_total:,}‚Ç¨\")\n    print(f\"  CO√õT TOTAL: {cost_total:,}‚Ç¨\")\n\ndf_results = pd.DataFrame(results)\n\n# Meilleur mod√®le\nbest_model = df_results.loc[df_results['Co√ªt Total'].idxmin()]\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RECOMMANDATION\")\nprint(\"=\" * 70)\nprint(f\"‚Üí Mod√®le {best_model['Mod√®le']} (co√ªt le plus faible)\")\nprint(f\"\\nJustification:\")\nprint(f\"  - Co√ªt total minimal: {best_model['Co√ªt Total']:,}‚Ç¨\")\nprint(f\"  - Recall √©lev√© ({df_models[df_models['Mod√®le']==best_model['Mod√®le']]['Recall'].values[0]:.0%}) critique car FN tr√®s co√ªteux\")\nprint(f\"  - Le surco√ªt en FP est n√©gligeable compar√© aux √©conomies en FN\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANALYSE DU MOD√àLE C\")\nprint(\"=\" * 70)\nprint(\"Accuracy de 99% mais performances m√©diocres:\")\nprint(\"  ‚Üí Pi√®ge des classes d√©s√©quilibr√©es!\")\nprint(f\"  ‚Üí Avec 2% de d√©fauts, pr√©dire 'normal' partout\")\nprint(f\"     donnerait d√©j√† 98% d'accuracy\")\nprint(\"  ‚Üí Recall de 60% = 40% de d√©fauts manqu√©s (inacceptable)\")\nprint(\"  ‚Üí AUC faible (0.75) confirme faible capacit√© discriminative\")\nprint(\"\\n  Conclusion: Accuracy est une m√©trique TROMPEUSE ici!\")\n\n# Visualisations\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Graphique 1: Comparaison des co√ªts\nx = df_results['Mod√®le']\nwidth = 0.35\n\nx_pos = range(len(x))\naxes[0].bar([p - width/2 for p in x_pos], df_results['Co√ªt FN'], \n           width, label='Co√ªt FN', color='#ff6b6b', alpha=0.8)\naxes[0].bar([p + width/2 for p in x_pos], df_results['Co√ªt FP'], \n           width, label='Co√ªt FP', color='#ffa94d', alpha=0.8)\n\naxes[0].set_xlabel('Mod√®le')\naxes[0].set_ylabel('Co√ªt (‚Ç¨)')\naxes[0].set_title('Comparaison des Co√ªts par Type d\\'Erreur')\naxes[0].set_xticks(x_pos)\naxes[0].set_xticklabels(x)\naxes[0].legend()\naxes[0].grid(axis='y', alpha=0.3)\n\n# Ajouter co√ªt total\nfor i, (idx, row) in enumerate(df_results.iterrows()):\n    total = row['Co√ªt Total']\n    axes[0].text(i, total + 5000, f'{total:,}‚Ç¨', \n                ha='center', fontweight='bold', fontsize=10)\n\n# Graphique 2: Radar chart des m√©triques\nfrom math import pi\n\ncategories = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\nN = len(categories)\n\nangles = [n / float(N) * 2 * pi for n in range(N)]\nangles += angles[:1]\n\nax = plt.subplot(122, projection='polar')\n\nfor idx, row in df_models.iterrows():\n    values = [row['Accuracy'], row['Precision'], row['Recall'], \n              row['F1'], row['AUC']]\n    values += values[:1]\n    \n    ax.plot(angles, values, 'o-', linewidth=2, label=f\"Mod√®le {row['Mod√®le']}\")\n    ax.fill(angles, values, alpha=0.15)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(categories)\nax.set_ylim(0, 1)\nax.set_title('Comparaison Multidimensionnelle des M√©triques', y=1.08)\nax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\nax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Tableau final\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TABLEAU R√âCAPITULATIF\")\nprint(\"=\" * 70)\nprint(df_results.to_string(index=False))\n\n\nR√©ponses:\n\nMod√®le recommand√©: A\nJustification avec co√ªts:\n\nMod√®le A: 10 FN √ó 10,000‚Ç¨ + 475 FP √ó 100‚Ç¨ = 147,500‚Ç¨\nMod√®le B: 60 FN √ó 10,000‚Ç¨ + 175 FP √ó 100‚Ç¨ = 617,500‚Ç¨\nMod√®le C: 80 FN √ó 10,000‚Ç¨ + 600 FP √ó 100‚Ç¨ = 860,000‚Ç¨\n\nLe Mod√®le A minimise le co√ªt total malgr√© plus de FP\nAccuracy √©lev√©e du Mod√®le C:\n\nPi√®ge classique des classes d√©s√©quilibr√©es!\n99% accuracy car pr√©dit surtout ‚Äúnormal‚Äù\nMais rate 40% des d√©faillances (Recall = 60%)\nLe√ßon: Accuracy seule est trompeuse avec d√©s√©quilibre",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#r√©sum√©-de-la-s√©ance",
    "href": "seance5.html#r√©sum√©-de-la-s√©ance",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "R√©sum√© de la S√©ance",
    "text": "R√©sum√© de la S√©ance\n\n\n\n\n\n\nImportantPoints cl√©s √† retenir\n\n\n\n\n1. Matrice de Confusion\n\nFondation de toutes les m√©triques\nTP, TN, FP, FN √† bien comprendre\nVisualisation claire des erreurs\n\n\n\n2. M√©triques Principales\n\nAccuracy: % correct (attention au d√©s√©quilibre!)\nPrecision: Fiabilit√© des pr√©dictions positives\nRecall: Couverture des vrais positifs\nF1-Score: √âquilibre Precision-Recall\n\n\n\n3. Compromis\n\nPrecision ‚Üë ‚Üí Recall ‚Üì (g√©n√©ralement)\nChoix selon le co√ªt des erreurs\nAjustement du seuil de d√©cision\n\n\n\n4. Courbe ROC et AUC\n\n√âvaluation ind√©pendante du seuil\nAUC = mesure globale de discrimination\nComparaison facile de mod√®les\n\n\n\n5. Choix Contextuel\n\nPas de m√©trique universelle!\nD√©pend du probl√®me m√©tier\nConsid√©rer les co√ªts r√©els des erreurs\nClasses d√©s√©quilibr√©es ‚Üí √©viter Accuracy seule\n\n\n\n6. R√®gles d‚ÄôOr\n\nToujours regarder plusieurs m√©triques\nPrivil√©gier Recall si FN co√ªteux\nPrivil√©gier Precision si FP co√ªteux\nUtiliser AUC pour comparer mod√®les\nValider avec co√ªts m√©tier r√©els",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#checklist-de-ma√Ætrise",
    "href": "seance5.html#checklist-de-ma√Ætrise",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "Checklist de Ma√Ætrise",
    "text": "Checklist de Ma√Ætrise\n\nJe sais construire et interpr√©ter une matrice de confusion\nJe comprends la diff√©rence entre Precision et Recall\nJe peux expliquer pourquoi Accuracy peut √™tre trompeuse\nJe sais calculer manuellement les m√©triques de base\nJe comprends le compromis Precision-Recall\nJe sais interpr√©ter une courbe ROC et l‚ÄôAUC\nJe peux choisir la m√©trique appropri√©e selon le contexte\nJe comprends l‚Äôimpact des classes d√©s√©quilibr√©es",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#exercices-suppl√©mentaires",
    "href": "seance5.html#exercices-suppl√©mentaires",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "Exercices Suppl√©mentaires",
    "text": "Exercices Suppl√©mentaires\n\n\n\n\n\n\nWarningPour s‚Äôentra√Æner\n\n\n\n\nImpl√©mentez toutes les m√©triques vues en Python sans Scikit-learn\nCr√©ez une fonction qui recommande la meilleure m√©trique selon le contexte\nAnalysez un dataset d√©s√©quilibr√© (ex: fraude, maladie rare) sur Kaggle\nComparez plusieurs mod√®les sur le m√™me probl√®me avec toutes les m√©triques\nExplorez l‚Äôeffet du seuil de d√©cision sur Precision et Recall",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#pr√©paration-s√©ance-suivante",
    "href": "seance5.html#pr√©paration-s√©ance-suivante",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "Pr√©paration S√©ance Suivante",
    "text": "Pr√©paration S√©ance Suivante\nLa S√©ance 6 (TP2) abordera:\n\nClassification multi-classes\nOptimisation d‚Äôhyperparam√®tres (GridSearchCV, RandomizedSearchCV)\nValidation crois√©e\nComparaison avanc√©e de mod√®les\n\n√Ä pr√©parer:\n\nRelire les concepts de validation crois√©e\nInstaller scikit-learn √† jour\nR√©fl√©chir aux hyperparam√®tres importants de chaque algorithme",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance5.html#ressources-compl√©mentaires",
    "href": "seance5.html#ressources-compl√©mentaires",
    "title": "S√©ance 5: TD2 - Crit√®res d‚Äô√âvaluation",
    "section": "Ressources Compl√©mentaires",
    "text": "Ressources Compl√©mentaires\n\nScikit-learn: Model Evaluation\nGoogle ML Crash Course: Classification\nStatQuest: Sensitivity and Specificity\nTowards Data Science: Beyond Accuracy",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>S√©ance 5: TD2 - Crit√®res d'√âvaluation</span>"
    ]
  },
  {
    "objectID": "seance6.html",
    "href": "seance6.html",
    "title": "S√©ance 6: TP2 - Classification Multi-classes & Optimisation",
    "section": "",
    "text": "Introduction\nDans ce TP, nous allons explorer la classification multi-classes et approfondir les techniques d‚Äôoptimisation des hyperparam√®tres. Contrairement √† la classification binaire, nous devrons g√©rer plusieurs cat√©gories et optimiser nos mod√®les pour obtenir les meilleures performances possibles.\nObjectifs du TP:",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>S√©ance 6: TP2 - Classification Multi-classes & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance6.html#introduction",
    "href": "seance6.html#introduction",
    "title": "S√©ance 6: TP2 - Classification Multi-classes & Optimisation",
    "section": "",
    "text": "Comprendre la classification multi-classes (One-vs-Rest, One-vs-One)\nMa√Ætriser GridSearchCV et RandomizedSearchCV\nAppliquer la validation crois√©e stratifi√©e\nComparer syst√©matiquement plusieurs algorithmes\nInterpr√©ter les r√©sultats et choisir le meilleur mod√®le",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>S√©ance 6: TP2 - Classification Multi-classes & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance6.html#classification-multi-classes-concepts",
    "href": "seance6.html#classification-multi-classes-concepts",
    "title": "S√©ance 6: TP2 - Classification Multi-classes & Optimisation",
    "section": "1. Classification Multi-classes : Concepts",
    "text": "1. Classification Multi-classes : Concepts\n\n1.1 Diff√©rence avec la Classification Binaire\nClassification Binaire:\n\n2 classes: Positif/N√©gatif, Oui/Non, 0/1\nExemple: Spam/Ham, Malade/Sain\n\nClassification Multi-classes:\n\n3+ classes mutuellement exclusives\nExemple: Reconnaissance de chiffres (0-9), Classification d‚Äôesp√®ces d‚Äôiris (setosa/versicolor/virginica)\n\n\n\n1.2 Strat√©gies de Classification Multi-classes\nCertains algorithmes ne supportent pas nativement le multi-classes. Deux strat√©gies principales:\nOne-vs-Rest (OvR) / One-vs-All (OvA):\n\nEntra√Æne N classificateurs binaires (N = nombre de classes)\nChaque classificateur: ‚ÄúClasse i vs Toutes les autres‚Äù\nPr√©diction: classe avec le score le plus √©lev√©\n\n# Exemple avec 3 classes: A, B, C\nClassificateur 1: A vs (B+C)\nClassificateur 2: B vs (A+C)  \nClassificateur 3: C vs (A+B)\nOne-vs-One (OvO):\n\nEntra√Æne N√ó(N-1)/2 classificateurs binaires\nChaque classificateur: ‚ÄúClasse i vs Classe j‚Äù\nPr√©diction: vote majoritaire\n\n# Exemple avec 3 classes: A, B, C\nClassificateur 1: A vs B\nClassificateur 2: A vs C\nClassificateur 3: B vs C\n\n\n\n\n\n\nTipQuel algorithme utilise quelle strat√©gie?\n\n\n\nNatif Multi-classes:\n\nArbre de d√©cision, Random Forest\nNaive Bayes\nk-NN\n\nOne-vs-Rest par d√©faut:\n\nR√©gression Logistique (multinomial possible)\nSVM lin√©aire\n\nOne-vs-One par d√©faut:\n\nSVM avec noyau RBF\n\n\n\n\n\nExercice 1.1: Calcul Th√©orique\nQuestions:\n\nPour un probl√®me √† 10 classes, combien de classificateurs sont n√©cessaires avec OvR? Avec OvO?\nQuel est l‚Äôavantage et l‚Äôinconv√©nient de chaque approche?\nPour un dataset de reconnaissance de chiffres manuscrits (0-9), quelle strat√©gie recommandez-vous?\n\n\n\n\n\n\n\nNoteSolution Exercice 1.1\n\n\n\n\n\n1. Nombre de classificateurs:\n\nOvR: N = 10 classificateurs\n\n1 classificateur par classe\n\nOvO: N√ó(N-1)/2 = 10√ó9/2 = 45 classificateurs\n\nToutes les paires possibles\n\n\n2. Avantages/Inconv√©nients:\n\n\n\n\n\n\n\n\nStrat√©gie\nAvantages\nInconv√©nients\n\n\n\n\nOvR\n‚Ä¢ Moins de mod√®les (N)‚Ä¢ Plus rapide √† entra√Æner‚Ä¢ Moins de m√©moire\n‚Ä¢ D√©s√©quilibre de classes‚Ä¢ Ambigu√Øt√© possible\n\n\nOvO\n‚Ä¢ Classes √©quilibr√©es‚Ä¢ Bonne performance\n‚Ä¢ Beaucoup de mod√®les‚Ä¢ Lent √† entra√Æner\n\n\n\n3. Recommandation pour chiffres (0-9):\nR√©ponse: OvR (One-vs-Rest)\nRaisons:\n\n10 classes ‚Üí OvO n√©cessiterait 45 mod√®les (trop co√ªteux)\nDatasets de chiffres souvent √©quilibr√©s (le d√©s√©quilibre de OvR n‚Äôest pas probl√©matique)\nPlus rapide en pr√©diction (seulement 10 scores √† calculer vs 45 votes)\nScikit-learn optimise bien cette approche\n\nAlternative: Utiliser directement des algorithmes natifs multi-classes comme Random Forest ou un r√©seau de neurones (pour de meilleures performances).",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>S√©ance 6: TP2 - Classification Multi-classes & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance6.html#optimisation-des-hyperparam√®tres",
    "href": "seance6.html#optimisation-des-hyperparam√®tres",
    "title": "S√©ance 6: TP2 - Classification Multi-classes & Optimisation",
    "section": "2. Optimisation des Hyperparam√®tres",
    "text": "2. Optimisation des Hyperparam√®tres\n\n2.1 Rappel: Param√®tres vs Hyperparam√®tres\nParam√®tres:\n\nAppris pendant l‚Äôentra√Ænement\nExemples: poids du r√©seau, coefficients de r√©gression\nOptimis√©s automatiquement par l‚Äôalgorithme\n\nHyperparam√®tres:\n\nD√©finis avant l‚Äôentra√Ænement\nExemples: taux d‚Äôapprentissage, profondeur d‚Äôarbre, nombre de voisins\nN√©cessitent une recherche manuelle ou automatique\n\n\n\n\n\n\n\nWarningPi√®ge classique\n\n\n\nErreur fr√©quente: Optimiser les hyperparam√®tres sur le test set!\nCons√©quence: Surajustement aux donn√©es de test ‚Üí performance irr√©aliste\nSolution correcte:\n\nSplit: Train / Validation / Test (ou Train/Test + Cross-Validation)\nOptimiser sur Train/Validation\n√âvaluer une seule fois sur Test\n\n\n\n\n\n2.2 Grid Search (Recherche par Grille)\nPrincipe: Teste toutes les combinaisons possibles d‚Äôhyperparam√®tres\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# D√©finir la grille de recherche\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Cr√©er le GridSearch\ngrid_search = GridSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_grid=param_grid,\n    cv=5,  # 5-fold cross-validation\n    scoring='accuracy',\n    n_jobs=-1,  # Utilise tous les CPU\n    verbose=2\n)\n\n# Entra√Æner\ngrid_search.fit(X_train, y_train)\n\n# Meilleurs hyperparam√®tres\nprint(\"Meilleurs param√®tres:\", grid_search.best_params_)\nprint(\"Meilleur score CV:\", grid_search.best_score_)\n\n# Utiliser le meilleur mod√®le\nbest_model = grid_search.best_estimator_\nAvantages:\n\nExhaustif: teste toutes les combinaisons\nGarantit de trouver le meilleur dans la grille\n\nInconv√©nients:\n\nCo√ªt computationnel: 3 √ó 4 √ó 3 = 36 mod√®les √ó 5 folds = 180 entra√Ænements!\nExplosion combinatoire avec nombreux hyperparam√®tres\n\n\n\n2.3 Randomized Search (Recherche Al√©atoire)\nPrincipe: √âchantillonne al√©atoirement des combinaisons\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\n# Distributions pour √©chantillonnage\nparam_distributions = {\n    'n_estimators': randint(50, 300),\n    'max_depth': randint(5, 30),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10)\n}\n\n# Cr√©er le RandomizedSearch\nrandom_search = RandomizedSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_distributions=param_distributions,\n    n_iter=50,  # Nombre d'it√©rations\n    cv=5,\n    scoring='f1_weighted',\n    n_jobs=-1,\n    random_state=42,\n    verbose=2\n)\n\nrandom_search.fit(X_train, y_train)\n\nprint(\"Meilleurs param√®tres:\", random_search.best_params_)\nprint(\"Meilleur score:\", random_search.best_score_)\nAvantages:\n\nPlus rapide que GridSearch\nExplore un espace plus large\nMieux pour nombreux hyperparam√®tres\n\nInconv√©nients:\n\nPas de garantie d‚Äôoptimalit√©\nD√©pend du nombre d‚Äôit√©rations\n\n\n\n\n\n\n\nTipQuand utiliser quoi?\n\n\n\nGridSearchCV ‚Üí Peu d‚Äôhyperparam√®tres (&lt;4), petites grilles, besoin d‚Äôexhaustivit√©\nRandomizedSearchCV ‚Üí Nombreux hyperparam√®tres, grand espace de recherche, budget de temps limit√©\n\n\n\n\n2.4 Validation Crois√©e Stratifi√©e\nPour le multi-classes, important d‚Äôutiliser StratifiedKFold:\nfrom sklearn.model_selection import StratifiedKFold\n\n# Validation crois√©e stratifi√©e (pr√©serve la distribution des classes)\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ngrid_search = GridSearchCV(\n    estimator=model,\n    param_grid=param_grid,\n    cv=cv,  # Utilise StratifiedKFold\n    scoring='f1_weighted'\n)\nPourquoi? Garantit que chaque fold a la m√™me proportion de classes qu‚Äô√† l‚Äôorigine.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>S√©ance 6: TP2 - Classification Multi-classes & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance6.html#cas-pratique-classification-iris-multi-classes",
    "href": "seance6.html#cas-pratique-classification-iris-multi-classes",
    "title": "S√©ance 6: TP2 - Classification Multi-classes & Optimisation",
    "section": "3. Cas Pratique: Classification Iris Multi-classes",
    "text": "3. Cas Pratique: Classification Iris Multi-classes\n\n3.1 Chargement et Exploration\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Conversion en DataFrame\ndf = pd.DataFrame(X, columns=iris.feature_names)\ndf['species'] = iris.target_names[y]\n\nprint(\"=\" * 60)\nprint(\"DATASET IRIS - CLASSIFICATION MULTI-CLASSES\")\nprint(\"=\" * 60)\nprint(f\"\\nShape: {X.shape}\")\nprint(f\"Classes: {iris.target_names}\")\nprint(f\"\\nDistribution des classes:\")\nprint(pd.Series(y).value_counts().sort_index())\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Distribution des classes\naxes[0].bar(iris.target_names, pd.Series(y).value_counts().sort_index())\naxes[0].set_title('Distribution des Classes')\naxes[0].set_ylabel('Nombre d\\'√©chantillons')\n\n# Pairplot simplifi√© (2 features)\nfor i, species in enumerate(iris.target_names):\n    mask = y == i\n    axes[1].scatter(X[mask, 0], X[mask, 1], label=species, alpha=0.6)\naxes[1].set_xlabel(iris.feature_names[0])\naxes[1].set_ylabel(iris.feature_names[1])\naxes[1].set_title('Visualisation 2D des Classes')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Standardisation\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"\\nTrain set: {X_train.shape}\")\nprint(f\"Test set: {X_test.shape}\")\n\n\nExercice 3.1: GridSearch sur Logistic Regression\nUtilisez GridSearchCV pour optimiser une R√©gression Logistique multi-classes.\nHyperparam√®tres √† tester:\n\nC: [0.01, 0.1, 1, 10, 100]\nsolver: [‚Äòlbfgs‚Äô, ‚Äòliblinear‚Äô, ‚Äòsaga‚Äô]\nmulti_class: [‚Äòovr‚Äô, ‚Äòmultinomial‚Äô]\n\nInstructions:\n\nCr√©ez la grille de param√®tres\nUtilisez 5-fold cross-validation stratifi√©e\nM√©trique: ‚Äòaccuracy‚Äô\nAffichez les meilleurs param√®tres et le score\n√âvaluez sur le test set\n\n\n\n\n\n\n\nNoteSolution Exercice 3.1\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# 1. D√©finir la grille\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100],\n    'solver': ['lbfgs', 'liblinear', 'saga'],\n    'multi_class': ['ovr', 'multinomial']\n}\n\n# 2. Cross-validation stratifi√©e\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 3. GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=LogisticRegression(max_iter=1000, random_state=42),\n    param_grid=param_grid,\n    cv=cv,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Entra√Ænement du GridSearch...\")\ngrid_search.fit(X_train_scaled, y_train)\n\n# 4. Meilleurs param√®tres\nprint(\"\\n\" + \"=\" * 60)\nprint(\"R√âSULTATS GRIDSEARCH\")\nprint(\"=\" * 60)\nprint(f\"\\nMeilleurs param√®tres: {grid_search.best_params_}\")\nprint(f\"Meilleur score CV: {grid_search.best_score_:.4f}\")\n\n# 5. √âvaluation sur test\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test_scaled)\ntest_accuracy = (y_pred == y_test).mean()\n\nprint(f\"\\nAccuracy sur test set: {test_accuracy:.4f}\")\n\n# Rapport de classification\nprint(\"\\n\" + \"=\" * 60)\nprint(\"RAPPORT DE CLASSIFICATION\")\nprint(\"=\" * 60)\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n\n# Matrice de confusion\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=iris.target_names,\n            yticklabels=iris.target_names)\nplt.ylabel('Vraie Classe')\nplt.xlabel('Pr√©diction')\nplt.title('Matrice de Confusion - Logistic Regression')\nplt.tight_layout()\nplt.show()\n\n# R√©sultats de toutes les combinaisons (top 10)\nresults = pd.DataFrame(grid_search.cv_results_)\ntop_10 = results.nsmallest(10, 'rank_test_score')[\n    ['param_C', 'param_solver', 'param_multi_class', 'mean_test_score', 'std_test_score']\n]\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TOP 10 COMBINAISONS\")\nprint(\"=\" * 60)\nprint(top_10.to_string(index=False))\nInterpr√©tation:\n\nMeilleurs hyperparam√®tres trouv√©s (exemple typique):\n\nC=1.0 (r√©gularisation mod√©r√©e)\nsolver='lbfgs' (efficace pour petits datasets)\nmulti_class='multinomial' (souvent meilleur que OvR)\n\nAccuracy ~95-97% sur Iris (dataset facile)\nLa multinomial regression traite toutes les classes simultan√©ment (plus coh√©rent qu‚ÄôOvR)\n\n\n\n\n\n\nExercice 3.2: RandomizedSearch sur Random Forest\nUtilisez RandomizedSearchCV pour optimiser un Random Forest.\nDistributions √† √©chantillonner:\n\nn_estimators: randint(50, 300)\nmax_depth: randint(3, 20) + [None]\nmin_samples_split: randint(2, 20)\nmin_samples_leaf: randint(1, 10)\nmax_features: [‚Äòsqrt‚Äô, ‚Äòlog2‚Äô, None]\n\nInstructions:\n\n50 it√©rations\n5-fold CV\nM√©trique: ‚Äòf1_weighted‚Äô\nComparez avec Logistic Regression\n\n\n\n\n\n\n\nNoteSolution Exercice 3.2\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\n# 1. Distributions\nparam_distributions = {\n    'n_estimators': randint(50, 300),\n    'max_depth': randint(3, 20),\n    'min_samples_split': randint(2, 20),\n    'min_samples_leaf': randint(1, 10),\n    'max_features': ['sqrt', 'log2', None]\n}\n\n# 2. RandomizedSearch\nrandom_search = RandomizedSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_distributions=param_distributions,\n    n_iter=50,\n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n    scoring='f1_weighted',\n    n_jobs=-1,\n    random_state=42,\n    verbose=1\n)\n\nprint(\"Entra√Ænement du RandomizedSearch...\")\nrandom_search.fit(X_train, y_train)  # Pas besoin de scaling pour RF\n\n# R√©sultats\nprint(\"\\n\" + \"=\" * 60)\nprint(\"R√âSULTATS RANDOMIZEDSEARCH - RANDOM FOREST\")\nprint(\"=\" * 60)\nprint(f\"\\nMeilleurs param√®tres: {random_search.best_params_}\")\nprint(f\"Meilleur score CV (F1): {random_search.best_score_:.4f}\")\n\n# √âvaluation\nbest_rf = random_search.best_estimator_\ny_pred_rf = best_rf.predict(X_test)\n\nfrom sklearn.metrics import f1_score, accuracy_score\nf1_test = f1_score(y_test, y_pred_rf, average='weighted')\nacc_test = accuracy_score(y_test, y_pred_rf)\n\nprint(f\"\\nF1-Score test: {f1_test:.4f}\")\nprint(f\"Accuracy test: {acc_test:.4f}\")\n\n# Comparaison avec Logistic Regression\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARAISON MOD√àLES\")\nprint(\"=\" * 60)\n\ncomparison = pd.DataFrame({\n    'Mod√®le': ['Logistic Regression', 'Random Forest'],\n    'CV Score': [grid_search.best_score_, random_search.best_score_],\n    'Test Accuracy': [test_accuracy, acc_test],\n    'Test F1': [f1_score(y_test, y_pred, average='weighted'), f1_test]\n})\nprint(comparison.to_string(index=False))\n\n# Importance des features (RF seulement)\nfeature_importance = pd.DataFrame({\n    'Feature': iris.feature_names,\n    'Importance': best_rf.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"IMPORTANCE DES FEATURES (Random Forest)\")\nprint(\"=\" * 60)\nprint(feature_importance.to_string(index=False))\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Importance\naxes[0].barh(feature_importance['Feature'], feature_importance['Importance'])\naxes[0].set_xlabel('Importance')\naxes[0].set_title('Importance des Features')\naxes[0].invert_yaxis()\n\n# Matrice de confusion RF\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nsns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n            xticklabels=iris.target_names,\n            yticklabels=iris.target_names)\naxes[1].set_ylabel('Vraie Classe')\naxes[1].set_xlabel('Pr√©diction')\naxes[1].set_title('Matrice de Confusion - Random Forest')\n\nplt.tight_layout()\nplt.show()\nInterpr√©tation:\n\nRandom Forest g√©n√©ralement l√©g√®rement meilleur ou √©quivalent √† Logistic Regression sur Iris\nAvantage de RF: g√®re les relations non-lin√©aires\nImportance des features r√©v√®le que petal length et petal width sont les plus discriminantes",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>S√©ance 6: TP2 - Classification Multi-classes & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance6.html#comparaison-syst√©matique-de-plusieurs-algorithmes",
    "href": "seance6.html#comparaison-syst√©matique-de-plusieurs-algorithmes",
    "title": "S√©ance 6: TP2 - Classification Multi-classes & Optimisation",
    "section": "4. Comparaison Syst√©matique de Plusieurs Algorithmes",
    "text": "4. Comparaison Syst√©matique de Plusieurs Algorithmes\n\nExercice 4.1: Pipeline de Comparaison\nComparez 5 algorithmes avec optimisation:\n\nLogistic Regression\nRandom Forest\nSVM (RBF kernel)\nk-NN\nGradient Boosting (XGBoost si disponible)\n\nPour chaque algorithme: - Optimisez les hyperparam√®tres avec RandomizedSearch (ou Grid si petite grille) - Utilisez 5-fold CV stratifi√©e - M√©trique: F1 weighted - √âvaluez sur test set - Cr√©ez un tableau comparatif final\n\n\n\n\n\n\nNoteSolution Exercice 4.1\n\n\n\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport time\n\n# Pr√©paration des mod√®les et grilles\nmodels_params = {\n    'Logistic Regression': {\n        'model': LogisticRegression(max_iter=1000, random_state=42),\n        'params': {\n            'C': [0.01, 0.1, 1, 10],\n            'solver': ['lbfgs', 'saga'],\n            'multi_class': ['ovr', 'multinomial']\n        },\n        'search_type': 'grid',\n        'needs_scaling': True\n    },\n    'Random Forest': {\n        'model': RandomForestClassifier(random_state=42),\n        'params': {\n            'n_estimators': randint(50, 200),\n            'max_depth': randint(3, 15),\n            'min_samples_split': randint(2, 10)\n        },\n        'search_type': 'random',\n        'n_iter': 30,\n        'needs_scaling': False\n    },\n    'SVM': {\n        'model': SVC(random_state=42),\n        'params': {\n            'C': [0.1, 1, 10, 100],\n            'gamma': ['scale', 'auto', 0.001, 0.01],\n            'kernel': ['rbf', 'linear']\n        },\n        'search_type': 'grid',\n        'needs_scaling': True\n    },\n    'k-NN': {\n        'model': KNeighborsClassifier(),\n        'params': {\n            'n_neighbors': range(3, 20),\n            'weights': ['uniform', 'distance'],\n            'metric': ['euclidean', 'manhattan']\n        },\n        'search_type': 'grid',\n        'needs_scaling': True\n    },\n    'Gradient Boosting': {\n        'model': GradientBoostingClassifier(random_state=42),\n        'params': {\n            'n_estimators': randint(50, 200),\n            'learning_rate': uniform(0.01, 0.3),\n            'max_depth': randint(3, 10),\n            'subsample': uniform(0.7, 0.3)\n        },\n        'search_type': 'random',\n        'n_iter': 30,\n        'needs_scaling': False\n    }\n}\n\n# CV stratifi√©e\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Stocker les r√©sultats\nresults = []\n\nprint(\"=\" * 80)\nprint(\"COMPARAISON SYST√âMATIQUE DE 5 ALGORITHMES\")\nprint(\"=\" * 80)\n\nfor name, config in models_params.items():\n    print(f\"\\n{'='*80}\")\n    print(f\"Entra√Ænement: {name}\")\n    print(f\"{'='*80}\")\n    \n    start_time = time.time()\n    \n    # Choisir les donn√©es (scaled ou non)\n    X_tr = X_train_scaled if config['needs_scaling'] else X_train\n    X_te = X_test_scaled if config['needs_scaling'] else X_test\n    \n    # Choisir Grid ou Randomized\n    if config['search_type'] == 'grid':\n        search = GridSearchCV(\n            estimator=config['model'],\n            param_grid=config['params'],\n            cv=cv,\n            scoring='f1_weighted',\n            n_jobs=-1\n        )\n    else:\n        search = RandomizedSearchCV(\n            estimator=config['model'],\n            param_distributions=config['params'],\n            n_iter=config['n_iter'],\n            cv=cv,\n            scoring='f1_weighted',\n            n_jobs=-1,\n            random_state=42\n        )\n    \n    # Entra√Ænement\n    search.fit(X_tr, y_train)\n    \n    # Pr√©diction\n    y_pred = search.best_estimator_.predict(X_te)\n    \n    # M√©triques\n    train_time = time.time() - start_time\n    test_acc = accuracy_score(y_test, y_pred)\n    test_f1 = f1_score(y_test, y_pred, average='weighted')\n    cv_f1 = search.best_score_\n    \n    # Stocker\n    results.append({\n        'Mod√®le': name,\n        'CV F1': cv_f1,\n        'Test Accuracy': test_acc,\n        'Test F1': test_f1,\n        'Temps (s)': train_time,\n        'Meilleurs Params': str(search.best_params_)\n    })\n    \n    print(f\"Meilleurs param√®tres: {search.best_params_}\")\n    print(f\"CV F1: {cv_f1:.4f}\")\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    print(f\"Test F1: {test_f1:.4f}\")\n    print(f\"Temps d'entra√Ænement: {train_time:.2f}s\")\n\n# DataFrame des r√©sultats\ndf_results = pd.DataFrame(results)\ndf_results = df_results.sort_values('Test F1', ascending=False).reset_index(drop=True)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TABLEAU COMPARATIF FINAL\")\nprint(\"=\" * 80)\nprint(df_results[['Mod√®le', 'CV F1', 'Test Accuracy', 'Test F1', 'Temps (s)']].to_string(index=False))\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Graphique 1: Comparaison F1\nx_pos = range(len(df_results))\naxes[0].barh(x_pos, df_results['Test F1'], color='skyblue', alpha=0.8, label='Test F1')\naxes[0].barh(x_pos, df_results['CV F1'], color='orange', alpha=0.6, label='CV F1')\naxes[0].set_yticks(x_pos)\naxes[0].set_yticklabels(df_results['Mod√®le'])\naxes[0].set_xlabel('F1-Score')\naxes[0].set_title('Comparaison des Performances (F1)')\naxes[0].legend()\naxes[0].grid(axis='x', alpha=0.3)\naxes[0].invert_yaxis()\n\n# Graphique 2: Temps vs Performance\naxes[1].scatter(df_results['Temps (s)'], df_results['Test F1'], s=100, alpha=0.6)\nfor idx, row in df_results.iterrows():\n    axes[1].annotate(row['Mod√®le'], \n                     (row['Temps (s)'], row['Test F1']),\n                     fontsize=9, ha='right')\naxes[1].set_xlabel('Temps d\\'entra√Ænement (s)')\naxes[1].set_ylabel('Test F1-Score')\naxes[1].set_title('Trade-off Performance vs Temps')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Recommandation\nbest_model_name = df_results.iloc[0]['Mod√®le']\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RECOMMANDATION\")\nprint(\"=\" * 80)\nprint(f\"‚Üí Meilleur mod√®le: {best_model_name}\")\nprint(f\"  Raison: Meilleur F1-Score sur test ({df_results.iloc[0]['Test F1']:.4f})\")\n\n# Si plusieurs mod√®les proches, consid√©rer le temps\ntop_3 = df_results.head(3)\nif (top_3['Test F1'].max() - top_3['Test F1'].min()) &lt; 0.02:\n    fastest = top_3.loc[top_3['Temps (s)'].idxmin()]\n    print(f\"\\nNote: Les 3 meilleurs mod√®les ont des performances similaires.\")\n    print(f\"   Consid√©rez {fastest['Mod√®le']} (plus rapide: {fastest['Temps (s)']:.1f}s)\")\n    \n# Sauvegarde des r√©sultats\nprint(\"\\nSauvegarde des r√©sultats...\")\ndf_results.to_csv('resultats_comparaison_modeles.csv', index=False)\nprint(\"R√©sultats sauvegard√©s dans 'resultats_comparaison_modeles.csv'\")\n# Pour le Datasets Digits (classification de chiffres)\n# Utilisez la fonction `load_digits` de sklearn\n\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# Analyse rapide\nprint(f\"Dataset Digits - Shape: {X.shape}\")\nprint(f\"Classes: {digits.target_names}\")\nprint(f\"Nombre d'images: {X.shape[0]}\")\nprint(f\"Dimensions de chaque image: {digits.images[0].shape} (8x8 pixels)\")\nprint(f\"Valeurs de pixel normalis√©es entre 0 et 16\")\n\n# Visualisation de quelques chiffres\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(digits.images[i], cmap='binary')\n    ax.set_title(f\"Chiffre: {digits.target[i]}\")\n    ax.axis('off')\nplt.suptitle(\"Exemples de chiffres manuscrits (8x8 pixels)\")\nplt.tight_layout()\nplt.show()\n\n# Distribution des classes\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\npd.Series(y).value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution des Classes (Chiffres 0-9)')\nplt.xlabel('Chiffre')\nplt.ylabel('Nombre d\\'√©chantillons')\nplt.grid(axis='y', alpha=0.3)\n\nplt.subplot(1, 2, 2)\npd.Series(y).value_counts().plot(kind='pie', autopct='%1.1f%%')\nplt.title('Proportion des Classes')\nplt.ylabel('')\nplt.tight_layout()\nplt.show()\n\n## 5.2 Classification Multi-classes sur Digits\n\n# Split et normalisation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Train set: {X_train.shape} ({X_train.shape[0]/X.shape[0]:.1%})\")\nprint(f\"Test set: {X_test.shape} ({X_test.shape[0]/X.shape[0]:.1%})\")\n\n### Exercice 5.1: OvR vs OvO avec SVM\n\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n\n# Comparaison OvR vs OvO\novr_clf = OneVsRestClassifier(SVC(kernel='rbf', random_state=42))\novo_clf = OneVsOneClassifier(SVC(kernel='rbf', random_state=42))\n\nprint(\"Entra√Ænement des mod√®les...\")\novr_clf.fit(X_train_scaled, y_train)\novo_clf.fit(X_train_scaled, y_train)\n\n# √âvaluation\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARAISON OVR vs OVO SUR DIGITS\")\nprint(\"=\"*60)\n\nfor name, clf in [(\"OvR\", ovr_clf), (\"OvO\", ovo_clf)]:\n    y_pred = clf.predict(X_test_scaled)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"\\n{name}:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  Nombre de classificateurs: {len(clf.estimators_)}\")\n\n# Visualisation des matrices de confusion\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\nfor idx, (name, clf) in enumerate([(\"OvR\", ovr_clf), (\"OvO\", ovo_clf)]):\n    y_pred = clf.predict(X_test_scaled)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n                xticklabels=range(10), yticklabels=range(10))\n    axes[idx].set_title(f'Matrice de Confusion - {name}')\n    axes[idx].set_ylabel('Vraie Classe')\n    axes[idx].set_xlabel('Pr√©diction')\nplt.tight_layout()\nplt.show()\n\n### Exercice 5.2: Optimisation avec GridSearchCV\n\n# GridSearch pour SVM avec param√®tres optimaux\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n    'kernel': ['rbf', 'linear', 'poly']\n}\n\ngrid_search_svm = GridSearchCV(\n    SVC(random_state=42),\n    param_grid,\n    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"\\nOptimisation SVM avec GridSearchCV...\")\ngrid_search_svm.fit(X_train_scaled, y_train)\n\nprint(f\"\\nMeilleurs param√®tres: {grid_search_svm.best_params_}\")\nprint(f\"Meilleur score CV: {grid_search_svm.best_score_:.4f}\")\n\n# Test avec le meilleur mod√®le\nbest_svm = grid_search_svm.best_estimator_\ny_pred_svm = best_svm.predict(X_test_scaled)\ntest_accuracy = accuracy_score(y_test, y_pred_svm)\nprint(f\"Accuracy sur test: {test_accuracy:.4f}\")\n\n### Exercice 5.3: Comparaison finale\n\n# Test de plusieurs algorithmes sans optimisation exhaustive\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nmodels = {\n    \"SVM (RBF)\": SVC(kernel='rbf', C=10, gamma='scale', random_state=42),\n    \"SVM (Linear)\": SVC(kernel='linear', C=1, random_state=42),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n    \"k-NN\": KNeighborsClassifier(n_neighbors=3),\n    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, random_state=42),\n    \"Gaussian NB\": GaussianNB()\n}\n\nresults = []\nfor name, model in models.items():\n    # Entra√Ænement\n    model.fit(X_train_scaled if name not in ['Random Forest', 'Decision Tree'] else X_train, \n              y_train)\n    \n    # Pr√©diction\n    X_te = X_test_scaled if name not in ['Random Forest', 'Decision Tree'] else X_test\n    y_pred = model.predict(X_te)\n    \n    # M√©triques\n    acc = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    results.append({\n        'Mod√®le': name,\n        'Accuracy': acc,\n        'F1-Score': f1\n    })\n\n# Affichage des r√©sultats\ndf_comparison = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARAISON DES ALGORITHMES SUR DIGITS\")\nprint(\"=\"*60)\nprint(df_comparison.to_string(index=False))\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nbars = plt.barh(range(len(df_comparison)), df_comparison['Accuracy'], color='steelblue')\nplt.yticks(range(len(df_comparison)), df_comparison['Mod√®le'])\nplt.xlabel('Accuracy')\nplt.title('Performance des Algorithmes sur Digits Dataset')\nplt.xlim([0.85, 1.0])\nplt.grid(axis='x', alpha=0.3)\n\n# Ajouter les valeurs sur les barres\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    plt.text(width + 0.005, bar.get_y() + bar.get_height()/2, \n             f'{width:.3f}', ha='left', va='center')\n\nplt.tight_layout()\nplt.show()\n\n## 6. Conclusion\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CONCLUSION DU TP\")\nprint(\"=\"*80)\n\nprint(\"\\nR√©capitulatif des points cl√©s abord√©s:\")\nprint(\"1. ‚úÖ Classification Multi-classes: OvR vs OvO\")\nprint(\"2. ‚úÖ Optimisation hyperparam√®tres: GridSearchCV et RandomizedSearchCV\")\nprint(\"3. ‚úÖ Validation crois√©e stratifi√©e (important pour classes d√©s√©quilibr√©es)\")\nprint(\"4. ‚úÖ Comparaison syst√©matique d'algorithmes\")\nprint(\"5. ‚úÖ Application sur deux datasets: Iris (facile) et Digits (plus complexe)\")\n\nprint(\"\\nRecommandations g√©n√©rales:\")\nprint(\"- Pour probl√®mes multi-classes: privil√©gier les algorithmes natifs ou OvR\")\nprint(\"- Pour optimisation: RandomizedSearchCV pour grands espaces, GridSearchCV sinon\")\nprint(\"- Toujours utiliser validation crois√©e pour √©viter le sur-ajustement\")\nprint(\"- Comparer plusieurs algorithmes avant de choisir le meilleur\")\n\nprint(\"\\nPour aller plus loin:\")\nprint(\"- Essayer d'autres datasets (fashion-MNIST, CIFAR-10)\")\nprint(\"- Explorer les m√©thodes d'ensembling (Stacking, Voting)\")\nprint(\"- Utiliser des techniques de r√©duction de dimension (PCA, t-SNE) pour la visualisation\")\nprint(\"- Impl√©menter un r√©seau de neurones pour la reconnaissance de chiffres\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FIN DU TP - CLASSIFICATION MULTI-CLASSES & OPTIMISATION\")\nprint(\"=\"*80)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>S√©ance 6: TP2 - Classification Multi-classes & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance7.html",
    "href": "seance7.html",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "",
    "text": "Introduction\nLa r√©gression est une t√¢che d‚Äôapprentissage supervis√© qui vise √† pr√©dire une valeur continue (quantitative) √† partir de features. Contrairement √† la classification qui pr√©dit des cat√©gories discr√®tes, la r√©gression pr√©dit des nombres r√©els.\nExemples d‚Äôapplications:",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#introduction",
    "href": "seance7.html#introduction",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "",
    "text": "Pr√©dire le prix d‚Äôune maison\nEstimer la consommation d‚Äô√©nergie\nPr√©voir les ventes futures\n√âvaluer le risque de cr√©dit (montant)\nPr√©dire la temp√©rature",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#r√©gression-vs-classification",
    "href": "seance7.html#r√©gression-vs-classification",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "1. R√©gression vs Classification",
    "text": "1. R√©gression vs Classification\n\n1.1 Diff√©rences Fondamentales\n\n\n\nAspect\nClassification\nR√©gression\n\n\n\n\nVariable cible\nDiscr√®te/Cat√©gorielle\nContinue/Num√©rique\n\n\nSortie\nClasse ou probabilit√©\nValeur r√©elle\n\n\nExemples\nSpam/Ham, Chien/Chat\nPrix, Temp√©rature\n\n\nM√©triques\nAccuracy, Precision, Recall\nMAE, MSE, R¬≤\n\n\nFonction de co√ªt\nCross-Entropy\nMSE, MAE\n\n\n\n\n\n1.2 Exemple Illustratif\n# Classification\nX = [[3, 2], [4, 3], [1, 1], [2, 1]]\ny = [0, 0, 1, 1]  # Classes: 0 ou 1\n# Pr√©diction: classe 0 ou 1\n\n# R√©gression  \nX = [[1500], [2000], [2500], [3000]]  # Surface en m¬≤\ny = [200000, 280000, 350000, 420000]  # Prix en ‚Ç¨\n# Pr√©diction: prix continu (ex: 315,750‚Ç¨)\n\n\n\n\n\n\nTipComment choisir?\n\n\n\nUtilisez la Classification si:\n\nR√©ponse = Cat√©gorie (‚ÄúOui/Non‚Äù, ‚ÄúRouge/Vert/Bleu‚Äù)\nObjectif = Classer, identifier, d√©tecter\n\nUtilisez la R√©gression si:\n\nR√©ponse = Nombre (‚Äú125.5‚Äù, ‚Äú3.14‚Äù)\nObjectif = Pr√©dire une quantit√©, estimer",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#r√©gression-lin√©aire-fondements-th√©oriques",
    "href": "seance7.html#r√©gression-lin√©aire-fondements-th√©oriques",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "2. R√©gression Lin√©aire: Fondements Th√©oriques",
    "text": "2. R√©gression Lin√©aire: Fondements Th√©oriques\n\n2.1 R√©gression Lin√©aire Simple\nObjectif: Mod√©liser la relation entre une variable explicative \\(x\\) et une variable cible \\(y\\)\n√âquation: \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nO√π:\n\n\\(y\\) : variable √† pr√©dire (variable d√©pendante)\n\\(x\\) : variable explicative (variable ind√©pendante)\n\\(\\beta_0\\) : ordonn√©e √† l‚Äôorigine (intercept)\n\\(\\beta_1\\) : pente (coefficient)\n\\(\\epsilon\\) : terme d‚Äôerreur\n\nObjectif d‚Äôapprentissage: Trouver \\(\\beta_0\\) et \\(\\beta_1\\) qui minimisent l‚Äôerreur\n\n\n2.2 R√©gression Lin√©aire Multiple\n√âquation g√©n√©rale avec p features: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\\]\nForme matricielle: \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\nO√π:\n\n\\(\\mathbf{y}\\) : vecteur des valeurs cibles (n √ó 1)\n\\(\\mathbf{X}\\) : matrice des features (n √ó p)\n\\(\\boldsymbol{\\beta}\\) : vecteur des coefficients (p √ó 1)\n\\(\\boldsymbol{\\epsilon}\\) : vecteur des erreurs (n √ó 1)\n\n\n\n2.3 Fonction de Co√ªt: Mean Squared Error (MSE)\nD√©finition: \\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nO√π:\n\n\\(y_i\\) : vraie valeur\n\\(\\hat{y}_i = \\mathbf{x}_i^T\\boldsymbol{\\beta}\\) : pr√©diction\n\\(n\\) : nombre d‚Äôexemples\n\nPourquoi le carr√©?\n\nP√©nalise davantage les grandes erreurs\nDiff√©rentiable partout (facilite l‚Äôoptimisation)\nCorrespond √† la vraisemblance gaussienne\n\n\n\n2.4 Solution Analytique: √âquation Normale\nPour minimiser MSE, on peut d√©river et r√©soudre:\n\\[\\boldsymbol{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nAvantages:\n\nSolution exacte en une √©tape\nPas besoin de r√©glage d‚Äôhyperparam√®tres\n\nInconv√©nients:\n\nCo√ªt computationnel: \\(O(p^3)\\) (inversion de matrice)\nProbl√®me si \\(\\mathbf{X}^T\\mathbf{X}\\) non inversible (colin√©arit√©)\nImpraticable pour grands datasets\n\n\n\n2.5 Impl√©mentation en Python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# G√©n√©ration de donn√©es\nnp.random.seed(42)\nX = np.random.rand(100, 1) * 10\ny = 3 + 2 * X + np.random.randn(100, 1) * 2  # y = 3 + 2x + bruit\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Mod√®le\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Pr√©dictions\ny_pred = model.predict(X_test)\n\n# Coefficients\nprint(f\"Intercept (beta0): {model.intercept_[0]:.2f}\")\nprint(f\"Coefficient (beta1): {model.coef_[0][0]:.2f}\")\nprint(f\"MSE: {mean_squared_error(y_test, y_pred):.2f}\")\nprint(f\"R¬≤: {r2_score(y_test, y_pred):.4f}\")\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nplt.scatter(X_train, y_train, alpha=0.6, label='Train')\nplt.scatter(X_test, y_test, alpha=0.6, label='Test', color='orange')\nplt.plot(X, model.predict(X), color='red', linewidth=2, label='Droite de r√©gression')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('R√©gression Lin√©aire Simple')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\nExercice 2.1: Comprendre les Coefficients\nSoit le mod√®le suivant pour pr√©dire le prix d‚Äôune maison: \\[\\text{Prix} = 50000 + 150 \\times \\text{Surface} + 10000 \\times \\text{Nb\\_Chambres}\\]\nQuestions:\n\nInterpr√©tez chaque coefficient\nPr√©disez le prix d‚Äôune maison de 100m¬≤ avec 3 chambres\nQuelle est l‚Äôaugmentation de prix si on ajoute 10m¬≤?\nSi Surface et Nb_Chambres sont corr√©l√©s, quel probl√®me peut survenir?\n\n\n\n\n\n\n\nNoteSolution Exercice 2.1\n\n\n\n\n\n1. Interpr√©tation des coefficients:\n\n\\(\\beta_0\\) = 50,000‚Ç¨ (Intercept)\n\nPrix de base (th√©orique) d‚Äôune maison avec 0m¬≤ et 0 chambres\nSouvent non interpr√©table physiquement (extrapolation)\n\n\\(\\beta_1 = 150\\,\\text{‚Ç¨}/\\text{m}^2\\) (Coefficient de Surface)\n\nAugmentation du prix pour chaque m¬≤ suppl√©mentaire\nInterpr√©tation: √Ä nombre de chambres constant, +1m¬≤ ‚Üí +150‚Ç¨\n\n\\(\\beta_2 = 10\\,000~\\text{‚Ç¨}/\\text{chambre}\\) (Coefficient de Nb_Chambres)\n\nAugmentation du prix pour chaque chambre suppl√©mentaire\nInterpr√©tation: √Ä surface constante, +1 chambre ‚Üí +10,000‚Ç¨\n\n\n2. Pr√©diction:\nPrix = 50,000 + 150 √ó 100 + 10,000 √ó 3 Prix = 50,000 + 15,000 + 30,000 Prix = 95,000‚Ç¨\n3. Augmentation pour +10m¬≤:\n\\[\\Delta\\text{Prix} = \\beta_1 \\times \\Delta\\text{Surface} = 150 \\times 10 = \\mathbf{1\\,500}~\\textbf{‚Ç¨}\\]\n(√Ä nombre de chambres constant)\n4. Probl√®me de corr√©lation (Multicolin√©arit√©):\nSi Surface et Nb_Chambres sont fortement corr√©l√©s (ex: plus de surface ‚Üí g√©n√©ralement plus de chambres):\nCons√©quences:\n\nCoefficients instables (varient beaucoup selon les donn√©es)\nDifficult√© d‚Äôinterpr√©tation (quel feature a vraiment l‚Äôimpact?)\nMatrice \\(\\mathbf{X}^T\\mathbf{X}\\) mal conditionn√©e\nVariance des estimateurs √©lev√©e\n\nSolutions:\n\nSupprimer une des features corr√©l√©es\nUtiliser des techniques de r√©gularisation (Ridge, Lasso)\nPCA pour cr√©er des features non corr√©l√©es",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#r√©gularisation-ridge-lasso-elasticnet",
    "href": "seance7.html#r√©gularisation-ridge-lasso-elasticnet",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "3. R√©gularisation: Ridge, Lasso, ElasticNet",
    "text": "3. R√©gularisation: Ridge, Lasso, ElasticNet\n\n3.1 Probl√®me du Surapprentissage\nR√©gression lin√©aire simple peut surapprendre avec:\n\nNombreuses features (p &gt;&gt; n)\nFeatures corr√©l√©es (multicolin√©arit√©)\nFeatures non pertinentes\n\nSympt√¥mes:\n\nCoefficients tr√®s grands (instables)\nExcellent fit sur train, mauvais sur test\nMod√®le sensible au bruit\n\nSolution: R√©gularisation = p√©naliser les grands coefficients\n\n\n3.2 Ridge Regression (R√©gularisation L2)\nFonction de co√ªt: \\[\\text{Cost}_{\\text{Ridge}} = \\text{MSE} + \\alpha \\sum_{j=1}^{p}\\beta_j^2\\]\n\\[= \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\|\\boldsymbol{\\beta}\\|_2^2\\]\nO√π:\n\n\\(\\alpha \\geq 0\\) : param√®tre de r√©gularisation (hyperparam√®tre)\n\\(\\|\\boldsymbol{\\beta}\\|_2^2 = \\sum_{j=1}^{p}\\beta_j^2\\) : norme L2 des coefficients\n\nEffet:\n\nP√©nalise les coefficients √©lev√©s\nCoefficients r√©tr√©cis vers 0 mais jamais exactement 0\nTous les features restent dans le mod√®le\n\nCas limites:\n\n\\(\\alpha = 0\\) ‚Üí R√©gression lin√©aire classique\n\\(\\alpha \\to \\infty\\) ‚Üí Tous les coefficients ‚Üí 0\n\nSolution analytique: \\[\\boldsymbol{\\beta}_{\\text{Ridge}} = (\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\nImpl√©mentation:\nfrom sklearn.linear_model import Ridge\n\n# Mod√®le Ridge\nridge = Ridge(alpha=1.0)  \nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_test)\n\nprint(f\"Coefficients Ridge: {ridge.coef_}\")\n\n\n3.3 Lasso Regression (R√©gularisation L1)\nFonction de co√ªt: \\[\\text{Cost}_{\\text{Lasso}} = \\text{MSE} + \\alpha \\sum_{j=1}^{p}|\\beta_j|\\]\n\\[= \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\alpha \\|\\boldsymbol{\\beta}\\|_1\\]\nO√π:\n\n\\(\\|\\boldsymbol{\\beta}\\|_1 = \\sum_{j=1}^{p}|\\beta_j|\\) : norme L1 des coefficients\n\nEffet:\n\nP√©nalise les coefficients √©lev√©s\nPeut mettre certains coefficients exactement √† 0\nS√©lection automatique de features\n\nAvantages:\n\nMod√®le plus simple et interpr√©table (moins de features)\nUtile quand on soup√ßonne que certaines features sont inutiles\n\nImpl√©mentation:\nfrom sklearn.linear_model import Lasso\n\n# Mod√®le Lasso\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\n\n# Compter les features s√©lectionn√©es\nselected_features = np.sum(lasso.coef_ != 0)\nprint(f\"Features s√©lectionn√©es: {selected_features}/{len(lasso.coef_)}\")\n\n\n3.4 ElasticNet (Combinaison L1 + L2)\nFonction de co√ªt: \\[\\text{Cost}_{\\text{ElasticNet}} = \\text{MSE} + \\alpha \\left( \\rho \\|\\boldsymbol{\\beta}\\|_1 + \\frac{1-\\rho}{2} \\|\\boldsymbol{\\beta}\\|_2^2 \\right)\\]\nO√π:\n\n\\(\\alpha\\) : force de r√©gularisation\n\\(\\rho \\in [0, 1]\\) : ratio L1/L2 (l1_ratio)\n\n\\(\\rho = 0\\) ‚Üí Ridge pure\n\\(\\rho = 1\\) ‚Üí Lasso pure\n\n\nAvantages:\n\nCombine avantages de Ridge et Lasso\nG√®re mieux les features corr√©l√©es que Lasso seul\nS√©lection de features comme Lasso\n\nImpl√©mentation:\nfrom sklearn.linear_model import ElasticNet\n\n# Mod√®le ElasticNet\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)  # 50% L1, 50% L2\nelastic.fit(X_train, y_train)\n\n\n3.5 Comparaison Visuelle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n\n# Dataset synth√©tique avec 20 features\nnp.random.seed(42)\nn_samples, n_features = 100, 20\nX = np.random.randn(n_samples, n_features)\n# Seulement 5 features vraiment utiles\ntrue_coef = np.zeros(n_features)\ntrue_coef[:5] = [5, -3, 2, -4, 3]\ny = X @ true_coef + np.random.randn(n_samples) * 0.5\n\n# Entra√Æner les mod√®les\nmodels = {\n    'Linear': LinearRegression(),\n    'Ridge (alpha=1)': Ridge(alpha=1.0),\n    'Ridge (alpha=10)': Ridge(alpha=10.0),\n    'Lasso (alpha=0.1)': Lasso(alpha=0.1),\n    'Lasso (alpha=1)': Lasso(alpha=1.0),\n    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n}\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nfor idx, (name, model) in enumerate(models.items()):\n    model.fit(X, y)\n    coefs = model.coef_\n    \n    axes[idx].bar(range(n_features), coefs, alpha=0.7)\n    axes[idx].bar(range(5), true_coef[:5], alpha=0.3, color='red', label='Vrais coefs')\n    axes[idx].axhline(0, color='black', linewidth=0.8, linestyle='--')\n    axes[idx].set_title(name)\n    axes[idx].set_xlabel('Feature Index')\n    axes[idx].set_ylabel('Coefficient Value')\n    axes[idx].legend()\n    axes[idx].grid(alpha=0.3)\n    \n    # Nombre de coefs non-nuls\n    non_zero = np.sum(np.abs(coefs) &gt; 1e-5)\n    axes[idx].text(0.95, 0.95, f'Non-zero: {non_zero}',\n                   transform=axes[idx].transAxes,\n                   ha='right', va='top',\n                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.tight_layout()\nplt.show()\nObservations:\n\nLinear: Coefficients instables, sensibles au bruit\nRidge: R√©duit tous les coefficients, mais aucun √† 0\nLasso: Met beaucoup de coefficients √† 0 (s√©lection)\nElasticNet: Compromis entre Ridge et Lasso\n\n\n\n3.6 Choix du Param√®tre \\(\\alpha\\)\nM√©thode: Validation crois√©e avec GridSearchCV ou RidgeCV/LassoCV\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.model_selection import cross_val_score\n\n# Ridge avec CV int√©gr√©e\nalphas = np.logspace(-3, 3, 100)\nridge_cv = RidgeCV(alphas=alphas, cv=5)\nridge_cv.fit(X_train, y_train)\n\nprint(f\"Meilleur alpha (Ridge): {ridge_cv.alpha_:.4f}\")\n\n# Lasso avec CV\nlasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42)\nlasso_cv.fit(X_train, y_train)\n\nprint(f\"Meilleur alpha (Lasso): {lasso_cv.alpha_:.4f}\")\n\n# Visualiser l'effet de alpha\nmse_scores = []\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    scores = cross_val_score(ridge, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n    mse_scores.append(-scores.mean())\n\nplt.figure(figsize=(10, 6))\nplt.plot(alphas, mse_scores, marker='o')\nplt.xscale('log')\nplt.xlabel('alpha (log scale)')\nplt.ylabel('MSE (Cross-Validation)')\nplt.title('Choix de alpha pour Ridge Regression')\nplt.axvline(ridge_cv.alpha_, color='red', linestyle='--', label=f'Optimal alpha={ridge_cv.alpha_:.2f}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n3.7 Quand Utiliser Quel Mod√®le?\n\n\n\n\n\n\n\n\nSituation\nMod√®le Recommand√©\nRaison\n\n\n\n\nFeatures non corr√©l√©es, toutes utiles\nLinear\nSimplicit√© suffisante\n\n\nMulticolin√©arit√© forte\nRidge\nStabilise les coefficients\n\n\nBeaucoup de features inutiles\nLasso\nS√©lection automatique\n\n\nMulticolin√©arit√© + features inutiles\nElasticNet\nCombine avantages L1+L2\n\n\nPetite dimension (p &lt; n)\nLinear ou Ridge\nPas de s√©lection n√©cessaire\n\n\nGrande dimension (p &gt;&gt; n)\nLasso ou ElasticNet\n√âvite surapprentissage",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#support-vector-regression-svr",
    "href": "seance7.html#support-vector-regression-svr",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "4. Support Vector Regression (SVR)",
    "text": "4. Support Vector Regression (SVR)\n\n4.1 Principe\nSVR adapte les SVM √† la r√©gression:\n\nCherche une fonction qui d√©vie au maximum de \\(\\epsilon\\) de la vraie valeur\nIgnore les erreurs inf√©rieures √† \\(\\epsilon\\) (\\(\\epsilon\\)-tube)\nMinimise les erreurs au-del√† de \\(\\epsilon\\)\n\nFonction objectif: \\[\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{i=1}^{n}(\\xi_i + \\xi_i^*)\\]\nSous contraintes: \\[|y_i - (\\mathbf{w}^T\\mathbf{x}_i + b)| \\leq \\epsilon + \\xi_i\\]\nO√π:\n\n\\(\\epsilon\\) : largeur de l‚Äô\\(\\epsilon\\)-tube (tol√©rance)\n\\(C\\) : param√®tre de r√©gularisation\n\\(\\xi_i\\) : variables de rel√¢chement\n\n\n\n4.2 Kernels pour Relations Non-Lin√©aires\nSVR lin√©aire:\nfrom sklearn.svm import SVR\n\nsvr_lin = SVR(kernel='linear', C=1.0, epsilon=0.1)\nsvr_lin.fit(X_train, y_train)\nSVR avec noyau RBF (Gaussian):\nsvr_rbf = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale')\nsvr_rbf.fit(X_train, y_train)\nHyperparam√®tres:\n\nC: Trade-off entre erreur et complexit√© (comme SVM)\nepsilon: Largeur du tube (erreurs &lt; \\(\\epsilon\\) ignor√©es)\ngamma: Influence du kernel RBF (si kernel=‚Äòrbf‚Äô)\n\n\n\n4.3 Avantages et Limites\nAvantages:\n\nG√®re relations non-lin√©aires avec kernels\nRobuste aux outliers (gr√¢ce √† \\(\\epsilon\\)-tube)\nEfficace en haute dimension\n\nLimites:\n\nLent sur grands datasets (pas de solution analytique)\nSensible au choix des hyperparam√®tres\nMoins interpr√©table que r√©gression lin√©aire",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#m√©triques-d√©valuation-pour-la-r√©gression",
    "href": "seance7.html#m√©triques-d√©valuation-pour-la-r√©gression",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "5. M√©triques d‚Äô√âvaluation pour la R√©gression",
    "text": "5. M√©triques d‚Äô√âvaluation pour la R√©gression\n\n5.1 Mean Absolute Error (MAE)\nD√©finition: \\[\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|\\]\nInterpr√©tation:\n\nErreur moyenne absolue\nM√™me unit√© que y\nMoins sensible aux outliers que MSE\n\nfrom sklearn.metrics import mean_absolute_error\n\nmae = mean_absolute_error(y_test, y_pred)\nprint(f\"MAE: {mae:.2f}\")\n\n\n5.2 Mean Squared Error (MSE)\nD√©finition: \\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nInterpr√©tation:\n\nErreur quadratique moyenne\nUnit√©: (unit√© de y)¬≤\nP√©nalise davantage les grandes erreurs\n\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"MSE: {mse:.2f}\")\n\n\n5.3 Root Mean Squared Error (RMSE)\nD√©finition: \\[\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\]\nInterpr√©tation:\n\nRacine carr√©e de MSE\nM√™me unit√© que y (plus interpr√©table que MSE)\nErreur ‚Äútypique‚Äù\n\nrmse = np.sqrt(mse)\nprint(f\"RMSE: {rmse:.2f}\")\n\n\n5.4 R¬≤ Score (Coefficient de D√©termination)\nD√©finition: \\[R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\]\nO√π \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i\\) (moyenne)\nInterpr√©tation:\n\nProportion de variance expliqu√©e par le mod√®le\nR¬≤ = 1: Pr√©diction parfaite\nR¬≤ = 0: Mod√®le aussi bon que la moyenne\nR¬≤ &lt; 0: Mod√®le pire que la moyenne\n\nfrom sklearn.metrics import r2_score\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R¬≤: {r2:.4f}\")\nAttention: R¬≤ peut √™tre trompeur!\n\nAugmente toujours avec plus de features (m√™me inutiles)\nSolution: Adjusted R¬≤\n\n\\[R^2_{\\text{adj}} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\n\n\n5.5 Comparaison des M√©triques\n\n\n\nM√©trique\nUnit√©\nSensibilit√© Outliers\nInterpr√©tation\nUsage\n\n\n\n\nMAE\ny\nFaible\nErreur moyenne\nRobuste, facile\n\n\nMSE\ny¬≤\nForte\nErreur quadratique\nOptimisation math\n\n\nRMSE\ny\nForte\nErreur ‚Äútypique‚Äù\nInterpr√©table\n\n\nR¬≤\nSans\nMoyenne\nVariance expliqu√©e\nComparaison mod√®les\n\n\n\nRecommandation:\n\nRapporter plusieurs m√©triques\nRMSE pour interpr√©tabilit√©\nR¬≤ pour comparaison de mod√®les\nMAE si pr√©sence d‚Äôoutliers",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#r√©sum√©-de-la-s√©ance",
    "href": "seance7.html#r√©sum√©-de-la-s√©ance",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "R√©sum√© de la S√©ance",
    "text": "R√©sum√© de la S√©ance\n\n\n\n\n\n\nImportantPoints cl√©s √† retenir\n\n\n\n\n1. R√©gression vs Classification\n\nR√©gression = pr√©dire une valeur continue\nClassification = pr√©dire une cat√©gorie\n\n\n\n2. R√©gression Lin√©aire\n\nMod√®le: \\(y = \\beta_0 + \\sum_{j=1}^{p}\\beta_j x_j + \\epsilon\\)\nSolution analytique (√©quation normale)\nRisque de surapprentissage si p √©lev√©\n\n\n\n3. R√©gularisation\n\nRidge (L2): R√©duit coefficients, aucun √† 0\nLasso (L1): S√©lection de features (coefs √† 0)\nElasticNet: Combine L1 + L2\nHyperparam√®tre \\(\\alpha\\) √† optimiser par CV\n\n\n\n4. SVR\n\nR√©gression avec SVMs\nG√®re non-lin√©arit√© (kernels)\nRobuste aux outliers\n\n\n\n5. M√©triques\n\nMAE: Erreur absolue moyenne\nRMSE: Erreur quadratique (m√™me unit√© que y)\nR¬≤: Variance expliqu√©e (0-1)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance7.html#pr√©paration-tp3",
    "href": "seance7.html#pr√©paration-tp3",
    "title": "S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression",
    "section": "Pr√©paration TP3",
    "text": "Pr√©paration TP3\nLe prochain TP mettra en pratique:\n\nImpl√©mentation de mod√®les de r√©gression\nComparaison Linear, Ridge, Lasso, ElasticNet, SVR\nOptimisation des hyperparam√®tres\n√âvaluation avec m√©triques multiples\nVisualisation des r√©sultats\n\n√Ä pr√©parer:\n\nInstaller scikit-learn √† jour\nR√©viser GridSearchCV/RandomizedSearchCV\nComprendre les hyperparam√®tres de chaque mod√®le",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>S√©ance 7: Cours - Apprentissage Supervis√© : R√©gression</span>"
    ]
  },
  {
    "objectID": "seance8.html",
    "href": "seance8.html",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "",
    "text": "Introduction\nDans ce TP, nous allons mettre en pratique les concepts de r√©gression vus en cours. Nous travaillerons sur un dataset r√©el pour pr√©dire les prix de maisons, en comparant diff√©rents mod√®les de r√©gression et en optimisant leurs hyperparam√®tres.\nObjectifs du TP:",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#introduction",
    "href": "seance8.html#introduction",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "",
    "text": "Pr√©traiter des donn√©es pour la r√©gression\nImpl√©menter et comparer Linear, Ridge, Lasso, ElasticNet, SVR\nOptimiser les hyperparam√®tres avec CV\n√âvaluer avec m√©triques multiples (MAE, RMSE, R¬≤)\nInterpr√©ter et visualiser les r√©sultats",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#chargement-et-exploration-du-dataset",
    "href": "seance8.html#chargement-et-exploration-du-dataset",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "1. Chargement et Exploration du Dataset",
    "text": "1. Chargement et Exploration du Dataset\nNous utilisons le Boston Housing Dataset (ou California Housing si Boston non disponible).\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement\ncalifornia = fetch_california_housing()\nX, y = california.data, california.target\n\n# DataFrame pour exploration\ndf = pd.DataFrame(X, columns=california.feature_names)\ndf['MedHouseVal'] = y\n\nprint(\"=\" * 70)\nprint(\"DATASET: CALIFORNIA HOUSING\")\nprint(\"=\" * 70)\nprint(f\"\\nShape: {X.shape}\")\nprint(f\"Features: {california.feature_names}\")\nprint(f\"\\nDescription du target (prix m√©dian en 100k$):\")\nprint(df['MedHouseVal'].describe())\n\n# Statistiques descriptives\nprint(\"\\n\" + \"=\" * 70)\nprint(\"STATISTIQUES DESCRIPTIVES\")\nprint(\"=\" * 70)\nprint(df.describe().T)\n\n# V√©rifier les valeurs manquantes\nprint(f\"\\nValeurs manquantes par feature:\")\nprint(df.isnull().sum())\n\n# Visualisations\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Distribution du target\naxes[0, 0].hist(y, bins=50, edgecolor='black', alpha=0.7)\naxes[0, 0].set_xlabel('Prix M√©dian (100k$)')\naxes[0, 0].set_ylabel('Fr√©quence')\naxes[0, 0].set_title('Distribution des Prix')\naxes[0, 0].axvline(y.mean(), color='red', linestyle='--', label=f'Moyenne: {y.mean():.2f}')\naxes[0, 0].legend()\n\n# Boxplot des features (normalis√©es pour comparaison)\ndf_normalized = (df - df.mean()) / df.std()\naxes[0, 1].boxplot([df_normalized[col] for col in california.feature_names],\n                    labels=california.feature_names, vert=True)\naxes[0, 1].set_xticklabels(california.feature_names, rotation=45, ha='right')\naxes[0, 1].set_ylabel('Valeurs normalis√©es')\naxes[0, 1].set_title('Distribution des Features (normalis√©es)')\naxes[0, 1].grid(alpha=0.3)\n\n# Matrice de corr√©lation\ncorr_matrix = df.corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nsns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n            center=0, ax=axes[1, 0], cbar_kws={'label': 'Corr√©lation'})\naxes[1, 0].set_title('Matrice de Corr√©lation')\n\n# Scatter: Feature la plus corr√©l√©e avec target\ncorr_with_target = corr_matrix['MedHouseVal'].drop('MedHouseVal').abs().sort_values(ascending=False)\nbest_feature = corr_with_target.index[0]\naxes[1, 1].scatter(df[best_feature], df['MedHouseVal'], alpha=0.3)\naxes[1, 1].set_xlabel(best_feature)\naxes[1, 1].set_ylabel('Prix M√©dian')\naxes[1, 1].set_title(f'Prix vs {best_feature} (corr={corr_matrix.loc[best_feature, \"MedHouseVal\"]:.2f})')\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CORR√âLATIONS AVEC LE TARGET\")\nprint(\"=\" * 70)\nprint(corr_with_target)\n\n======================================================================\nDATASET: CALIFORNIA HOUSING\n======================================================================\n\nShape: (20640, 8)\nFeatures: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n\nDescription du target (prix m√©dian en 100k$):\ncount    20640.000000\nmean         2.068558\nstd          1.153956\nmin          0.149990\n25%          1.196000\n50%          1.797000\n75%          2.647250\nmax          5.000010\nName: MedHouseVal, dtype: float64\n\n======================================================================\nSTATISTIQUES DESCRIPTIVES\n======================================================================\n               count         mean          std         min         25%  \\\nMedInc       20640.0     3.870671     1.899822    0.499900    2.563400   \nHouseAge     20640.0    28.639486    12.585558    1.000000   18.000000   \nAveRooms     20640.0     5.429000     2.474173    0.846154    4.440716   \nAveBedrms    20640.0     1.096675     0.473911    0.333333    1.006079   \nPopulation   20640.0  1425.476744  1132.462122    3.000000  787.000000   \nAveOccup     20640.0     3.070655    10.386050    0.692308    2.429741   \nLatitude     20640.0    35.631861     2.135952   32.540000   33.930000   \nLongitude    20640.0  -119.569704     2.003532 -124.350000 -121.800000   \nMedHouseVal  20640.0     2.068558     1.153956    0.149990    1.196000   \n\n                     50%          75%           max  \nMedInc          3.534800     4.743250     15.000100  \nHouseAge       29.000000    37.000000     52.000000  \nAveRooms        5.229129     6.052381    141.909091  \nAveBedrms       1.048780     1.099526     34.066667  \nPopulation   1166.000000  1725.000000  35682.000000  \nAveOccup        2.818116     3.282261   1243.333333  \nLatitude       34.260000    37.710000     41.950000  \nLongitude    -118.490000  -118.010000   -114.310000  \nMedHouseVal     1.797000     2.647250      5.000010  \n\nValeurs manquantes par feature:\nMedInc         0\nHouseAge       0\nAveRooms       0\nAveBedrms      0\nPopulation     0\nAveOccup       0\nLatitude       0\nLongitude      0\nMedHouseVal    0\ndtype: int64\n\n\n\n\n\n\n\n\n\n\n======================================================================\nCORR√âLATIONS AVEC LE TARGET\n======================================================================\nMedInc        0.688075\nAveRooms      0.151948\nLatitude      0.144160\nHouseAge      0.105623\nAveBedrms     0.046701\nLongitude     0.045967\nPopulation    0.024650\nAveOccup      0.023737\nName: MedHouseVal, dtype: float64\n\n\n\nExercice 1.1: Analyse Exploratoire\nQuestions:\n\nQuelle feature est la plus corr√©l√©e avec le prix?\nY a-t-il des features fortement corr√©l√©es entre elles? (Multicolin√©arit√© potentielle?)\nLa distribution du target est-elle gaussienne? Quel traitement pourrait √™tre appliqu√© si non?\nIdentifiez des outliers potentiels\n\n\n\n\n\n\n\nNoteSolution Exercice 1.1\n\n\n\n\n\n\nprint(\"=\" * 70)\nprint(\"ANALYSE EXPLORATOIRE - R√âPONSES\")\nprint(\"=\" * 70)\n\n# 1. Feature la plus corr√©l√©e\nprint(f\"\\n1. Feature la plus corr√©l√©e avec le prix:\")\nprint(f\"   ‚Üí {best_feature} (corr√©lation = {corr_matrix.loc[best_feature, 'MedHouseVal']:.3f})\")\n\n# 2. Multicolin√©arit√©\nprint(f\"\\n2. Paires de features fortement corr√©l√©es (|corr| &gt; 0.7):\")\ncorr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if i != j:\n            corr_val = corr_matrix.iloc[i, j]\n            if abs(corr_val) &gt; 0.7:\n                corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n\nif corr_pairs:\n    for feat1, feat2, corr_val in corr_pairs:\n        print(f\"   ‚Ä¢ {feat1} &lt;-&gt; {feat2}: {corr_val:.3f}\")\nelse:\n    print(\"   ‚Üí Aucune multicolin√©arit√© forte d√©tect√©e\")\n\n# 3. Distribution du target\nfrom scipy import stats\nshapiro_stat, shapiro_p = stats.shapiro(y[:1000])  # Test sur √©chantillon\nprint(f\"\\n3. Test de normalit√© (Shapiro-Wilk):\")\nprint(f\"   Statistique: {shapiro_stat:.4f}, p-value: {shapiro_p:.4e}\")\nif shapiro_p &lt; 0.05:\n    print(\"   ‚Üí Distribution NON gaussienne (p &lt; 0.05)\")\n    print(\"   Traitements possibles:\")\n    print(\"   ‚Ä¢ Transformation log(y)\")\n    print(\"   ‚Ä¢ Transformation Box-Cox\")\n    print(\"   ‚Ä¢ Utiliser des mod√®les robustes aux distributions non-gaussiennes\")\nelse:\n    print(\"   ‚Üí Distribution gaussienne (p &gt;= 0.05)\")\n\n# Skewness et Kurtosis\nskewness = stats.skew(y)\nkurtosis = stats.kurtosis(y)\nprint(f\"   Asym√©trie (Skewness): {skewness:.3f}\")\nprint(f\"   Aplatissement (Kurtosis): {kurtosis:.3f}\")\n\n# 4. Outliers\nprint(f\"\\n4. D√©tection d'outliers:\")\nQ1 = df['MedHouseVal'].quantile(0.25)\nQ3 = df['MedHouseVal'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = df[(df['MedHouseVal'] &lt; lower_bound) | (df['MedHouseVal'] &gt; upper_bound)]\nprint(f\"   Intervalle IQR: [{Q1:.2f}, {Q3:.2f}]\")\nprint(f\"   Bornes: [{lower_bound:.2f}, {upper_bound:.2f}]\")\nprint(f\"   Nombre d'outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n\n# Visualisation des outliers\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(range(len(y)), np.sort(y), alpha=0.5, s=10)\nax.axhline(lower_bound, color='red', linestyle='--', label='Lower bound')\nax.axhline(upper_bound, color='red', linestyle='--', label='Upper bound')\nax.set_xlabel('Index (tri√©)')\nax.set_ylabel('Prix')\nax.set_title('D√©tection d\\'Outliers par IQR')\nax.legend()\nax.grid(alpha=0.3)\nplt.show()\n\n======================================================================\nANALYSE EXPLORATOIRE - R√âPONSES\n======================================================================\n\n1. Feature la plus corr√©l√©e avec le prix:\n   ‚Üí MedInc (corr√©lation = 0.688)\n\n2. Paires de features fortement corr√©l√©es (|corr| &gt; 0.7):\n   ‚Ä¢ AveRooms &lt;-&gt; AveBedrms: 0.848\n   ‚Ä¢ Latitude &lt;-&gt; Longitude: -0.925\n\n3. Test de normalit√© (Shapiro-Wilk):\n   Statistique: 0.9460, p-value: 1.2095e-18\n   ‚Üí Distribution NON gaussienne (p &lt; 0.05)\n   Traitements possibles:\n   ‚Ä¢ Transformation log(y)\n   ‚Ä¢ Transformation Box-Cox\n   ‚Ä¢ Utiliser des mod√®les robustes aux distributions non-gaussiennes\n   Asym√©trie (Skewness): 0.978\n   Aplatissement (Kurtosis): 0.328\n\n4. D√©tection d'outliers:\n   Intervalle IQR: [1.20, 2.65]\n   Bornes: [-0.98, 4.82]\n   Nombre d'outliers: 1071 (5.2%)\n\n\n\n\n\n\n\n\n\nConclusions typiques:\n\nMedInc (revenu m√©dian) g√©n√©ralement le plus corr√©l√© (~0.68)\nMulticolin√©arit√© possible ‚Üí consid√©rer Ridge/ElasticNet\nDistribution souvent l√©g√®rement asym√©trique ‚Üí transformation log possible\nQuelques outliers mais acceptable (&lt;5%)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#pr√©traitement-et-split",
    "href": "seance8.html#pr√©traitement-et-split",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "2. Pr√©traitement et Split",
    "text": "2. Pr√©traitement et Split\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Split train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"=\" * 70)\nprint(\"SPLIT DES DONN√âES\")\nprint(\"=\" * 70)\nprint(f\"Train set: {X_train.shape}\")\nprint(f\"Test set: {X_test.shape}\")\n\n# Standardisation (importante pour Ridge, Lasso, SVR)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"\\nStandardisation:\")\nprint(f\"Mean (train): {X_train_scaled.mean(axis=0)}\")  # Devrait √™tre ~0\nprint(f\"Std (train): {X_train_scaled.std(axis=0)}\")    # Devrait √™tre ~1\n\n# V√©rification\nprint(f\"\\nV√©rification apr√®s scaling:\")\nprint(f\"  Mean proche de 0: {np.allclose(X_train_scaled.mean(axis=0), 0, atol=1e-10)}\")\nprint(f\"  Std proche de 1: {np.allclose(X_train_scaled.std(axis=0), 1, atol=1e-2)}\")\n\n======================================================================\nSPLIT DES DONN√âES\n======================================================================\nTrain set: (16512, 8)\nTest set: (4128, 8)\n\nStandardisation:\nMean (train): [-6.59266865e-15 -6.68608149e-17  8.01559239e-15 -1.17273358e-15\n -2.60880895e-18 -1.13675656e-16  7.99652724e-14 -3.87910056e-13]\nStd (train): [1. 1. 1. 1. 1. 1. 1. 1.]\n\nV√©rification apr√®s scaling:\n  Mean proche de 0: True\n  Std proche de 1: True\n\n\n\n\n\n\n\n\nWarningImportance de la Standardisation\n\n\n\nPourquoi standardiser? - Ridge/Lasso: P√©nalisation √©quitable des coefficients - SVR: Sensible √† l‚Äô√©chelle des features - Convergence plus rapide des algorithmes it√©ratifs\nQuand ne PAS standardiser? - Arbres de d√©cision / Random Forest (invariants aux transformations monotones) - R√©gression lin√©aire simple (si pas de r√©gularisation)\nR√®gle: Toujours fit sur train, transform sur test (√©viter data leakage)",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#mod√®les-de-base",
    "href": "seance8.html#mod√®les-de-base",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "3. Mod√®les de Base",
    "text": "3. Mod√®les de Base\n\nExercice 3.1: R√©gression Lin√©aire Simple\nEntra√Ænez une r√©gression lin√©aire et √©valuez-la.\nInstructions: 1. Cr√©ez et entra√Ænez le mod√®le 2. Pr√©disez sur train et test 3. Calculez MAE, RMSE, R¬≤ pour les deux ensembles 4. Affichez les 5 coefficients les plus importants 5. Visualisez pr√©dictions vs vraies valeurs\n\n\n\n\n\n\nNoteSolution Exercice 3.1\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nprint(\"=\" * 70)\nprint(\"R√âGRESSION LIN√âAIRE SIMPLE\")\nprint(\"=\" * 70)\n\n# 1. Entra√Ænement\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_train_scaled, y_train)\n\n# 2. Pr√©dictions\ny_train_pred = model_lr.predict(X_train_scaled)\ny_test_pred = model_lr.predict(X_test_scaled)\n\n# 3. M√©triques\ndef evaluate_model(y_true, y_pred, dataset_name=\"\"):\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n    \n    print(f\"\\n{dataset_name}:\")\n    print(f\"  MAE:  {mae:.4f}\")\n    print(f\"  RMSE: {rmse:.4f}\")\n    print(f\"  R¬≤:   {r2:.4f}\")\n    \n    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n\ntrain_metrics = evaluate_model(y_train, y_train_pred, \"Train Set\")\ntest_metrics = evaluate_model(y_test, y_test_pred, \"Test Set\")\n\n# 4. Coefficients importants\ncoef_df = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Coefficient': model_lr.coef_\n})\ncoef_df['Abs_Coef'] = np.abs(coef_df['Coefficient'])\ncoef_df = coef_df.sort_values('Abs_Coef', ascending=False)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COEFFICIENTS (TOP 5)\")\nprint(\"=\" * 70)\nprint(coef_df[['Feature', 'Coefficient']].head().to_string(index=False))\n\n# 5. Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\n# Pr√©dictions vs Vraies valeurs (Test)\naxes[0].scatter(y_test, y_test_pred, alpha=0.3, s=10)\naxes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n             'r--', lw=2, label='Pr√©diction parfaite')\naxes[0].set_xlabel('Vraie valeur')\naxes[0].set_ylabel('Pr√©diction')\naxes[0].set_title(f'R√©gression Lin√©aire (Test R¬≤={test_metrics[\"R2\"]:.3f})')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# R√©sidus\nresiduals = y_test - y_test_pred\naxes[1].scatter(y_test_pred, residuals, alpha=0.3, s=10)\naxes[1].axhline(0, color='red', linestyle='--', lw=2)\naxes[1].set_xlabel('Pr√©dictions')\naxes[1].set_ylabel('R√©sidus')\naxes[1].set_title('Graphique des R√©sidus')\naxes[1].grid(alpha=0.3)\n\n# Distribution des r√©sidus\naxes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\naxes[2].axvline(residuals.mean(), color='red', linestyle='--', \n               label=f'Moyenne: {residuals.mean():.4f}')\naxes[2].set_xlabel('R√©sidus')\naxes[2].set_ylabel('Fr√©quence')\naxes[2].set_title('Distribution des R√©sidus')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Analyse des r√©sidus\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANALYSE DES R√âSIDUS\")\nprint(\"=\" * 70)\nprint(f\"Moyenne: {residuals.mean():.6f} (devrait √™tre $\\approx$ 0)\")\nprint(f\"Std: {residuals.std():.4f}\")\nprint(f\"Min: {residuals.min():.4f}\")\nprint(f\"Max: {residuals.max():.4f}\")\n\n======================================================================\nR√âGRESSION LIN√âAIRE SIMPLE\n======================================================================\n\nTrain Set:\n  MAE:  0.5286\n  RMSE: 0.7197\n  R¬≤:   0.6126\n\nTest Set:\n  MAE:  0.5332\n  RMSE: 0.7456\n  R¬≤:   0.5758\n\n======================================================================\nCOEFFICIENTS (TOP 5)\n======================================================================\n  Feature  Coefficient\n Latitude    -0.896929\nLongitude    -0.869842\n   MedInc     0.854383\nAveBedrms     0.339259\n AveRooms    -0.294410\n\n\n\n\n\n\n\n\n\n\n======================================================================\nANALYSE DES R√âSIDUS\n======================================================================\nMoyenne: 0.003479 (devrait √™tre $\u0007pprox$ 0)\nStd: 0.7456\nMin: -9.8753\nMax: 4.1484\n\n\nInterpr√©tation: - R¬≤ train &gt; R¬≤ test ‚Üí l√©ger surapprentissage (normal) - R√©sidus centr√©s sur 0 ‚Üí mod√®le non biais√© - Distribution des r√©sidus approximativement gaussienne ‚Üí hypoth√®ses v√©rifi√©es",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#mod√®les-r√©gularis√©s",
    "href": "seance8.html#mod√®les-r√©gularis√©s",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "4. Mod√®les R√©gularis√©s",
    "text": "4. Mod√®les R√©gularis√©s\n\nExercice 4.1: Comparaison Ridge, Lasso, ElasticNet\nComparez les 3 mod√®les r√©gularis√©s avec diff√©rentes valeurs de alpha.\nInstructions:\n\nTestez alpha dans [0.001, 0.01, 0.1, 1, 10, 100]\nPour chaque mod√®le et chaque alpha:\n\nEntra√Ænez sur train\nCalculez R¬≤ sur test\n\nTracez R¬≤ vs alpha pour les 3 mod√®les\nIdentifiez le meilleur mod√®le et le meilleur alpha\n\n\n\n\n\n\n\nNoteSolution Exercice 4.1\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\nprint(\"=\" * 70)\nprint(\"COMPARAISON RIDGE, LASSO, ELASTICNET\")\nprint(\"=\" * 70)\n\n# 1. Grille d'alphas\nalphas = [0.001, 0.01, 0.1, 1, 10, 100]\n\n# Stocker les r√©sultats\nresults = {'Ridge': [], 'Lasso': [], 'ElasticNet': []}\n\n# 2. Entra√Ænement\nfor alpha in alphas:\n    # Ridge\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_scaled, y_train)\n    r2_ridge = r2_score(y_test, ridge.predict(X_test_scaled))\n    results['Ridge'].append(r2_ridge)\n    \n    # Lasso\n    lasso = Lasso(alpha=alpha, max_iter=10000)\n    lasso.fit(X_train_scaled, y_train)\n    r2_lasso = r2_score(y_test, lasso.predict(X_test_scaled))\n    results['Lasso'].append(r2_lasso)\n    \n    # ElasticNet\n    elastic = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000)\n    elastic.fit(X_train_scaled, y_train)\n    r2_elastic = r2_score(y_test, elastic.predict(X_test_scaled))\n    results['ElasticNet'].append(r2_elastic)\n\n# 3. Visualisation\nplt.figure(figsize=(12, 6))\nfor model_name, r2_scores in results.items():\n    plt.plot(alphas, r2_scores, marker='o', label=model_name, linewidth=2)\n\nplt.xscale('log')\nplt.xlabel('alpha (log scale)')\nplt.ylabel('R¬≤ Score (Test)')\nplt.title('Comparaison Ridge, Lasso, ElasticNet')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.axhline(test_metrics['R2'], color='red', linestyle='--', \n           label=f'Linear (alpha=0): {test_metrics[\"R2\"]:.4f}', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n# 4. Meilleur mod√®le\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MEILLEURS HYPERPARAM√àTRES\")\nprint(\"=\" * 70)\n\nfor model_name, r2_scores in results.items():\n    best_idx = np.argmax(r2_scores)\n    best_alpha = alphas[best_idx]\n    best_r2 = r2_scores[best_idx]\n    print(f\"\\n{model_name}:\")\n    print(f\"  Meilleur alpha: {best_alpha}\")\n    print(f\"  R¬≤ Test: {best_r2:.4f}\")\n\n# Tableau comparatif\ndf_comparison = pd.DataFrame(results, index=[f'alpha={a}' for a in alphas])\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TABLEAU COMPARATIF (R¬≤ Test)\")\nprint(\"=\" * 70)\nprint(df_comparison.to_string())\n\n======================================================================\nCOMPARAISON RIDGE, LASSO, ELASTICNET\n======================================================================\n\n\n\n\n\n\n\n\n\n\n======================================================================\nMEILLEURS HYPERPARAM√àTRES\n======================================================================\n\nRidge:\n  Meilleur alpha: 100\n  R¬≤ Test: 0.5778\n\nLasso:\n  Meilleur alpha: 0.01\n  R¬≤ Test: 0.5816\n\nElasticNet:\n  Meilleur alpha: 0.01\n  R¬≤ Test: 0.5803\n\n======================================================================\nTABLEAU COMPARATIF (R¬≤ Test)\n======================================================================\n                Ridge     Lasso  ElasticNet\nalpha=0.001  0.575788  0.576856    0.576543\nalpha=0.01   0.575788  0.581615    0.580319\nalpha=0.1    0.575791  0.481361    0.514765\nalpha=1      0.575816 -0.000219    0.203126\nalpha=10     0.576060 -0.000219   -0.000219\nalpha=100    0.577791 -0.000219   -0.000219\n\n\nObservations attendues: - Ridge: R¬≤ stable, peu sensible √† alpha - Lasso: R¬≤ peut chuter si alpha trop √©lev√© (trop de coefficients √† 0) - ElasticNet: Compromis entre Ridge et Lasso\n\n\n\n\n\nExercice 4.2: S√©lection de Features avec Lasso\nUtilisez Lasso pour identifier les features importantes.\nInstructions: 1. Entra√Ænez Lasso avec alpha=0.1 2. Affichez le nombre de coefficients non-nuls 3. Identifiez les features s√©lectionn√©es 4. Comparez les coefficients Lasso vs Linear\n\n\n\n\n\n\nNoteSolution Exercice 4.2\n\n\n\n\n\nprint(\"=\" * 70)\nprint(\"S√âLECTION DE FEATURES AVEC LASSO\")\nprint(\"=\" * 70)\n\n# 1. Entra√Ænement\nlasso = Lasso(alpha=0.1, max_iter=10000)\nlasso.fit(X_train_scaled, y_train)\n\n# 2. Coefficients non-nuls\nnon_zero_mask = np.abs(lasso.coef_) &gt; 1e-5\nn_selected = np.sum(non_zero_mask)\nn_total = len(lasso.coef_)\n\nprint(f\"\\nNombre de features s√©lectionn√©es: {n_selected}/{n_total}\")\n\n# 3. Features s√©lectionn√©es\nselected_features = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Lasso_Coef': lasso.coef_,\n    'Linear_Coef': model_lr.coef_,\n    'Selected': non_zero_mask\n})\nselected_features['Abs_Lasso'] = np.abs(selected_features['Lasso_Coef'])\nselected_features = selected_features.sort_values('Abs_Lasso', ascending=False)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON COEFFICIENTS\")\nprint(\"=\" * 70)\nprint(selected_features[['Feature', 'Lasso_Coef', 'Linear_Coef', 'Selected']].to_string(index=False))\n\n# 4. Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Barplot comparatif\nx_pos = np.arange(len(california.feature_names))\nwidth = 0.35\n\naxes[0].bar(x_pos - width/2, model_lr.coef_, width, label='Linear', alpha=0.7)\naxes[0].bar(x_pos + width/2, lasso.coef_, width, label='Lasso (alpha=0.1)', alpha=0.7)\naxes[0].set_xticks(x_pos)\naxes[0].set_xticklabels(california.feature_names, rotation=45, ha='right')\naxes[0].set_ylabel('Coefficient')\naxes[0].set_title('Comparaison Coefficients: Linear vs Lasso')\naxes[0].legend()\naxes[0].axhline(0, color='black', linewidth=0.8)\naxes[0].grid(alpha=0.3, axis='y')\n\n# Scatter: Linear vs Lasso\naxes[1].scatter(model_lr.coef_, lasso.coef_, s=100, alpha=0.6)\nfor i, feature in enumerate(california.feature_names):\n    axes[1].annotate(feature, (model_lr.coef_[i], lasso.coef_[i]),\n                     fontsize=8, alpha=0.7)\naxes[1].plot([model_lr.coef_.min(), model_lr.coef_.max()],\n             [model_lr.coef_.min(), model_lr.coef_.max()],\n             'r--', label='y=x')\naxes[1].set_xlabel('Coefficient Linear')\naxes[1].set_ylabel('Coefficient Lasso')\naxes[1].set_title('Linear vs Lasso Coefficients')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FEATURES √âLIMIN√âES PAR LASSO\")\nprint(\"=\" * 70)\neliminated = selected_features[~selected_features['Selected']]['Feature'].tolist()\nif eliminated:\n    print(\"Features mises √† 0:\")\n    for feat in eliminated:\n        print(f\"  ‚Ä¢ {feat}\")\nelse:\n    print(\"Aucune feature √©limin√©e (alpha peut-√™tre trop faible)\")\nInterpr√©tation: - Lasso r√©duit certains coefficients exactement √† 0 - Features √©limin√©es = probablement redondantes ou peu informatives - Peut am√©liorer l‚Äôinterpr√©tabilit√© du mod√®le",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#optimisation-avec-cross-validation",
    "href": "seance8.html#optimisation-avec-cross-validation",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "5. Optimisation avec Cross-Validation",
    "text": "5. Optimisation avec Cross-Validation\n\nExercice 5.1: RidgeCV et LassoCV\nUtilisez les versions CV pour trouver automatiquement le meilleur alpha.\n\n\n\n\n\n\nNoteSolution Exercice 5.1\n\n\n\n\n\nfrom sklearn.linear_model import RidgeCV, LassoCV\n\nprint(\"=\" * 70)\nprint(\"OPTIMISATION AUTOMATIQUE AVEC CV\")\nprint(\"=\" * 70)\n\n# Grille d'alphas\nalphas_cv = np.logspace(-3, 3, 50)\n\n# RidgeCV\nprint(\"\\n1. RidgeCV...\")\nridge_cv = RidgeCV(alphas=alphas_cv, cv=5, scoring='r2')\nridge_cv.fit(X_train_scaled, y_train)\nprint(f\"   Meilleur alpha: {ridge_cv.alpha_:.4f}\")\n\ny_pred_ridge = ridge_cv.predict(X_test_scaled)\nridge_metrics = evaluate_model(y_test, y_pred_ridge, \"Ridge (CV)\")\n\n# LassoCV\nprint(\"\\n2. LassoCV...\")\nlasso_cv = LassoCV(alphas=alphas_cv, cv=5, max_iter=10000, random_state=42)\nlasso_cv.fit(X_train_scaled, y_train)\nprint(f\"   Meilleur alpha: {lasso_cv.alpha_:.4f}\")\n\ny_pred_lasso = lasso_cv.predict(X_test_scaled)\nlasso_metrics = evaluate_model(y_test, y_pred_lasso, \"Lasso (CV)\")\n\n# ElasticNetCV\nfrom sklearn.linear_model import ElasticNetCV\n\nprint(\"\\n3. ElasticNetCV...\")\nelastic_cv = ElasticNetCV(alphas=alphas_cv, l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n                          cv=5, max_iter=10000, random_state=42)\nelastic_cv.fit(X_train_scaled, y_train)\nprint(f\"   Meilleur alpha: {elastic_cv.alpha_:.4f}\")\nprint(f\"   Meilleur l1_ratio: {elastic_cv.l1_ratio_:.2f}\")\n\ny_pred_elastic = elastic_cv.predict(X_test_scaled)\nelastic_metrics = evaluate_model(y_test, y_pred_elastic, \"ElasticNet (CV)\")\n\n# Comparaison finale\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON FINALE\")\nprint(\"=\" * 70)\n\nfinal_comparison = pd.DataFrame({\n    'Mod√®le': ['Linear', 'Ridge (CV)', 'Lasso (CV)', 'ElasticNet (CV)'],\n    'MAE': [test_metrics['MAE'], ridge_metrics['MAE'], \n            lasso_metrics['MAE'], elastic_metrics['MAE']],\n    'RMSE': [test_metrics['RMSE'], ridge_metrics['RMSE'], \n             lasso_metrics['RMSE'], elastic_metrics['RMSE']],\n    'R¬≤': [test_metrics['R2'], ridge_metrics['R2'], \n           lasso_metrics['R2'], elastic_metrics['R2']]\n})\n\nprint(final_comparison.to_string(index=False))\n\n# Meilleur mod√®le\nbest_idx = final_comparison['R¬≤'].idxmax()\nbest_model_name = final_comparison.loc[best_idx, 'Mod√®le']\nprint(f\"\\n‚Üí Meilleur mod√®le: {best_model_name}\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#svr-support-vector-regression",
    "href": "seance8.html#svr-support-vector-regression",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "6. SVR (Support Vector Regression)",
    "text": "6. SVR (Support Vector Regression)\n\nExercice 6.1: SVR avec diff√©rents kernels\nComparez SVR lin√©aire et RBF.\n\n\n\n\n\n\nNoteSolution Exercice 6.1\n\n\n\n\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"=\" * 70)\nprint(\"SUPPORT VECTOR REGRESSION\")\nprint(\"=\" * 70)\n\n# Note: SVR est lent, on r√©duit le dataset pour d√©mo\nX_train_small = X_train_scaled[:5000]\ny_train_small = y_train[:5000]\n\n# 1. SVR Lin√©aire\nprint(\"\\n1. SVR Lin√©aire...\")\nsvr_lin = SVR(kernel='linear', C=1.0)\nsvr_lin.fit(X_train_small, y_train_small)\ny_pred_svr_lin = svr_lin.predict(X_test_scaled)\nsvr_lin_metrics = evaluate_model(y_test, y_pred_svr_lin, \"SVR Linear\")\n\n# 2. SVR RBF\nprint(\"\\n2. SVR RBF...\")\nsvr_rbf = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr_rbf.fit(X_train_small, y_train_small)\ny_pred_svr_rbf = svr_rbf.predict(X_test_scaled)\nsvr_rbf_metrics = evaluate_model(y_test, y_pred_svr_rbf, \"SVR RBF\")\n\n# 3. Comparaison\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON SVR\")\nprint(\"=\" * 70)\n\nsvr_comparison = pd.DataFrame({\n    'Kernel': ['linear', 'rbf'],\n    'MAE': [svr_lin_metrics['MAE'], svr_rbf_metrics['MAE']],\n    'RMSE': [svr_lin_metrics['RMSE'], svr_rbf_metrics['RMSE']],\n    'R¬≤': [svr_lin_metrics['R2'], svr_rbf_metrics['R2']]\n})\nprint(svr_comparison.to_string(index=False))\n\n# 4. Optimisation SVR RBF\nprint(\"\\n\" + \"=\" * 70)\nprint(\"OPTIMISATION SVR RBF AVEC GRIDSEARCH\")\nprint(\"=\" * 70)\n\n# R√©duction suppl√©mentaire pour vitesse\nX_val = X_train_scaled[5000:6000]\ny_val = y_train[5000:6000]\n\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'gamma': ['scale', 'auto', 0.01, 0.1],\n    'epsilon': [0.01, 0.1, 0.5]\n}\n\ngrid_search = GridSearchCV(\n    SVR(kernel='rbf'),\n    param_grid,\n    cv=3,\n    scoring='r2',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Entra√Ænement GridSearch (peut √™tre long)...\")\ngrid_search.fit(X_train_small, y_train_small)\n\nprint(f\"\\nMeilleurs param√®tres: {grid_search.best_params_}\")\nbest_svr = grid_search.best_estimator_\ny_pred_best_svr = best_svr.predict(X_val)\nbest_svr_metrics = evaluate_model(y_val, y_pred_best_svr, \"SVR optimis√© (val)\")\n\n# Visualisation SVR\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Pr√©dictions vs Vraies valeurs\naxes[0].scatter(y_val, y_pred_best_svr, alpha=0.3, s=10)\naxes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n             'r--', lw=2, label='Pr√©diction parfaite')\naxes[0].set_xlabel('Vraie valeur')\naxes[0].set_ylabel('Pr√©diction')\naxes[0].set_title(f'SVR Optimis√© (R¬≤={best_svr_metrics[\"R2\"]:.3f})')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Comparaison mod√®les\nmodels_comparison = pd.DataFrame({\n    'Mod√®le': ['Linear', 'Ridge', 'Lasso', 'SVR Linear', 'SVR RBF'],\n    'RMSE': [test_metrics['RMSE'], ridge_metrics['RMSE'], \n             lasso_metrics['RMSE'], svr_lin_metrics['RMSE'], svr_rbf_metrics['RMSE']],\n    'R¬≤': [test_metrics['R2'], ridge_metrics['R2'], \n           lasso_metrics['R2'], svr_lin_metrics['R2'], svr_rbf_metrics['R2']]\n})\n\nx_pos = np.arange(len(models_comparison))\nwidth = 0.35\n\nbars1 = axes[1].bar(x_pos - width/2, models_comparison['RMSE'], width, label='RMSE')\nbars2 = axes[1].bar(x_pos + width/2, models_comparison['R¬≤'], width, label='R¬≤')\n\naxes[1].set_xticks(x_pos)\naxes[1].set_xticklabels(models_comparison['Mod√®le'], rotation=45, ha='right')\naxes[1].set_ylabel('Score')\naxes[1].set_title('Comparaison Mod√®les de R√©gression')\naxes[1].legend()\n\n# Annoter les barres\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        axes[1].annotate(f'{height:.3f}',\n                        xy=(bar.get_x() + bar.get_width() / 2, height),\n                        xytext=(0, 3),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CONS√âILS POUR SVR\")\nprint(\"=\" * 70)\nprint(\"\"\"\n‚Ä¢ SVR est puissant mais COMPUTATIONNELLEMENT CO√õTEUX\n‚Ä¢ Scaling des features OBLIGATOIRE\n‚Ä¢ GridSearch peut √™tre tr√®s long\n‚Ä¢ Pour grands datasets, consid√©rez:\n  - LinearSVR (plus rapide que SVR kernel='linear')\n  - R√©duction de features (PCA) avant SVR\n  - √âchantillonnage pour hyperparam√®tres\n\"\"\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#comparaison-finale-de-tous-les-mod√®les",
    "href": "seance8.html#comparaison-finale-de-tous-les-mod√®les",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "7. Comparaison Finale de Tous les Mod√®les",
    "text": "7. Comparaison Finale de Tous les Mod√®les\n\nExercice 7.1: Tableau de Bord Complet\nCr√©ez un tableau de bord comparatif de tous les mod√®les.\n\n\n\n\n\n\nNote\n\n\n\n\n\nprint(\"=\" * 70)\nprint(\"TABLEAU DE BORD COMPARATIF\")\nprint(\"=\" * 70)\n\n# Collecte de tous les r√©sultats\nall_results = []\n\n# Fonction pour ajouter un mod√®le\ndef add_result(name, y_pred, metrics_func=evaluate_model):\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    return {'Mod√®le': name, 'MAE': mae, 'RMSE': rmse, 'R¬≤': r2}\n\n# Ajout de tous les mod√®les\nall_results.append(add_result('Linear', y_test_pred))\nall_results.append(add_result('Ridge (CV)', y_pred_ridge))\nall_results.append(add_result('Lasso (CV)', y_pred_lasso))\nall_results.append(add_result('ElasticNet (CV)', y_pred_elastic))\nall_results.append(add_result('SVR Linear', y_pred_svr_lin))\nall_results.append(add_result('SVR RBF', y_pred_svr_rbf))\n\ndf_all_results = pd.DataFrame(all_results)\n\n# Tri par R¬≤\ndf_all_results = df_all_results.sort_values('R¬≤', ascending=False).reset_index(drop=True)\n\nprint(\"\\nClassement par R¬≤:\")\nprint(df_all_results.to_string(index=False))\n\n# Visualisation\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\n# 1. Barplot R¬≤\nx_pos = np.arange(len(df_all_results))\naxes[0, 0].barh(x_pos, df_all_results['R¬≤'], color='skyblue')\naxes[0, 0].set_yticks(x_pos)\naxes[0, 0].set_yticklabels(df_all_results['Mod√®le'])\naxes[0, 0].set_xlabel('R¬≤ Score')\naxes[0, 0].set_title('Comparaison R¬≤ des Mod√®les')\naxes[0, 0].invert_yaxis()  # Meilleur en haut\n\n# Annoter les valeurs\nfor i, v in enumerate(df_all_results['R¬≤']):\n    axes[0, 0].text(v + 0.001, i, f'{v:.4f}', va='center')\n\n# 2. Comparaison MAE vs RMSE\nscatter = axes[0, 1].scatter(df_all_results['MAE'], df_all_results['RMSE'], \n                             s=200, alpha=0.6, c=df_all_results['R¬≤'], cmap='viridis')\nfor i, row in df_all_results.iterrows():\n    axes[0, 1].annotate(row['Mod√®le'], (row['MAE'], row['RMSE']), fontsize=8)\naxes[0, 1].set_xlabel('MAE')\naxes[0, 1].set_ylabel('RMSE')\naxes[0, 1].set_title('MAE vs RMSE (couleur = R¬≤)')\nplt.colorbar(scatter, ax=axes[0, 1], label='R¬≤ Score')\n\n# 3. Temps d'entra√Ænement (exemple)\n# Dans un cas r√©el, on mesurerait le temps\ntraining_times = [0.1, 0.2, 0.3, 0.4, 2.0, 5.0]  # valeurs d'exemple\ndf_all_results['Temps(s)'] = training_times\n\naxes[1, 0].scatter(df_all_results['Temps(s)'], df_all_results['R¬≤'], s=100)\nfor i, row in df_all_results.iterrows():\n    axes[1, 0].annotate(row['Mod√®le'], (row['Temps(s)'], row['R¬≤']), fontsize=8)\naxes[1, 0].set_xlabel('Temps d\\'entra√Ænement (s)')\naxes[1, 0].set_ylabel('R¬≤ Score')\naxes[1, 0].set_title('Performance vs Temps d\\'entra√Ænement')\naxes[1, 0].grid(alpha=0.3)\n\n# 4. Heatmap des m√©triques\nmetrics_df = df_all_results.set_index('Mod√®le')[['MAE', 'RMSE', 'R¬≤']]\nsns.heatmap(metrics_df, annot=True, fmt='.4f', cmap='YlOrRd', \n            center=0, ax=axes[1, 1], cbar_kws={'label': 'Valeur'})\naxes[1, 1].set_title('Heatmap des M√©triques par Mod√®le')\naxes[1, 1].tick_params(axis='x', rotation=45)\naxes[1, 1].tick_params(axis='y', rotation=0)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RECOMMANDATIONS FINALES\")\nprint(\"=\" * 70)\n\n# Recommandation bas√©e sur diff√©rents crit√®res\nprint(\"\"\"\n1. Pour PERFORMANCE MAX (R¬≤):\n   ‚Üí {} (R¬≤={:.4f})\n   \n2. Pour INTERPR√âTABILIT√â (coefficients):\n   ‚Üí Lasso (CV) (s√©lection de features)\n   \n3. Pour RAPIDIT√â:\n   ‚Üí Linear ou Ridge (CV)\n   \n4. Pour COMPLEXIT√â NON-LIN√âAIRE:\n   ‚Üí SVR RBF (mais plus lent)\n   \n5. COMPROMIS PERFORMANCE/TEMPS:\n   ‚Üí ElasticNet (CV)\n\"\"\".format(df_all_results.iloc[0]['Mod√®le'], df_all_results.iloc[0]['R¬≤']))\n\n# Sauvegarde des r√©sultats\ndf_all_results.to_csv('resultats_regression_comparaison.csv', index=False)\nprint(\"\\nR√©sultats sauvegard√©s dans 'resultats_regression_comparaison.csv'\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#projet-bonus-regression-avanc√©e",
    "href": "seance8.html#projet-bonus-regression-avanc√©e",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "8. Projet Bonus: Regression Avanc√©e",
    "text": "8. Projet Bonus: Regression Avanc√©e\n\nExercice 8.1: Ensemble Methods\nTestez Random Forest et Gradient Boosting pour la r√©gression.\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# TODO: Impl√©mentez Random Forest et Gradient Boosting\n# Optimisez avec RandomizedSearchCV\n# Comparez avec les mod√®les lin√©aires\nIndices: - Pas besoin de standardisation pour les arbres - Optimisez: n_estimators, max_depth, min_samples_split - M√©trique: RMSE ou MAE - Visualisez l‚Äôimportance des features\n\n\n\n\n\n\nNoteSolution Exercice 8.1\n\n\n\n\n\nprint(\"=\" * 70)\nprint(\"ENSEMBLE METHODS: RANDOM FOREST & GRADIENT BOOSTING\")\nprint(\"=\" * 70)\n\n# Pas besoin de scaling pour les arbres\nX_train_trees = X_train\nX_test_trees = X_test\n\n# 1. Random Forest\nprint(\"\\n1. Random Forest Regressor...\")\nrf = RandomForestRegressor(random_state=42, n_jobs=-1)\n\n# Hyperparam√®tres\nparam_dist_rf = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf_random = RandomizedSearchCV(\n    rf, param_dist_rf, n_iter=20, cv=3, \n    scoring='neg_mean_squared_error', n_jobs=-1, random_state=42\n)\n\nrf_random.fit(X_train_trees, y_train)\nbest_rf = rf_random.best_estimator_\ny_pred_rf = best_rf.predict(X_test_trees)\n\nprint(f\"Meilleurs param√®tres RF: {rf_random.best_params_}\")\nrf_metrics = evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n\n# 2. Gradient Boosting\nprint(\"\\n2. Gradient Boosting Regressor...\")\ngbr = GradientBoostingRegressor(random_state=42)\n\nparam_dist_gbr = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 4, 5],\n    'subsample': [0.8, 0.9, 1.0]\n}\n\ngbr_random = RandomizedSearchCV(\n    gbr, param_dist_gbr, n_iter=15, cv=3,\n    scoring='neg_mean_squared_error', n_jobs=-1, random_state=42\n)\n\ngbr_random.fit(X_train_trees, y_train)\nbest_gbr = gbr_random.best_estimator_\ny_pred_gbr = best_gbr.predict(X_test_trees)\n\nprint(f\"Meilleurs param√®tres GBR: {gbr_random.best_params_}\")\ngbr_metrics = evaluate_model(y_test, y_pred_gbr, \"Gradient Boosting\")\n\n# 3. Comparaison\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON MOD√àLES AVANC√âS\")\nprint(\"=\" * 70)\n\nensemble_results = pd.DataFrame([\n    {'Mod√®le': 'Random Forest', 'MAE': rf_metrics['MAE'], \n     'RMSE': rf_metrics['RMSE'], 'R¬≤': rf_metrics['R2']},\n    {'Mod√®le': 'Gradient Boosting', 'MAE': gbr_metrics['MAE'], \n     'RMSE': gbr_metrics['RMSE'], 'R¬≤': gbr_metrics['R2']},\n    {'Mod√®le': 'Meilleur Lin√©aire', 'MAE': df_all_results.iloc[0]['MAE'],\n     'RMSE': df_all_results.iloc[0]['RMSE'], 'R¬≤': df_all_results.iloc[0]['R¬≤']}\n])\n\nprint(ensemble_results.to_string(index=False))\n\n# 4. Importance des features\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Random Forest\nrf_importances = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Importance': best_rf.feature_importances_\n}).sort_values('Importance', ascending=True)\n\naxes[0].barh(rf_importances['Feature'], rf_importances['Importance'])\naxes[0].set_xlabel('Importance')\naxes[0].set_title('Importance des Features - Random Forest')\n\n# Gradient Boosting\ngbr_importances = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Importance': best_gbr.feature_importances_\n}).sort_values('Importance', ascending=True)\n\naxes[1].barh(gbr_importances['Feature'], gbr_importances['Importance'])\naxes[1].set_xlabel('Importance')\naxes[1].set_title('Importance des Features - Gradient Boosting')\n\nplt.tight_layout()\nplt.show()\n\n# 5. Visualisation pr√©dictions\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].scatter(y_test, y_pred_rf, alpha=0.3, s=10)\naxes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\naxes[0].set_xlabel('Vraie valeur')\naxes[0].set_ylabel('Pr√©diction')\naxes[0].set_title(f'Random Forest (R¬≤={rf_metrics[\"R2\"]:.3f})')\naxes[0].grid(alpha=0.3)\n\naxes[1].scatter(y_test, y_pred_gbr, alpha=0.3, s=10)\naxes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\naxes[1].set_xlabel('Vraie valeur')\naxes[1].set_ylabel('Pr√©diction')\naxes[1].set_title(f'Gradient Boosting (R¬≤={gbr_metrics[\"R2\"]:.3f})')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CONCLUSION ENSEMBLE METHODS\")\nprint(\"=\" * 70)\nprint(\"\"\"\n‚Ä¢ Random Forest et Gradient Boosting PERFORMENT TR√àS BIEN\n‚Ä¢ Pas besoin de feature scaling\n‚Ä¢ Capturent relations non-lin√©aires complexes\n‚Ä¢ MOINS INTERPR√âTABLES que les mod√®les lin√©aires\n‚Ä¢ PLUS LENTS √† entra√Æner\n‚Ä¢ Risque de surapprentissage si pas bien r√©gularis√©s\n\n‚Üí Recommandation: Utiliser pour comp√©titions Kaggle\n‚Üí Pour production: Privil√©gier mod√®les plus simples si performance similaire\n\"\"\")",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance8.html#conclusion",
    "href": "seance8.html#conclusion",
    "title": "S√©ance 8: TP3 - R√©gression & Optimisation",
    "section": "Conclusion",
    "text": "Conclusion\n\nR√©sum√© des Points Cl√©s\n\nPr√©traitement:\n\nStandardisation cruciale pour mod√®les r√©gularis√©s et SVR\nSplit stratifi√© (si target stratifiable)\nPas de data leakage\n\nMod√®les Lin√©aires:\n\nLinear: Simple, rapide, interpr√©table\nRidge: R√©gularisation L2, r√©duit overfitting\nLasso: R√©gularisation L1, s√©lection features\nElasticNet: Compromis L1+L2\n\nMod√®les Non-Lin√©aires:\n\nSVR: Puissant mais lent, sensible aux hyperparam√®tres\nRandom Forest: Robuste, capture non-lin√©arit√©s\nGradient Boosting: Souvent meilleure performance\n\nOptimisation:\n\nUtiliser *CV (RidgeCV, LassoCV) pour alpha automatique\nGridSearch pour petit espace\nRandomizedSearch pour grand espace\n\n√âvaluation:\n\nMultiples m√©triques: MAE, RMSE, R¬≤\nVisualisations: r√©sidus, pr√©dictions vs vraies valeurs\nImportance des features pour interpr√©tation\n\n\n\n\nChecklist de Validation\nAvant de soumettre votre travail:\n\nExploratory Data Analysis compl√®te\nPr√©traitement correct (train/test s√©par√©s)\nAu moins 4 mod√®les compar√©s\nOptimisation hyperparam√®tres avec CV\n√âvaluation sur test set (une seule fois)\nVisualisations claires et annot√©es\nInterpr√©tation des r√©sultats\nCode comment√© et organis√©\n\n\n\nPour Aller Plus Loin\nExtensions possibles:\n\nFeature Engineering:\n\nCr√©er interactions entre features\nTransformations polynomiales\nVariables dummy pour cat√©gorielles\n\nPipeline Scikit-learn:\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', RidgeCV())\n])\nValidation Crois√©e Temporelle:\n\nPour donn√©es chronologiques\nTimeSeriesSplit au lieu de KFold\n\nPr√©diction d‚ÄôIntervalles:\n\nQuantile Regression\nBootstrap pour incertitude\n\nD√©ploiement:\n\nSauvegarde mod√®le (joblib)\nAPI avec FastAPI/Flask\nMonitoring des performances\n\n\nExercices suppl√©mentaires:\n\nTestez PolynomialFeatures + Regression\nImpl√©mentez une validation crois√©e imbriqu√©e\nAjoutez XGBoost ou LightGBM √† la comparaison\nCr√©ez un dashboard interactif avec Plotly\n\nProchain TP: S√©ries Temporelles ou Deep Learning\n\n\n\n\n\n\nTipAstuce Finale\n\n\n\nLa meilleure pratique: Commencez toujours par un mod√®le simple (r√©gression lin√©aire), puis complexifiez si n√©cessaire. Souvent, les mod√®les simples suffisent et sont plus faciles √† maintenir en production!",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>S√©ance 8: TP3 - R√©gression & Optimisation</span>"
    ]
  },
  {
    "objectID": "seance9.html",
    "href": "seance9.html",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "",
    "text": "D√©finitions et Principes\nL‚Äôapprentissage non supervis√© est un type d‚Äôapprentissage o√π le mod√®le apprend √† partir de donn√©es non √©tiquet√©es, sans r√©ponses connues.\nObjectif principal: D√©couvrir des structures, des patterns ou des regroupements naturels dans les donn√©es.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#d√©finitions-et-principes",
    "href": "seance9.html#d√©finitions-et-principes",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "",
    "text": "TipPourquoi l‚Äôapprentissage non supervis√© ?\n\n\n\n\nLes donn√©es √©tiquet√©es sont rares ou co√ªteuses √† obtenir\nExploration de donn√©es inconnues\nR√©duction de dimension pour visualisation\nD√©tection d‚Äôanomalies",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#clustering-regroupement",
    "href": "seance9.html#clustering-regroupement",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "Clustering (Regroupement)",
    "text": "Clustering (Regroupement)\nLe clustering consiste √† regrouper des donn√©es similaires dans des clusters (groupes).\n\nk-means\nL‚Äôalgorithme k-means est l‚Äôune des m√©thodes de clustering les plus populaires.\nPrincipe:\n\nChoisir k points initiaux (centro√Ødes)\nAssigner chaque point au centro√Øde le plus proche\nRecalculer les centro√Ødes (moyenne des points du cluster)\nR√©p√©ter jusqu‚Äô√† convergence\n\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Donn√©es non √©tiquet√©es\nX = np.array([[1, 2], [1, 4], [1, 0],\n              [10, 2], [10, 4], [10, 0]])\n\n# Clustering avec k=2\nkmeans = KMeans(n_clusters=2, random_state=42)\nlabels = kmeans.fit_predict(X)\n\nprint(\"Labels des clusters:\", labels)\nprint(\"Centro√Ødes:\", kmeans.cluster_centers_)\n\nAvantages:\n\nSimple et rapide\n√âvolutif pour grands datasets\nR√©sultats faciles √† interpr√©ter\n\nInconv√©nients:\n\nN√©cessite de sp√©cifier k\nSensible aux valeurs aberrantes\nSuppose des clusters sph√©riques et de taille similaire\n\n\n\nDBSCAN (Density-Based Spatial Clustering)\nDBSCAN regroupe les points bas√©s sur la densit√©.\nParam√®tres cl√©s:\n\neps: distance maximale entre deux points pour √™tre consid√©r√©s voisins\nmin_samples: nombre minimum de points pour former un cluster dense\n\n\nfrom sklearn.cluster import DBSCAN\n\n# Clustering par densit√©\ndbscan = DBSCAN(eps=1.5, min_samples=2)\nlabels = dbscan.fit_predict(X)\n\nprint(\"Labels DBSCAN:\", labels)\n# -1 = bruit (outliers)\n\nAvantages:\n\nPas besoin de sp√©cifier le nombre de clusters\nD√©tecte les clusters de forme arbitraire\nRobuste aux outliers\n\nInconv√©nients:\n\nSensible aux param√®tres eps et min_samples\nDifficult√© avec des densit√©s vari√©es\n\n\n\nAutres m√©thodes\n\nAgglomerative Clustering: approche hi√©rarchique\nGaussian Mixture Models (GMM): mod√®le probabiliste\nMean Shift: bas√© sur la densit√© de noyau",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#mesures-de-qualit√©",
    "href": "seance9.html#mesures-de-qualit√©",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "Mesures de Qualit√©",
    "text": "Mesures de Qualit√©\nComment √©valuer la qualit√© d‚Äôun clustering sans labels vrais ?\n\nSilhouette Score\nMesure de coh√©rence intra-cluster et s√©paration inter-cluster.\nValeurs:\n\nProche de 1: bonne s√©paration\nProche de 0: clusters se chevauchent\nN√©gatif: mauvais clustering\n\n\nfrom sklearn.metrics import silhouette_score\n\nscore = silhouette_score(X, labels)\nprint(f\"Silhouette Score: {score:.3f}\")\n\n\n\nInertie (Elbow Method)\nSomme des distances carr√©es des points √† leur centro√Øde.\n\nimport matplotlib.pyplot as plt\n\ninertias = []\nK = range(1, 10)\n\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(K, inertias, 'bo-')\nplt.xlabel('Nombre de clusters (k)')\nplt.ylabel('Inertie')\nplt.title('M√©thode du coude (Elbow Method)')\nplt.grid(True)\nplt.show()\n\n\n\nDavies-Bouldin Index\nMesure de similarit√© moyenne entre clusters.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#applications-r√©elles",
    "href": "seance9.html#applications-r√©elles",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "Applications R√©elles",
    "text": "Applications R√©elles\n\nSegmentation Client\n\n# Exemple fictif de segmentation client\nimport pandas as pd\n\ndata = {\n    'age': [25, 30, 35, 40, 45, 50, 55, 60],\n    'revenu_annuel_k': [40, 45, 50, 80, 90, 30, 35, 25],\n    'score_depense': [8, 7, 6, 9, 8, 3, 4, 2]\n}\n\ndf = pd.DataFrame(data)\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Normalisation\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df)\n\n# Clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = kmeans.fit_predict(X_scaled)\n\nprint(df.groupby('cluster').mean())\n\n\n\nRegroupement de Documents\n\nGroupement d‚Äôarticles par th√®me\nOrganisation d‚Äôemails\nCat√©gorisation de produits\n\n\n\nAnalyse d‚ÄôImages\n\nSegmentation d‚Äôimage\nRegroupement de pixels similaires\nCompression d‚Äôimage",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#r√©duction-de-dimension",
    "href": "seance9.html#r√©duction-de-dimension",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "R√©duction de Dimension",
    "text": "R√©duction de Dimension\n\nPourquoi r√©duire la dimension ?\n\nVisualisation de donn√©es multidimensionnelles\nR√©duction du bruit\nAcc√©l√©ration des algorithmes\n√âviter le ‚Äúfl√©au de la dimension‚Äù\n\n\n\nPCA (Principal Component Analysis)\nPCA transforme les donn√©es en composantes orthogonales capturant la variance maximale.\n\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\n# Donn√©es de d√©monstration\nnp.random.seed(42)\nX = np.random.randn(100, 5)  # 100 √©chantillons, 5 features\n\n# PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nprint(f\"Variance expliqu√©e: {pca.explained_variance_ratio_}\")\nprint(f\"Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\n\n# Visualisation\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('Premi√®re composante principale')\nplt.ylabel('Deuxi√®me composante principale')\nplt.title('PCA - Visualisation 2D')\nplt.show()\n\n\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding)\nM√©thode non lin√©aire particuli√®rement efficace pour la visualisation.\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)\n\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nplt.xlabel('t-SNE 1')\nplt.ylabel('t-SNE 2')\nplt.title('t-SNE - Visualisation 2D')\nplt.show()",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#exercices-de-r√©flexion",
    "href": "seance9.html#exercices-de-r√©flexion",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "Exercices de R√©flexion",
    "text": "Exercices de R√©flexion\n\n\n\n\n\n\nWarningQuestion 1\n\n\n\nPour chacun des sc√©narios suivants, proposez une m√©thode de clustering adapt√©e et justifiez votre choix :\n\nSegmentation de clients avec des variables d√©mographiques et comportementales\nD√©tection de fraudes dans des transactions bancaires\nRegroupement de documents textuels\nAnalyse de pixels d‚Äôune image satellite\n\n\n\n\n\n\n\n\n\nNoteCorrection Question 1\n\n\n\n\n\na) Segmentation de clients:\n\nM√©thode recommand√©e: k-means\nJustification:\n\nVariables d√©mographiques et comportementales ‚Üí donn√©es num√©riques\nNombre de segments g√©n√©ralement connu √† l‚Äôavance (ex: 3-5 segments)\nBesoin d‚Äôinterpr√©tabilit√© pour le marketing\nRapide et efficace sur grands volumes de clients\n\n\n\n# Exemple de segmentation client\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Pr√©paration\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(client_features)\n\n# K-means avec 4 segments\nkmeans = KMeans(n_clusters=4, random_state=42)\nsegments = kmeans.fit_predict(X_scaled)\n\n# Profilage des segments\nprofiles = pd.DataFrame(X_scaled, columns=feature_names)\nprofiles['segment'] = segments\nprint(profiles.groupby('segment').mean())\n\nb) D√©tection de fraudes:\n\nM√©thode recommand√©e: DBSCAN ou Isolation Forest\nJustification:\n\nFraudes = anomalies (outliers)\nDBSCAN identifie les points de bruit (label -1)\nPas besoin de conna√Ætre le nombre de types de fraude\nD√©tecte des patterns de fraude de formes vari√©es\n\n\n\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN pour d√©tecter les anomalies\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(transactions_features)\n\n# Points anormaux (potentielles fraudes)\nanomalies = transactions_features[labels == -1]\nprint(f\"Nombre de transactions suspectes: {len(anomalies)}\")\n\nc) Regroupement de documents textuels:\n\nM√©thode recommand√©e: k-means sur TF-IDF + Hierarchical Clustering\nJustification:\n\nTF-IDF transforme texte en vecteurs num√©riques\nK-means efficace en haute dimension (nombreux mots)\nHierarchical permet d‚Äôexplorer la hi√©rarchie des th√®mes\nPeut combiner avec topic modeling (LDA)\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n# Vectorisation des textes\nvectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\nX_tfidf = vectorizer.fit_transform(documents)\n\n# Clustering\nkmeans = KMeans(n_clusters=5, random_state=42)\ndoc_clusters = kmeans.fit_predict(X_tfidf)\n\n# Top mots par cluster\nterms = vectorizer.get_feature_names_out()\nfor i in range(5):\n    center = kmeans.cluster_centers_[i]\n    top_terms = [terms[j] for j in center.argsort()[-10:]]\n    print(f\"Cluster {i}: {', '.join(top_terms)}\")\n\nd) Analyse de pixels d‚Äôimage satellite:\n\nM√©thode recommand√©e: k-means ou Mean Shift\nJustification:\n\nSegmentation d‚Äôimage = clustering de pixels (RGB ou multi-spectral)\nK-means rapide pour millions de pixels\nMean Shift d√©tecte automatiquement le nombre de segments\nPeut identifier zones (for√™t, eau, ville, etc.)\n\n\n\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Image satellite (exemple)\n# image shape: (height, width, channels)\npixels = image.reshape(-1, image.shape[2])  # Reshape en (n_pixels, channels)\n\n# K-means sur pixels\nkmeans = KMeans(n_clusters=5, random_state=42)\nlabels = kmeans.fit_predict(pixels)\n\n# Reconstruction de l'image segment√©e\nsegmented_image = labels.reshape(image.shape[:2])\nplt.imshow(segmented_image, cmap='tab10')\nplt.title('Segmentation de l\\'image satellite')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nWarningQuestion 2\n\n\n\nPour un dataset avec 10 000 √©chantillons et 50 features :\n\nExpliquez comment d√©terminer le nombre optimal de clusters pour k-means\nProposez une approche pour visualiser la structure des clusters\nQuel avantage PCA peut-il apporter avant le clustering ?\n\n\n\n\n\n\n\n\n\nNoteCorrection Question 2\n\n\n\n\n\na) D√©terminer le nombre optimal de clusters:\nM√©thode 1: Elbow Method (M√©thode du coude)\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Tester diff√©rentes valeurs de k\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    \n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n\n# Visualisation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Courbe du coude\nax1.plot(K_range, inertias, 'bo-')\nax1.set_xlabel('Nombre de clusters (k)')\nax1.set_ylabel('Inertie')\nax1.set_title('M√©thode du Coude')\nax1.grid(True)\n\n# Silhouette score\nax2.plot(K_range, silhouette_scores, 'go-')\nax2.set_xlabel('Nombre de clusters (k)')\nax2.set_ylabel('Silhouette Score')\nax2.set_title('Score Silhouette')\nax2.grid(True)\n\nplt.show()\n\n# Le k optimal est au \"coude\" de la courbe d'inertie\n# ET avec un bon silhouette score\n\nM√©thode 2: Gap Statistic\n\n# Compare l'inertie observ√©e vs inertie sur donn√©es al√©atoires\ndef gap_statistic(X, k_max=10, n_refs=10):\n    gaps = []\n    for k in range(1, k_max + 1):\n        # Inertie sur donn√©es r√©elles\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(X)\n        real_inertia = kmeans.inertia_\n        \n        # Inertie moyenne sur donn√©es de r√©f√©rence\n        ref_inertias = []\n        for _ in range(n_refs):\n            X_ref = np.random.uniform(X.min(), X.max(), X.shape)\n            kmeans_ref = KMeans(n_clusters=k, random_state=42)\n            kmeans_ref.fit(X_ref)\n            ref_inertias.append(kmeans_ref.inertia_)\n        \n        gap = np.log(np.mean(ref_inertias)) - np.log(real_inertia)\n        gaps.append(gap)\n    \n    return gaps\n\n# K optimal = premier k o√π gap commence √† d√©cro√Ætre\n\nM√©thode 3: Silhouette Analysis d√©taill√©e\n\nfrom sklearn.metrics import silhouette_samples\nimport matplotlib.cm as cm\n\nfor k in [2, 3, 4, 5]:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X)\n    \n    silhouette_vals = silhouette_samples(X, labels)\n    \n    plt.figure(figsize=(10, 6))\n    y_lower = 10\n    \n    for i in range(k):\n        cluster_silhouette_vals = silhouette_vals[labels == i]\n        cluster_silhouette_vals.sort()\n        \n        size = cluster_silhouette_vals.shape[0]\n        y_upper = y_lower + size\n        \n        plt.fill_betweenx(np.arange(y_lower, y_upper),\n                         0, cluster_silhouette_vals,\n                         alpha=0.7)\n        y_lower = y_upper + 10\n    \n    plt.title(f'Silhouette Plot (k={k})')\n    plt.xlabel('Coefficient Silhouette')\n    plt.ylabel('Cluster')\n    plt.axvline(x=silhouette_score(X, labels), color=\"red\", linestyle=\"--\")\n    plt.show()\n\nb) Visualiser la structure des clusters:\nApproche 1: PCA pour r√©duction 2D/3D\n\nfrom sklearn.decomposition import PCA\n\n# R√©duction √† 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                     c=labels, cmap='viridis', alpha=0.6)\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nplt.title('Clusters visualis√©s avec PCA')\nplt.colorbar(scatter, label='Cluster')\nplt.show()\n\nprint(f\"Variance expliqu√©e totale: {sum(pca.explained_variance_ratio_):.2%}\")\n\nApproche 2: t-SNE pour visualisation non-lin√©aire\n\nfrom sklearn.manifold import TSNE\n\n# t-SNE (plus lent mais meilleure visualisation)\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_tsne = tsne.fit_transform(X)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\nplt.title('Clusters visualis√©s avec t-SNE')\nplt.colorbar(label='Cluster')\nplt.show()\n\nApproche 3: Pairplot des features importantes\n\nimport seaborn as sns\n\n# S√©lectionner top features par variance\nfrom sklearn.feature_selection import VarianceThreshold\n\nselector = VarianceThreshold(threshold=0.5)\nX_selected = selector.fit_transform(X)\n\n# Pairplot avec 4-5 features les plus variables\ndf_plot = pd.DataFrame(X_selected[:, :5], columns=[f'F{i}' for i in range(5)])\ndf_plot['cluster'] = labels\n\nsns.pairplot(df_plot, hue='cluster', palette='tab10')\nplt.show()\n\nc) Avantages de PCA avant le clustering:\n1. R√©duction de dimension ‚Üí Efficacit√© computationnelle\n\n# Sans PCA: 50 features\nimport time\n\nstart = time.time()\nkmeans_full = KMeans(n_clusters=5, random_state=42)\nkmeans_full.fit(X)  # X: (10000, 50)\ntime_full = time.time() - start\n\n# Avec PCA: 10 features (gardant 95% de variance)\npca = PCA(n_components=0.95)  # Garde 95% de variance\nX_pca = pca.fit_transform(X)  # X_pca: (10000, ~10)\n\nstart = time.time()\nkmeans_pca = KMeans(n_clusters=5, random_state=42)\nkmeans_pca.fit(X_pca)\ntime_pca = time.time() - start\n\nprint(f\"Temps sans PCA: {time_full:.2f}s\")\nprint(f\"Temps avec PCA: {time_pca:.2f}s\")\nprint(f\"Acc√©l√©ration: {time_full/time_pca:.1f}x\")\nprint(f\"Dimensions r√©duites: {X.shape[1]} ‚Üí {X_pca.shape[1]}\")\n\n2. R√©duction du bruit\n\n# PCA √©limine les composantes de faible variance (souvent du bruit)\npca_full = PCA()\npca_full.fit(X)\n\n# Afficher la variance par composante\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         pca_full.explained_variance_ratio_, 'bo-')\nplt.xlabel('Composante')\nplt.ylabel('Variance expliqu√©e')\nplt.title('Scree Plot')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         np.cumsum(pca_full.explained_variance_ratio_), 'ro-')\nplt.xlabel('Nombre de composantes')\nplt.ylabel('Variance cumul√©e')\nplt.axhline(y=0.95, color='g', linestyle='--', label='95%')\nplt.title('Variance Cumul√©e')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Garder les composantes qui expliquent 95% de la variance\n# ‚Üí √©limine le bruit des derni√®res composantes\n\n3. √âvite la mal√©diction de la dimensionnalit√©\n\n# En haute dimension, les distances deviennent moins significatives\nfrom scipy.spatial.distance import pdist, squareform\n\n# Calcul des distances moyennes\ndistances_full = pdist(X[:100])  # Sur 100 √©chantillons pour rapidit√©\ndistances_pca = pdist(X_pca[:100])\n\nprint(f\"Distance moyenne (50D): {np.mean(distances_full):.2f}\")\nprint(f\"Distance moyenne (10D): {np.mean(distances_pca):.2f}\")\nprint(f\"√âcart-type distances (50D): {np.std(distances_full):.2f}\")\nprint(f\"√âcart-type distances (10D): {np.std(distances_pca):.2f}\")\n\n# En dimension r√©duite, les distances sont plus discriminantes\n\n4. D√©corr√©lation des features\n\n# PCA produit des composantes non-corr√©l√©es\n# ‚Üí Am√©liore k-means qui suppose ind√©pendance\n\n# Corr√©lation avant PCA\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.heatmap(np.corrcoef(X.T), cmap='coolwarm', center=0,\n            cbar_kws={'label': 'Corr√©lation'})\nplt.title('Corr√©lations avant PCA')\n\n# Corr√©lation apr√®s PCA\nplt.subplot(1, 2, 2)\nsns.heatmap(np.corrcoef(X_pca.T), cmap='coolwarm', center=0,\n            cbar_kws={'label': 'Corr√©lation'})\nplt.title('Corr√©lations apr√®s PCA')\n\nplt.tight_layout()\nplt.show()\n\n# Apr√®s PCA: corr√©lations nulles entre composantes\n\nR√©sum√© des avantages:\n\n\n\n\n\n\n\n\nAvantage\nDescription\nImpact\n\n\n\n\nEfficacit√©\n50 ‚Üí 10 dimensions\n5-10x plus rapide\n\n\nD√©bruitage\n√âlimine variance faible\nClusters plus nets\n\n\nDistances\nPlus discriminantes en faible dim\nMeilleur clustering\n\n\nD√©corr√©lation\nFeatures ind√©pendantes\nK-means plus efficace\n\n\nVisualisation\nR√©duction √† 2-3D\nInterpr√©tation facile\n\n\n\n\n\n\n\n\n\n\n\n\nWarningQuestion 3\n\n\n\nImpl√©mentez un pipeline complet de clustering sur le dataset Iris :\n\nChargez les donn√©es (ignorer les labels pour l‚Äôapprentissage non supervis√©)\nAppliquez PCA pour r√©duire √† 2 dimensions\nTestez k-means avec k=2,3,4 et comparez les r√©sultats\nVisualisez les clusters obtenus\nCalculez le silhouette score pour chaque k\n\n\n\n\n\n\n\n\n\nNoteCorrection Question 3\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, adjusted_rand_score\n\n# 1. Chargement des donn√©es (SANS utiliser les labels pour clustering)\nprint(\"=\" * 70)\nprint(\"PIPELINE COMPLET DE CLUSTERING - DATASET IRIS\")\nprint(\"=\" * 70)\n\niris = datasets.load_iris()\nX = iris.data  # Features seulement (ignorer iris.target)\nfeature_names = iris.feature_names\ntrue_labels = iris.target  # Gard√© seulement pour √©valuation finale\n\nprint(f\"\\n1. Chargement des donn√©es:\")\nprint(f\"   Dimensions: {X.shape}\")\nprint(f\"   Features: {feature_names}\")\n\n# Normalisation (important avant PCA)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(f\"   ‚úì Donn√©es normalis√©es\")\n\n# 2. Application de PCA pour r√©duction √† 2D\nprint(f\"\\n2. R√©duction de dimension avec PCA:\")\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"   Variance expliqu√©e par composante: {pca.explained_variance_ratio_}\")\nprint(f\"   Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\nprint(f\"   Dimensions: {X.shape[1]}D ‚Üí {X_pca.shape[1]}D\")\n\n# Visualisation des donn√©es apr√®s PCA (sans clustering)\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                     c=true_labels, cmap='viridis', \n                     alpha=0.6, edgecolors='w')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\nplt.title('Dataset Iris apr√®s PCA (color√© par vraies classes)')\nplt.colorbar(scatter, label='Vraie classe')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 3. Test de k-means avec k=2,3,4\nprint(f\"\\n3. Clustering k-means avec diff√©rentes valeurs de k:\")\nprint(\"-\" * 70)\n\nK_values = [2, 3, 4]\nresults = []\n\nfor k in K_values:\n    # Clustering\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_pca)\n    \n    # M√©triques\n    inertia = kmeans.inertia_\n    silhouette = silhouette_score(X_pca, labels)\n    \n    # Comparaison avec vraies classes (juste pour curiosit√©)\n    ari = adjusted_rand_score(true_labels, labels)\n    \n    results.append({\n        'k': k,\n        'Inertie': inertia,\n        'Silhouette': silhouette,\n        'ARI (vs vrai)': ari,\n        'labels': labels,\n        'centroids': kmeans.cluster_centers_\n    })\n    \n    print(f\"\\nk = {k}:\")\n    print(f\"   Inertie: {inertia:.2f}\")\n    print(f\"   Silhouette Score: {silhouette:.3f}\")\n    print(f\"   Taille des clusters: {np.bincount(labels)}\")\n    print(f\"   ARI (comparaison avec vraies classes): {ari:.3f}\")\n\n# DataFrame des r√©sultats\ndf_results = pd.DataFrame([{k: v for k, v in r.items() if k not in ['labels', 'centroids']} \n                          for r in results])\nprint(f\"\\nüìä Tableau r√©capitulatif:\")\nprint(df_results.to_string(index=False))\n\n# Meilleur k selon silhouette\nbest_k = df_results.loc[df_results['Silhouette'].idxmax(), 'k']\nprint(f\"\\n‚≠ê Meilleur k selon Silhouette Score: {best_k}\")\n\n# 4. Visualisation des clusters pour chaque k\nprint(f\"\\n4. Visualisation des clusters:\")\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, result in enumerate(results):\n    k = result['k']\n    labels = result['labels']\n    centroids = result['centroids']\n    ax = axes[idx]\n    # Scatter plot des points\n    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], \n                        c=labels, cmap='tab10', \n                        alpha=0.6, edgecolors='w', s=50)\n    \n    # Centro√Ødes\n    ax.scatter(centroids[:, 0], centroids[:, 1], \n              c='red', marker='X', s=200, \n              edgecolors='black', linewidths=2,\n              label='Centro√Ødes')\n    \n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n    ax.set_title(f'k={k} (Silhouette={result[\"Silhouette\"]:.3f})')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Colorbar\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\nplt.tight_layout()\nplt.show()\n\n# 5. Analyse approfondie du meilleur k\nprint(f\"\\n5. Analyse d√©taill√©e pour k={best_k}:\")\nprint(\"-\" * 70)\n\nbest_result = [r for r in results if r['k'] == best_k][0]\nbest_labels = best_result['labels']\n\n# Profilage des clusters\nprint(f\"\\nProfilage des clusters (features originales):\")\n\ndf_analysis = pd.DataFrame(X, columns=feature_names)\ndf_analysis['Cluster'] = best_labels\n\ncluster_profiles = df_analysis.groupby('Cluster').agg(['mean', 'std'])\nprint(cluster_profiles)\n\n# Heatmap des caract√©ristiques par cluster\ncluster_means = df_analysis.groupby('Cluster').mean()\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(cluster_means.T, annot=True, fmt='.2f', cmap='YlOrRd',\n            cbar_kws={'label': 'Valeur moyenne'})\nplt.xlabel('Cluster')\nplt.ylabel('Feature')\nplt.title(f'Profil des {best_k} clusters (valeurs moyennes)')\nplt.tight_layout()\nplt.show()\n\n# Comparaison avec les vraies classes (curiosit√© acad√©mique)\nprint(f\"\\nüìà Comparaison avec les vraies esp√®ces d'Iris:\")\nprint(\"(Note: Le clustering est NON SUPERVIS√â, cette comparaison est\")\nprint(\" juste pour comprendre ce que l'algorithme a trouv√©)\")\n\nconfusion_unsupervised = pd.crosstab(\n    pd.Series(true_labels, name='Vraie esp√®ce'),\n    pd.Series(best_labels, name='Cluster trouv√©')\n)\nprint(confusion_unsupervised)\n\n# Conclusion\nprint(f\"\\n\" + \"=\" * 70)\nprint(\"CONCLUSION\")\nprint(\"=\" * 70)\nprint(f\"‚úì PCA a r√©duit les donn√©es de 4D √† 2D\")\nprint(f\"‚úì {sum(pca.explained_variance_ratio_):.1%} de variance pr√©serv√©e\")\nprint(f\"‚úì K optimal selon silhouette: {best_k}\")\nprint(f\"‚úì Silhouette score: {best_result['Silhouette']:.3f}\")\nprint(f\"‚úì Les clusters correspondent {'assez bien' if best_result['ARI (vs vrai)'] &gt; 0.7 else 'partiellement'} aux vraies esp√®ces\")\nprint(f\"  (ARI = {best_result['ARI (vs vrai)']:.3f})\")\n\nR√©sultat attendu:\nLe pipeline devrait r√©v√©ler que:\n\nk=3 est optimal (correspond aux 3 esp√®ces d‚ÄôIris)\nLe silhouette score sera autour de 0.5-0.6\nPCA capture environ 95% de la variance en 2D\nLes clusters trouv√©s correspondent assez bien aux vraies esp√®ces\nUne esp√®ce (Setosa) sera bien s√©par√©e, les deux autres se chevaucheront un peu",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#r√©sum√©-de-la-s√©ance",
    "href": "seance9.html#r√©sum√©-de-la-s√©ance",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "R√©sum√© de la S√©ance",
    "text": "R√©sum√© de la S√©ance\n\n\n\n\n\n\nImportantPoints cl√©s √† retenir\n\n\n\n\nApprentissage non supervis√© = d√©couvrir des patterns dans des donn√©es non √©tiquet√©es\nClustering = regrouper des donn√©es similaires (k-means, DBSCAN, hi√©rarchique)\nMesures de qualit√© : silhouette score, inertie, Davies-Bouldin\nR√©duction de dimension : PCA (lin√©aire), t-SNE (non-lin√©aire)\nApplications : segmentation client, analyse de documents, traitement d‚Äôimage\nD√©fis : choix du nombre de clusters, qualit√© sans v√©rit√© terrain",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance9.html#lectures-compl√©mentaires",
    "href": "seance9.html#lectures-compl√©mentaires",
    "title": "S√©ance 9: Apprentissage Non Supervis√©",
    "section": "Lectures Compl√©mentaires",
    "text": "Lectures Compl√©mentaires\n\nG√©ron, A. (2019) - Chapitre 9: Unsupervised Learning Techniques\nScikit-learn Clustering Documentation\nVisualizing Data using t-SNE",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>S√©ance 9: Apprentissage Non Supervis√©</span>"
    ]
  },
  {
    "objectID": "seance10.html",
    "href": "seance10.html",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "",
    "text": "1. Objectifs du TP\nDans ce TP, vous allez :",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#objectifs-du-tp",
    "href": "seance10.html#objectifs-du-tp",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "",
    "text": "Appliquer diff√©rentes m√©thodes de clustering sur des datasets r√©els\nUtiliser des techniques pour d√©terminer le nombre optimal de clusters\nVisualiser et interpr√©ter les r√©sultats de clustering\nUtiliser PCA pour am√©liorer l‚Äôanalyse et la visualisation\nInterpr√©ter les r√©sultats dans un contexte m√©tier",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#pr√©paration-de-lenvironnement",
    "href": "seance10.html#pr√©paration-de-lenvironnement",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "2. Pr√©paration de l‚ÄôEnvironnement",
    "text": "2. Pr√©paration de l‚ÄôEnvironnement\n\n# Importations n√©cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.manifold import TSNE\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Configuration des graphiques\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"‚úÖ Environnement pr√™t!\")\n\n‚úÖ Environnement pr√™t!",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#dataset-1-mall-customers-segmentation",
    "href": "seance10.html#dataset-1-mall-customers-segmentation",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "3. Dataset 1: Mall Customers Segmentation",
    "text": "3. Dataset 1: Mall Customers Segmentation\n\n3.1 Chargement et Exploration\n\n# Dataset: Caract√©ristiques de clients d'un centre commercial\n# Source: https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python\n\n# Cr√©ation d'un dataset synth√©tique pour l'exemple\nnp.random.seed(42)\nn_samples = 300\n\n# G√©n√©ration de donn√©es\nage = np.random.normal(35, 10, n_samples).clip(18, 70)\nannual_income = np.random.normal(60, 20, n_samples).clip(15, 140)\nspending_score = np.random.normal(50, 25, n_samples).clip(1, 100)\n\n# Cr√©ation de clusters artificiels\n# Cluster 1: Jeunes d√©pensiers\nmask1 = (age &lt; 30) & (spending_score &gt; 60)\nannual_income[mask1] = np.random.normal(40, 5, mask1.sum()).clip(30, 50)\n\n# Cluster 2: Seniors √©conomes\nmask2 = (age &gt; 50) & (spending_score &lt; 40)\nannual_income[mask2] = np.random.normal(70, 10, mask2.sum()).clip(60, 90)\n\n# DataFrame\nmall_data = pd.DataFrame({\n    'Age': age,\n    'Annual_Income_k': annual_income,\n    'Spending_Score': spending_score\n})\n\n# Affichage des premi√®res lignes\nprint(\"üìä Dataset Mall Customers:\")\nprint(f\"Dimensions: {mall_data.shape}\")\nprint(\"\\nPremi√®res lignes:\")\nprint(mall_data.head())\nprint(\"\\nStatistiques descriptives:\")\nprint(mall_data.describe())\n\n# Visualisation 3D\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(mall_data['Age'], \n                     mall_data['Annual_Income_k'], \n                     mall_data['Spending_Score'],\n                     c='blue', alpha=0.6, edgecolors='w', s=50)\nax.set_xlabel('Age')\nax.set_ylabel('Revenu Annuel (k$)')\nax.set_zlabel('Score de D√©pense')\nax.set_title('Distribution des Clients - 3D')\nplt.tight_layout()\nplt.show()\n\nüìä Dataset Mall Customers:\nDimensions: (300, 3)\n\nPremi√®res lignes:\n         Age  Annual_Income_k  Spending_Score\n0  39.967142        43.420100       68.924715\n1  33.617357        48.796379       26.945867\n2  41.476885        74.945872       71.740148\n3  50.230299        72.207405       83.890946\n4  32.658466        59.581968       60.335873\n\nStatistiques descriptives:\n              Age  Annual_Income_k  Spending_Score\ncount  300.000000       300.000000      300.000000\nmean    35.075182        57.599007       51.901542\nstd      9.482022        19.021360       23.928198\nmin     18.000000        15.000000        1.000000\n25%     28.167541        43.164043       35.713119\n50%     35.592195        57.575426       51.068513\n75%     41.266577        70.819138       67.786740\nmax     70.000000       121.577616      100.000000\n\n\n\n\n\n\n\n\n\n\n\n3.2 Pr√©traitement des Donn√©es\n\n# Normalisation des donn√©es\nscaler = StandardScaler()\nmall_scaled = scaler.fit_transform(mall_data)\n\nprint(\"‚úÖ Donn√©es normalis√©es (moyenne=0, √©cart-type=1)\")\nprint(f\"Moyennes apr√®s normalisation: {mall_scaled.mean(axis=0).round(2)}\")\nprint(f\"√âcarts-types apr√®s normalisation: {mall_scaled.std(axis=0).round(2)}\")\n\n‚úÖ Donn√©es normalis√©es (moyenne=0, √©cart-type=1)\nMoyennes apr√®s normalisation: [ 0. -0. -0.]\n√âcarts-types apr√®s normalisation: [1. 1. 1.]\n\n\n\n\n3.3 D√©termination du Nombre Optimal de Clusters\n\n# M√©thode du coude (Elbow Method)\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(mall_scaled)\n    inertias.append(kmeans.inertia_)\n    \n    if k &gt; 1:  # silhouette_score n√©cessite au moins 2 clusters\n        score = silhouette_score(mall_scaled, kmeans.labels_)\n        silhouette_scores.append(score)\n\n# Graphique Elbow Method\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Courbe d'inertie\nax1.plot(K_range, inertias, 'bo-')\nax1.set_xlabel('Nombre de clusters (k)')\nax1.set_ylabel('Inertie')\nax1.set_title('M√©thode du Coude')\nax1.grid(True)\n\n# Score silhouette\nax2.plot(range(2, 11), silhouette_scores, 'go-')\nax2.set_xlabel('Nombre de clusters (k)')\nax2.set_ylabel('Score Silhouette')\nax2.set_title('Score Silhouette par k')\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìà Analyse:\")\nprint(f\"Inertie pour k=3: {inertias[1]:.2f}\")\nprint(f\"Inertie pour k=4: {inertias[2]:.2f}\")\nprint(f\"Silhouette pour k=3: {silhouette_scores[1]:.3f}\")\nprint(f\"Silhouette pour k=4: {silhouette_scores[2]:.3f}\")\n\n\n\n\n\n\n\n\nüìà Analyse:\nInertie pour k=3: 523.08\nInertie pour k=4: 422.27\nSilhouette pour k=3: 0.238\nSilhouette pour k=4: 0.256\n\n\n\n\n3.4 Application de k-means\n\n# Clustering avec k=3 (choisi d'apr√®s l'analyse)\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nmall_data['Cluster_kmeans'] = kmeans.fit_predict(mall_scaled)\n\n# Affichage des r√©sultats\nprint(\"üéØ R√©sultats du clustering k-means (k=3):\")\nprint(f\"Taille des clusters: {np.bincount(mall_data['Cluster_kmeans'])}\")\n\n# Caract√©ristiques par cluster\ncluster_stats = mall_data.groupby('Cluster_kmeans').agg({\n    'Age': ['mean', 'std', 'count'],\n    'Annual_Income_k': ['mean', 'std'],\n    'Spending_Score': ['mean', 'std']\n}).round(2)\n\nprint(\"\\nüìä Statistiques par cluster:\")\nprint(cluster_stats)\n\n# Interpr√©tation m√©tier\nprint(\"\\nüí° Interpr√©tation m√©tier:\")\nprint(\"Cluster 0: Clients moyens (√¢ge et revenu moyens, d√©penses moyennes)\")\nprint(\"Cluster 1: Jeunes d√©pensiers (√¢ge jeune, revenu mod√©r√©, d√©penses √©lev√©es)\")\nprint(\"Cluster 2: Seniors √©conomes (√¢ge √©lev√©, revenu √©lev√©, d√©penses faibles)\")\n\nüéØ R√©sultats du clustering k-means (k=3):\nTaille des clusters: [100 101  99]\n\nüìä Statistiques par cluster:\n                  Age             Annual_Income_k        Spending_Score       \n                 mean   std count            mean    std           mean    std\nCluster_kmeans                                                                \n0               42.03  8.39   100           74.40  14.19          47.83  19.62\n1               29.68  7.12   101           54.36  16.21          33.41  14.86\n2               33.55  8.35    99           43.93  12.03          74.88  15.18\n\nüí° Interpr√©tation m√©tier:\nCluster 0: Clients moyens (√¢ge et revenu moyens, d√©penses moyennes)\nCluster 1: Jeunes d√©pensiers (√¢ge jeune, revenu mod√©r√©, d√©penses √©lev√©es)\nCluster 2: Seniors √©conomes (√¢ge √©lev√©, revenu √©lev√©, d√©penses faibles)\n\n\n\n\n3.5 Visualisation avec PCA\n\n# R√©duction √† 2D avec PCA pour visualisation\npca = PCA(n_components=2)\nmall_pca = pca.fit_transform(mall_scaled)\n\n# Ajout des composantes principales au DataFrame\nmall_data['PCA1'] = mall_pca[:, 0]\nmall_data['PCA2'] = mall_pca[:, 1]\n\nprint(f\"üìâ Variance expliqu√©e par PCA: {pca.explained_variance_ratio_.round(3)}\")\nprint(f\"üìä Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\n\n# Visualisation des clusters avec PCA\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nscatter = plt.scatter(mall_data['PCA1'], mall_data['PCA2'], \n                     c=mall_data['Cluster_kmeans'], cmap='viridis', \n                     alpha=0.7, s=50)\nplt.xlabel(f'Premi√®re Composante Principale ({pca.explained_variance_ratio_[0]:.1%})')\nplt.ylabel(f'Deuxi√®me Composante Principale ({pca.explained_variance_ratio_[1]:.1%})')\nplt.title('Clusters k-means visualis√©s avec PCA')\nplt.colorbar(scatter, label='Cluster')\nplt.grid(True, alpha=0.3)\n\n# Visualisation t-SNE (comparaison)\nplt.subplot(1, 2, 2)\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nmall_tsne = tsne.fit_transform(mall_scaled)\n\nplt.scatter(mall_tsne[:, 0], mall_tsne[:, 1], \n           c=mall_data['Cluster_kmeans'], cmap='viridis', \n           alpha=0.7, s=50)\nplt.xlabel('t-SNE 1')\nplt.ylabel('t-SNE 2')\nplt.title('Clusters k-means visualis√©s avec t-SNE')\nplt.colorbar(scatter, label='Cluster')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nüìâ Variance expliqu√©e par PCA: [0.416 0.323]\nüìä Variance totale expliqu√©e: 73.93%",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#dataset-2-clustering-hi√©rarchique-sur-donn√©es-de-fleurs",
    "href": "seance10.html#dataset-2-clustering-hi√©rarchique-sur-donn√©es-de-fleurs",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "4. Dataset 2: Clustering Hi√©rarchique sur Donn√©es de Fleurs",
    "text": "4. Dataset 2: Clustering Hi√©rarchique sur Donn√©es de Fleurs\n\n4.1 Chargement et Exploration\n\n# Chargement du dataset Iris (sans utiliser les labels pour l'apprentissage non supervis√©)\niris = datasets.load_iris()\niris_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n\nprint(\"üå∏ Dataset Iris (sans les labels):\")\nprint(f\"Dimensions: {iris_data.shape}\")\nprint(\"\\nDescription:\")\nprint(iris_data.describe())\n\n# Matrice de corr√©lation\nplt.figure(figsize=(8, 6))\nsns.heatmap(iris_data.corr(), annot=True, cmap='coolwarm', center=0)\nplt.title('Matrice de Corr√©lation - Dataset Iris')\nplt.tight_layout()\nplt.show()\n\nüå∏ Dataset Iris (sans les labels):\nDimensions: (150, 4)\n\nDescription:\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount         150.000000        150.000000         150.000000   \nmean            5.843333          3.057333           3.758000   \nstd             0.828066          0.435866           1.765298   \nmin             4.300000          2.000000           1.000000   \n25%             5.100000          2.800000           1.600000   \n50%             5.800000          3.000000           4.350000   \n75%             6.400000          3.300000           5.100000   \nmax             7.900000          4.400000           6.900000   \n\n       petal width (cm)  \ncount        150.000000  \nmean           1.199333  \nstd            0.762238  \nmin            0.100000  \n25%            0.300000  \n50%            1.300000  \n75%            1.800000  \nmax            2.500000  \n\n\n\n\n\n\n\n\n\n\n\n4.2 Clustering Hi√©rarchique\n\n# Normalisation\niris_scaled = StandardScaler().fit_transform(iris_data)\n\n# Clustering hi√©rarchique agglom√©ratif\nagg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\niris_labels = agg_clustering.fit_predict(iris_scaled)\n\n# Dendrogramme\nplt.figure(figsize=(12, 5))\n\n# Sous-√©chantillon pour le dendrogramme (pour lisibilit√©)\nlinkage_matrix = linkage(iris_scaled[:50], method='ward')\n\nplt.subplot(1, 2, 1)\ndendrogram(linkage_matrix, truncate_mode='level', p=5)\nplt.xlabel('Indices des √©chantillons')\nplt.ylabel('Distance')\nplt.title('Dendrogramme - Clustering Hi√©rarchique')\nplt.grid(True, alpha=0.3)\n\n# Visualisation avec PCA\nplt.subplot(1, 2, 2)\niris_pca = PCA(n_components=2).fit_transform(iris_scaled)\nplt.scatter(iris_pca[:, 0], iris_pca[:, 1], c=iris_labels, cmap='tab20c', s=50)\nplt.xlabel('Premi√®re Composante Principale')\nplt.ylabel('Deuxi√®me Composante Principale')\nplt.title('Clusters Hi√©rarchiques - Visualisation PCA')\nplt.colorbar(label='Cluster')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# √âvaluation\nprint(\"üìä √âvaluation du clustering hi√©rarchique:\")\nprint(f\"Silhouette Score: {silhouette_score(iris_scaled, iris_labels):.3f}\")\nprint(f\"Calinski-Harabasz Score: {calinski_harabasz_score(iris_scaled, iris_labels):.2f}\")\nprint(f\"Davies-Bouldin Score: {davies_bouldin_score(iris_scaled, iris_labels):.3f}\")\n\n\n\n\n\n\n\n\nüìä √âvaluation du clustering hi√©rarchique:\nSilhouette Score: 0.447\nCalinski-Harabasz Score: 222.72\nDavies-Bouldin Score: 0.803",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#comparaison-des-m√©thodes-de-clustering",
    "href": "seance10.html#comparaison-des-m√©thodes-de-clustering",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "5. Comparaison des M√©thodes de Clustering",
    "text": "5. Comparaison des M√©thodes de Clustering\n\n# Test de diff√©rentes m√©thodes sur le dataset Iris\nmethods = {\n    'K-means (k=3)': KMeans(n_clusters=3, random_state=42, n_init=10),\n    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n    'Agglomerative (Ward)': AgglomerativeClustering(n_clusters=3, linkage='ward'),\n    'Agglomerative (Average)': AgglomerativeClustering(n_clusters=3, linkage='average')\n}\n\nresults = []\n\nfor name, model in methods.items():\n    labels = model.fit_predict(iris_scaled)\n    \n    if len(set(labels)) &gt; 1:  # Au moins 2 clusters\n        silhouette = silhouette_score(iris_scaled, labels)\n        n_clusters = len(set(labels))\n    else:\n        silhouette = np.nan\n        n_clusters = len(set(labels))\n    \n    results.append({\n        'M√©thode': name,\n        'Nombre de clusters': n_clusters,\n        'Silhouette Score': silhouette\n    })\n\nresults_df = pd.DataFrame(results)\nprint(\"üìã Comparaison des m√©thodes de clustering:\")\nprint(results_df.to_string(index=False))\n\nüìã Comparaison des m√©thodes de clustering:\n                M√©thode  Nombre de clusters  Silhouette Score\n          K-means (k=3)                   3          0.459948\n                 DBSCAN                   3          0.356516\n   Agglomerative (Ward)                   3          0.446689\nAgglomerative (Average)                   3          0.480267",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#exercice-pratique-guid√©",
    "href": "seance10.html#exercice-pratique-guid√©",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "6. Exercice Pratique Guid√©",
    "text": "6. Exercice Pratique Guid√©\n\n\n\n\n\n\nWarningExercice 1: Dataset Wine\n\n\n\n\nChargez le dataset Wine de scikit-learn (datasets.load_wine())\nNormalisez les donn√©es\nD√©terminez le nombre optimal de clusters avec la m√©thode du coude et le silhouette score\nAppliquez k-means avec le k optimal\nVisualisez les clusters avec PCA\nInterpr√©tez les r√©sultats en termes de caract√©ristiques des vins\n\n\n\n\n\n\n\n\n\nNoteSolution Exercice 1\n\n\n\n\n\n\nprint(\"üç∑ Dataset Wine - Analyse compl√®te\")\nprint(\"=\"*60)\n\n# 1. Chargement\nwine = datasets.load_wine()\nwine_data = pd.DataFrame(wine.data, columns=wine.feature_names)\n\nprint(f\"\\nüìä Dimensions: {wine_data.shape}\")\nprint(f\"Nombre de classes originales: {len(np.unique(wine.target))}\")\nprint(f\"\\nCaract√©ristiques: {wine.feature_names}\")\n\n# Exploration initiale\nprint(\"\\nüìà Statistiques descriptives:\")\nprint(wine_data.describe())\n\n# 2. Normalisation\nwine_scaled = StandardScaler().fit_transform(wine_data)\nprint(\"\\n‚úÖ Donn√©es normalis√©es\")\n\n# 3. D√©termination du nombre optimal de clusters\nprint(\"\\nüîç D√©termination du nombre optimal de clusters...\")\n\ninertias_wine = []\nsilhouette_scores_wine = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans_wine = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans_wine.fit(wine_scaled)\n    inertias_wine.append(kmeans_wine.inertia_)\n    \n    score = silhouette_score(wine_scaled, kmeans_wine.labels_)\n    silhouette_scores_wine.append(score)\n\n# Visualisation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Courbe d'inertie\nax1.plot(K_range, inertias_wine, 'bo-', linewidth=2, markersize=8)\nax1.set_xlabel('Nombre de clusters (k)', fontsize=12)\nax1.set_ylabel('Inertie', fontsize=12)\nax1.set_title('M√©thode du Coude - Dataset Wine', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.axvline(x=3, color='r', linestyle='--', alpha=0.5, label='k optimal sugg√©r√©')\nax1.legend()\n\n# Score silhouette\nax2.plot(K_range, silhouette_scores_wine, 'go-', linewidth=2, markersize=8)\nax2.set_xlabel('Nombre de clusters (k)', fontsize=12)\nax2.set_ylabel('Score Silhouette', fontsize=12)\nax2.set_title('Score Silhouette - Dataset Wine', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\nax2.axvline(x=3, color='r', linestyle='--', alpha=0.5, label='k optimal sugg√©r√©')\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Analyse des scores\nprint(\"\\nüìä Analyse des m√©triques:\")\nfor i, k in enumerate(K_range):\n    print(f\"k={k}: Inertie={inertias_wine[i]:.2f}, Silhouette={silhouette_scores_wine[i]:.3f}\")\n\n# Identification du k optimal\noptimal_k = K_range[np.argmax(silhouette_scores_wine)]\nprint(f\"\\nüéØ Nombre optimal de clusters sugg√©r√©: k={optimal_k}\")\n\n# 4. Application de k-means avec k optimal\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nwine_clusters = kmeans_final.fit_predict(wine_scaled)\n\nprint(f\"\\n‚úÖ Clustering effectu√© avec k={optimal_k}\")\nprint(f\"Taille des clusters: {np.bincount(wine_clusters)}\")\n\n# Ajout des clusters au DataFrame\nwine_data['Cluster'] = wine_clusters\n\n# Statistiques par cluster\nprint(\"\\nüìä Caract√©ristiques moyennes par cluster:\")\ncluster_profiles = wine_data.groupby('Cluster').mean()\nprint(cluster_profiles.round(2))\n\n# 5. Visualisation avec PCA\npca_wine = PCA(n_components=2)\nwine_pca = pca_wine.fit_transform(wine_scaled)\n\nprint(f\"\\nüìâ Variance expliqu√©e par PCA:\")\nprint(f\"PC1: {pca_wine.explained_variance_ratio_[0]:.2%}\")\nprint(f\"PC2: {pca_wine.explained_variance_ratio_[1]:.2%}\")\nprint(f\"Total: {sum(pca_wine.explained_variance_ratio_):.2%}\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Clusters identifi√©s\nscatter1 = axes[0].scatter(wine_pca[:, 0], wine_pca[:, 1], \n                          c=wine_clusters, cmap='viridis', \n                          s=100, alpha=0.7, edgecolors='black', linewidth=0.5)\naxes[0].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})', fontsize=12)\naxes[0].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})', fontsize=12)\naxes[0].set_title('Clusters identifi√©s - k-means', fontsize=14, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# Vraies classes (pour comparaison)\nscatter2 = axes[1].scatter(wine_pca[:, 0], wine_pca[:, 1], \n                          c=wine.target, cmap='plasma', \n                          s=100, alpha=0.7, edgecolors='black', linewidth=0.5)\naxes[1].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})', fontsize=12)\naxes[1].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})', fontsize=12)\naxes[1].set_title('Vraies classes (r√©f√©rence)', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\nplt.colorbar(scatter2, ax=axes[1], label='Classe r√©elle')\n\nplt.tight_layout()\nplt.show()\n\n# 6. Interpr√©tation en termes de caract√©ristiques des vins\nprint(\"\\nüçá Interpr√©tation m√©tier - Profils des clusters de vins:\")\nprint(\"=\"*60)\n\nfor cluster_id in range(optimal_k):\n    cluster_mask = wine_data['Cluster'] == cluster_id\n    cluster_subset = wine_data[cluster_mask]\n    \n    print(f\"\\nüç∑ CLUSTER {cluster_id} ({cluster_mask.sum()} vins):\")\n    print(\"-\" * 60)\n    \n    # Caract√©ristiques principales\n    top_features = cluster_profiles.loc[cluster_id].nlargest(5)\n    print(f\"Caract√©ristiques dominantes:\")\n    for feat, val in top_features.items():\n        if feat != 'Cluster':\n            print(f\"  ‚Ä¢ {feat}: {val:.2f}\")\n    \n    # Interpr√©tation qualitative\n    alcohol = cluster_profiles.loc[cluster_id, 'alcohol']\n    color_intensity = cluster_profiles.loc[cluster_id, 'color_intensity']\n    flavanoids = cluster_profiles.loc[cluster_id, 'flavanoids']\n    \n    print(f\"\\nProfil g√©n√©ral:\")\n    if alcohol &gt; 13:\n        print(f\"  ‚Ä¢ Teneur en alcool √©lev√©e ({alcohol:.1f}%)\")\n    elif alcohol &lt; 12:\n        print(f\"  ‚Ä¢ Teneur en alcool mod√©r√©e ({alcohol:.1f}%)\")\n    else:\n        print(f\"  ‚Ä¢ Teneur en alcool moyenne ({alcohol:.1f}%)\")\n    \n    if color_intensity &gt; 5:\n        print(f\"  ‚Ä¢ Couleur intense ({color_intensity:.1f})\")\n    else:\n        print(f\"  ‚Ä¢ Couleur l√©g√®re ({color_intensity:.1f})\")\n    \n    if flavanoids &gt; 2.5:\n        print(f\"  ‚Ä¢ Riche en flavono√Ødes ({flavanoids:.1f})\")\n    else:\n        print(f\"  ‚Ä¢ Pauvre en flavono√Ødes ({flavanoids:.1f})\")\n\n# M√©triques de qualit√© du clustering\nprint(\"\\nüìä √âvaluation de la qualit√© du clustering:\")\nprint(f\"Silhouette Score: {silhouette_score(wine_scaled, wine_clusters):.3f}\")\nprint(f\"Calinski-Harabasz Score: {calinski_harabasz_score(wine_scaled, wine_clusters):.2f}\")\nprint(f\"Davies-Bouldin Score: {davies_bouldin_score(wine_scaled, wine_clusters):.3f}\")\nprint(\"\\nüí° Note: Un bon silhouette score &gt; 0.5, plus il est proche de 1, mieux c'est\")\n\nüç∑ Dataset Wine - Analyse compl√®te\n============================================================\n\nüìä Dimensions: (178, 13)\nNombre de classes originales: 3\n\nCaract√©ristiques: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n\nüìà Statistiques descriptives:\n          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  178.000000  178.000000  178.000000         178.000000  178.000000   \nmean    13.000618    2.336348    2.366517          19.494944   99.741573   \nstd      0.811827    1.117146    0.274344           3.339564   14.282484   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n75%     13.677500    3.082500    2.557500          21.500000  107.000000   \nmax     14.830000    5.800000    3.230000          30.000000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     178.000000  178.000000            178.000000       178.000000   \nmean        2.295112    2.029270              0.361854         1.590899   \nstd         0.625851    0.998859              0.124453         0.572359   \nmin         0.980000    0.340000              0.130000         0.410000   \n25%         1.742500    1.205000              0.270000         1.250000   \n50%         2.355000    2.135000              0.340000         1.555000   \n75%         2.800000    2.875000              0.437500         1.950000   \nmax         3.880000    5.080000              0.660000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       178.000000  178.000000                    178.000000   178.000000  \nmean          5.058090    0.957449                      2.611685   746.893258  \nstd           2.318286    0.228572                      0.709990   314.907474  \nmin           1.280000    0.480000                      1.270000   278.000000  \n25%           3.220000    0.782500                      1.937500   500.500000  \n50%           4.690000    0.965000                      2.780000   673.500000  \n75%           6.200000    1.120000                      3.170000   985.000000  \nmax          13.000000    1.710000                      4.000000  1680.000000  \n\n‚úÖ Donn√©es normalis√©es\n\nüîç D√©termination du nombre optimal de clusters...\n\n\n\n\n\n\n\n\n\n\nüìä Analyse des m√©triques:\nk=2: Inertie=1658.76, Silhouette=0.259\nk=3: Inertie=1277.93, Silhouette=0.285\nk=4: Inertie=1175.43, Silhouette=0.260\nk=5: Inertie=1109.51, Silhouette=0.202\nk=6: Inertie=1046.00, Silhouette=0.237\nk=7: Inertie=981.60, Silhouette=0.204\nk=8: Inertie=935.20, Silhouette=0.157\nk=9: Inertie=889.89, Silhouette=0.150\nk=10: Inertie=845.90, Silhouette=0.144\n\nüéØ Nombre optimal de clusters sugg√©r√©: k=3\n\n‚úÖ Clustering effectu√© avec k=3\nTaille des clusters: [65 51 62]\n\nüìä Caract√©ristiques moyennes par cluster:\n         alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  \\\nCluster                                                            \n0          12.25        1.90  2.23              20.06      92.74   \n1          13.13        3.31  2.42              21.24      98.67   \n2          13.68        2.00  2.47              17.46     107.97   \n\n         total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\nCluster                                                                     \n0                 2.25        2.05                  0.36             1.62   \n1                 1.68        0.82                  0.45             1.15   \n2                 2.85        3.00                  0.29             1.92   \n\n         color_intensity   hue  od280/od315_of_diluted_wines  proline  \nCluster                                                                \n0                   2.97  1.06                          2.80   510.17  \n1                   7.23  0.69                          1.70   619.06  \n2                   5.45  1.07                          3.16  1100.23  \n\nüìâ Variance expliqu√©e par PCA:\nPC1: 36.20%\nPC2: 19.21%\nTotal: 55.41%\n\n\n\n\n\n\n\n\n\n\nüçá Interpr√©tation m√©tier - Profils des clusters de vins:\n============================================================\n\nüç∑ CLUSTER 0 (65 vins):\n------------------------------------------------------------\nCaract√©ristiques dominantes:\n  ‚Ä¢ proline: 510.17\n  ‚Ä¢ magnesium: 92.74\n  ‚Ä¢ alcalinity_of_ash: 20.06\n  ‚Ä¢ alcohol: 12.25\n  ‚Ä¢ color_intensity: 2.97\n\nProfil g√©n√©ral:\n  ‚Ä¢ Teneur en alcool moyenne (12.3%)\n  ‚Ä¢ Couleur l√©g√®re (3.0)\n  ‚Ä¢ Pauvre en flavono√Ødes (2.0)\n\nüç∑ CLUSTER 1 (51 vins):\n------------------------------------------------------------\nCaract√©ristiques dominantes:\n  ‚Ä¢ proline: 619.06\n  ‚Ä¢ magnesium: 98.67\n  ‚Ä¢ alcalinity_of_ash: 21.24\n  ‚Ä¢ alcohol: 13.13\n  ‚Ä¢ color_intensity: 7.23\n\nProfil g√©n√©ral:\n  ‚Ä¢ Teneur en alcool √©lev√©e (13.1%)\n  ‚Ä¢ Couleur intense (7.2)\n  ‚Ä¢ Pauvre en flavono√Ødes (0.8)\n\nüç∑ CLUSTER 2 (62 vins):\n------------------------------------------------------------\nCaract√©ristiques dominantes:\n  ‚Ä¢ proline: 1100.23\n  ‚Ä¢ magnesium: 107.97\n  ‚Ä¢ alcalinity_of_ash: 17.46\n  ‚Ä¢ alcohol: 13.68\n  ‚Ä¢ color_intensity: 5.45\n\nProfil g√©n√©ral:\n  ‚Ä¢ Teneur en alcool √©lev√©e (13.7%)\n  ‚Ä¢ Couleur intense (5.5)\n  ‚Ä¢ Riche en flavono√Ødes (3.0)\n\nüìä √âvaluation de la qualit√© du clustering:\nSilhouette Score: 0.285\nCalinski-Harabasz Score: 70.94\nDavies-Bouldin Score: 1.389\n\nüí° Note: Un bon silhouette score &gt; 0.5, plus il est proche de 1, mieux c'est\n\n\n\n\n\n\n\n\n\n\n\nWarningExercice 2: Clustering DBSCAN\n\n\n\n\nSur le dataset Mall Customers, testez DBSCAN avec diff√©rents param√®tres\nComparez les r√©sultats avec k-means\nVisualisez les clusters obtenus\nIdentifiez les points consid√©r√©s comme bruit (-1)\nAnalysez les avantages/inconv√©nients de DBSCAN pour ce dataset\n\n\n\n\n\n\n\n\n\nNoteSolution Exercice 2\n\n\n\n\n\n\nprint(\"üõçÔ∏è Clustering DBSCAN sur Mall Customers\")\nprint(\"=\"*60)\n\n# Test de diff√©rents param√®tres DBSCAN\neps_values = [0.3, 0.5, 0.7, 1.0]\nmin_samples_values = [3, 5, 10]\n\nprint(\"\\nüîç Test de diff√©rentes configurations DBSCAN:\")\nprint(\"-\" * 60)\n\nbest_config = {'eps': None, 'min_samples': None, 'score': -1, 'n_clusters': 0}\ndbscan_results = []\n\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(mall_scaled)\n        \n        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n        n_noise = list(labels).count(-1)\n        \n        # Calcul du silhouette score (si au moins 2 clusters)\n        if n_clusters &gt;= 2:\n            # Exclure les points de bruit pour le calcul du score\n            mask = labels != -1\n            if mask.sum() &gt; 0:\n                score = silhouette_score(mall_scaled[mask], labels[mask])\n            else:\n                score = -1\n        else:\n            score = -1\n        \n        dbscan_results.append({\n            'eps': eps,\n            'min_samples': min_samples,\n            'n_clusters': n_clusters,\n            'n_noise': n_noise,\n            'silhouette': score\n        })\n        \n        print(f\"eps={eps}, min_samples={min_samples}: \"\n              f\"{n_clusters} clusters, {n_noise} points de bruit, \"\n              f\"silhouette={score:.3f}\")\n        \n        if score &gt; best_config['score'] and n_clusters &gt; 0:\n            best_config = {\n                'eps': eps,\n                'min_samples': min_samples,\n                'score': score,\n                'n_clusters': n_clusters,\n                'labels': labels\n            }\n\nprint(f\"\\nüéØ Meilleure configuration: eps={best_config['eps']}, \"\n      f\"min_samples={best_config['min_samples']}\")\n\n# Application de DBSCAN avec les meilleurs param√®tres\ndbscan_best = DBSCAN(eps=best_config['eps'], min_samples=best_config['min_samples'])\nmall_data['Cluster_DBSCAN'] = dbscan_best.fit_predict(mall_scaled)\n\n# Analyse des r√©sultats\nn_clusters_dbscan = len(set(mall_data['Cluster_DBSCAN'])) - (1 if -1 in mall_data['Cluster_DBSCAN'].values else 0)\nn_noise_dbscan = (mall_data['Cluster_DBSCAN'] == -1).sum()\n\nprint(f\"\\nüìä R√©sultats DBSCAN:\")\nprint(f\"Nombre de clusters: {n_clusters_dbscan}\")\nprint(f\"Nombre de points de bruit: {n_noise_dbscan} ({n_noise_dbscan/len(mall_data)*100:.1f}%)\")\n\n# Statistiques par cluster (sans le bruit)\nprint(\"\\nüìä Taille des clusters (hors bruit):\")\ncluster_counts = mall_data[mall_data['Cluster_DBSCAN'] != -1]['Cluster_DBSCAN'].value_counts().sort_index()\nfor cluster_id, count in cluster_counts.items():\n    print(f\"Cluster {cluster_id}: {count} clients\")\n\n# Comparaison avec k-means\nprint(\"\\n‚öñÔ∏è Comparaison K-means vs DBSCAN:\")\nprint(\"-\" * 60)\nprint(f\"K-means:\")\nprint(f\"  ‚Ä¢ Nombre de clusters: 3 (pr√©d√©fini)\")\nprint(f\"  ‚Ä¢ Silhouette score: {silhouette_score(mall_scaled, mall_data['Cluster_kmeans']):.3f}\")\nprint(f\"  ‚Ä¢ Tous les points assign√©s\")\n\nif n_clusters_dbscan &gt;= 2:\n    mask_dbscan = mall_data['Cluster_DBSCAN'] != -1\n    score_dbscan = silhouette_score(mall_scaled[mask_dbscan], \n                                    mall_data.loc[mask_dbscan, 'Cluster_DBSCAN'])\n    print(f\"\\nDBSCAN:\")\n    print(f\"  ‚Ä¢ Nombre de clusters: {n_clusters_dbscan} (automatique)\")\n    print(f\"  ‚Ä¢ Silhouette score: {score_dbscan:.3f}\")\n    print(f\"  ‚Ä¢ Points de bruit: {n_noise_dbscan}\")\n\n# Visualisation comparative\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# K-means - 2D (Age vs Income)\nax1 = axes[0, 0]\nscatter1 = ax1.scatter(mall_data['Age'], mall_data['Annual_Income_k'],\n                      c=mall_data['Cluster_kmeans'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax1.set_xlabel('Age', fontsize=12)\nax1.set_ylabel('Revenu Annuel (k$)', fontsize=12)\nax1.set_title('K-means: Age vs Revenu', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\nplt.colorbar(scatter1, ax=ax1, label='Cluster')\n\n# DBSCAN - 2D (Age vs Income)\nax2 = axes[0, 1]\nscatter2 = ax2.scatter(mall_data['Age'], mall_data['Annual_Income_k'],\n                      c=mall_data['Cluster_DBSCAN'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax2.set_xlabel('Age', fontsize=12)\nax2.set_ylabel('Revenu Annuel (k$)', fontsize=12)\nax2.set_title('DBSCAN: Age vs Revenu (points noirs = bruit)', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\nplt.colorbar(scatter2, ax=ax2, label='Cluster')\n\n# K-means - PCA\nax3 = axes[1, 0]\nscatter3 = ax3.scatter(mall_data['PCA1'], mall_data['PCA2'],\n                      c=mall_data['Cluster_kmeans'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax3.set_xlabel('PCA1', fontsize=12)\nax3.set_ylabel('PCA2', fontsize=12)\nax3.set_title('K-means: Visualisation PCA', fontsize=14, fontweight='bold')\nax3.grid(True, alpha=0.3)\nplt.colorbar(scatter3, ax=ax3, label='Cluster')\n\n# DBSCAN - PCA\nax4 = axes[1, 1]\nscatter4 = ax4.scatter(mall_data['PCA1'], mall_data['PCA2'],\n                      c=mall_data['Cluster_DBSCAN'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax4.set_xlabel('PCA1', fontsize=12)\nax4.set_ylabel('PCA2', fontsize=12)\nax4.set_title('DBSCAN: Visualisation PCA (points noirs = bruit)', fontsize=14, fontweight='bold')\nax4.grid(True, alpha=0.3)\nplt.colorbar(scatter4, ax=ax4, label='Cluster')\n\nplt.tight_layout()\nplt.show()\n\n# Analyse des points de bruit\nif n_noise_dbscan &gt; 0:\n    noise_points = mall_data[mall_data['Cluster_DBSCAN'] == -1]\n    print(\"\\nüîç Analyse des points de bruit (outliers):\")\n    print(\"-\" * 60)\n    print(f\"Nombre: {len(noise_points)}\")\n    print(\"\\nCaract√©ristiques moyennes des outliers:\")\n    print(noise_points[['Age', 'Annual_Income_k', 'Spending_Score']].describe())\n    \n    print(\"\\nüí° Interpr√©tation:\")\n    print(\"Les points de bruit repr√©sentent des clients atypiques qui ne correspondent\")\n    print(\"√† aucun segment majeur - ils peuvent √™tre des cas particuliers √† traiter\")\n    print(\"individuellement en marketing.\")\n\n# Profils des clusters DBSCAN\nprint(\"\\nüìä Profils des clusters DBSCAN:\")\nprint(\"-\" * 60)\nfor cluster_id in sorted(mall_data['Cluster_DBSCAN'].unique()):\n    if cluster_id != -1:\n        cluster_data = mall_data[mall_data['Cluster_DBSCAN'] == cluster_id]\n        print(f\"\\nCluster {cluster_id} ({len(cluster_data)} clients):\")\n        print(f\"  ‚Ä¢ Age moyen: {cluster_data['Age'].mean():.1f} ans\")\n        print(f\"  ‚Ä¢ Revenu moyen: {cluster_data['Annual_Income_k'].mean():.1f}k$\")\n        print(f\"  ‚Ä¢ Score de d√©pense moyen: {cluster_data['Spending_Score'].mean():.1f}\")\n\n# Avantages et inconv√©nients\nprint(\"\\n‚úÖ Avantages de DBSCAN pour ce dataset:\")\nprint(\"-\" * 60)\nprint(\"1. D√©tection automatique du nombre de clusters\")\nprint(\"2. Identification des outliers (points atypiques)\")\nprint(\"3. Capacit√© √† d√©tecter des clusters de formes non-sph√©riques\")\nprint(\"4. Robustesse face au bruit dans les donn√©es\")\n\nprint(\"\\n‚ùå Inconv√©nients de DBSCAN pour ce dataset:\")\nprint(\"-\" * 60)\nprint(\"1. Sensibilit√© aux param√®tres eps et min_samples\")\nprint(\"2. Difficult√© avec des clusters de densit√©s variables\")\nprint(\"3. Certains clients sont exclus (marqu√©s comme bruit)\")\nprint(\"4. Moins intuitif pour la segmentation marketing traditionnelle\")\n\nprint(\"\\nüéØ Recommandation:\")\nprint(\"Pour ce dataset de segmentation client:\")\nprint(\"‚Ä¢ K-means est pr√©f√©rable si on veut assigner TOUS les clients √† un segment\")\nprint(\"‚Ä¢ DBSCAN est utile si on veut identifier les clients atypiques s√©par√©ment\")\nprint(\"‚Ä¢ Une approche hybride pourrait combiner les deux m√©thodes\")\n\nüõçÔ∏è Clustering DBSCAN sur Mall Customers\n============================================================\n\nüîç Test de diff√©rentes configurations DBSCAN:\n------------------------------------------------------------\neps=0.3, min_samples=3: 17 clusters, 230 points de bruit, silhouette=0.548\neps=0.3, min_samples=5: 1 clusters, 290 points de bruit, silhouette=-1.000\neps=0.3, min_samples=10: 0 clusters, 300 points de bruit, silhouette=-1.000\neps=0.5, min_samples=3: 9 clusters, 64 points de bruit, silhouette=-0.019\neps=0.5, min_samples=5: 7 clusters, 96 points de bruit, silhouette=0.024\neps=0.5, min_samples=10: 3 clusters, 237 points de bruit, silhouette=0.528\neps=0.7, min_samples=3: 3 clusters, 14 points de bruit, silhouette=0.126\neps=0.7, min_samples=5: 1 clusters, 33 points de bruit, silhouette=-1.000\neps=0.7, min_samples=10: 1 clusters, 69 points de bruit, silhouette=-1.000\neps=1.0, min_samples=3: 1 clusters, 3 points de bruit, silhouette=-1.000\neps=1.0, min_samples=5: 1 clusters, 6 points de bruit, silhouette=-1.000\neps=1.0, min_samples=10: 1 clusters, 11 points de bruit, silhouette=-1.000\n\nüéØ Meilleure configuration: eps=0.3, min_samples=3\n\nüìä R√©sultats DBSCAN:\nNombre de clusters: 17\nNombre de points de bruit: 230 (76.7%)\n\nüìä Taille des clusters (hors bruit):\nCluster 0: 3 clients\nCluster 1: 11 clients\nCluster 2: 4 clients\nCluster 3: 4 clients\nCluster 4: 4 clients\nCluster 5: 4 clients\nCluster 6: 4 clients\nCluster 7: 3 clients\nCluster 8: 4 clients\nCluster 9: 4 clients\nCluster 10: 3 clients\nCluster 11: 3 clients\nCluster 12: 3 clients\nCluster 13: 3 clients\nCluster 14: 4 clients\nCluster 15: 5 clients\nCluster 16: 4 clients\n\n‚öñÔ∏è Comparaison K-means vs DBSCAN:\n------------------------------------------------------------\nK-means:\n  ‚Ä¢ Nombre de clusters: 3 (pr√©d√©fini)\n  ‚Ä¢ Silhouette score: 0.238\n  ‚Ä¢ Tous les points assign√©s\n\nDBSCAN:\n  ‚Ä¢ Nombre de clusters: 17 (automatique)\n  ‚Ä¢ Silhouette score: 0.548\n  ‚Ä¢ Points de bruit: 230\n\n\n\n\n\n\n\n\n\n\nüîç Analyse des points de bruit (outliers):\n------------------------------------------------------------\nNombre: 230\n\nCaract√©ristiques moyennes des outliers:\n              Age  Annual_Income_k  Spending_Score\ncount  230.000000       230.000000      230.000000\nmean    35.789658        58.892871       50.383212\nstd      9.702947        20.438536       25.027695\nmin     18.000000        15.000000        1.000000\n25%     29.421887        43.889952       32.231994\n50%     35.944192        58.870828       49.493150\n75%     42.070654        72.720614       67.044845\nmax     70.000000       121.577616      100.000000\n\nüí° Interpr√©tation:\nLes points de bruit repr√©sentent des clients atypiques qui ne correspondent\n√† aucun segment majeur - ils peuvent √™tre des cas particuliers √† traiter\nindividuellement en marketing.\n\nüìä Profils des clusters DBSCAN:\n------------------------------------------------------------\n\nCluster 0 (3 clients):\n  ‚Ä¢ Age moyen: 51.0 ans\n  ‚Ä¢ Revenu moyen: 63.8k$\n  ‚Ä¢ Score de d√©pense moyen: 31.6\n\nCluster 1 (11 clients):\n  ‚Ä¢ Age moyen: 29.3 ans\n  ‚Ä¢ Revenu moyen: 42.0k$\n  ‚Ä¢ Score de d√©pense moyen: 65.0\n\nCluster 2 (4 clients):\n  ‚Ä¢ Age moyen: 29.4 ans\n  ‚Ä¢ Revenu moyen: 73.3k$\n  ‚Ä¢ Score de d√©pense moyen: 55.7\n\nCluster 3 (4 clients):\n  ‚Ä¢ Age moyen: 34.0 ans\n  ‚Ä¢ Revenu moyen: 64.4k$\n  ‚Ä¢ Score de d√©pense moyen: 45.6\n\nCluster 4 (4 clients):\n  ‚Ä¢ Age moyen: 41.3 ans\n  ‚Ä¢ Revenu moyen: 42.8k$\n  ‚Ä¢ Score de d√©pense moyen: 40.9\n\nCluster 5 (4 clients):\n  ‚Ä¢ Age moyen: 21.1 ans\n  ‚Ä¢ Revenu moyen: 50.0k$\n  ‚Ä¢ Score de d√©pense moyen: 41.4\n\nCluster 6 (4 clients):\n  ‚Ä¢ Age moyen: 27.3 ans\n  ‚Ä¢ Revenu moyen: 63.4k$\n  ‚Ä¢ Score de d√©pense moyen: 45.2\n\nCluster 7 (3 clients):\n  ‚Ä¢ Age moyen: 37.0 ans\n  ‚Ä¢ Revenu moyen: 66.0k$\n  ‚Ä¢ Score de d√©pense moyen: 96.0\n\nCluster 8 (4 clients):\n  ‚Ä¢ Age moyen: 27.9 ans\n  ‚Ä¢ Revenu moyen: 41.1k$\n  ‚Ä¢ Score de d√©pense moyen: 76.3\n\nCluster 9 (4 clients):\n  ‚Ä¢ Age moyen: 37.6 ans\n  ‚Ä¢ Revenu moyen: 49.6k$\n  ‚Ä¢ Score de d√©pense moyen: 22.7\n\nCluster 10 (3 clients):\n  ‚Ä¢ Age moyen: 44.5 ans\n  ‚Ä¢ Revenu moyen: 67.2k$\n  ‚Ä¢ Score de d√©pense moyen: 79.3\n\nCluster 11 (3 clients):\n  ‚Ä¢ Age moyen: 30.6 ans\n  ‚Ä¢ Revenu moyen: 48.1k$\n  ‚Ä¢ Score de d√©pense moyen: 76.2\n\nCluster 12 (3 clients):\n  ‚Ä¢ Age moyen: 40.3 ans\n  ‚Ä¢ Revenu moyen: 49.8k$\n  ‚Ä¢ Score de d√©pense moyen: 36.1\n\nCluster 13 (3 clients):\n  ‚Ä¢ Age moyen: 39.8 ans\n  ‚Ä¢ Revenu moyen: 40.5k$\n  ‚Ä¢ Score de d√©pense moyen: 54.4\n\nCluster 14 (4 clients):\n  ‚Ä¢ Age moyen: 42.0 ans\n  ‚Ä¢ Revenu moyen: 60.1k$\n  ‚Ä¢ Score de d√©pense moyen: 53.0\n\nCluster 15 (5 clients):\n  ‚Ä¢ Age moyen: 18.4 ans\n  ‚Ä¢ Revenu moyen: 36.7k$\n  ‚Ä¢ Score de d√©pense moyen: 86.0\n\nCluster 16 (4 clients):\n  ‚Ä¢ Age moyen: 26.3 ans\n  ‚Ä¢ Revenu moyen: 76.0k$\n  ‚Ä¢ Score de d√©pense moyen: 48.5\n\n‚úÖ Avantages de DBSCAN pour ce dataset:\n------------------------------------------------------------\n1. D√©tection automatique du nombre de clusters\n2. Identification des outliers (points atypiques)\n3. Capacit√© √† d√©tecter des clusters de formes non-sph√©riques\n4. Robustesse face au bruit dans les donn√©es\n\n‚ùå Inconv√©nients de DBSCAN pour ce dataset:\n------------------------------------------------------------\n1. Sensibilit√© aux param√®tres eps et min_samples\n2. Difficult√© avec des clusters de densit√©s variables\n3. Certains clients sont exclus (marqu√©s comme bruit)\n4. Moins intuitif pour la segmentation marketing traditionnelle\n\nüéØ Recommandation:\nPour ce dataset de segmentation client:\n‚Ä¢ K-means est pr√©f√©rable si on veut assigner TOUS les clients √† un segment\n‚Ä¢ DBSCAN est utile si on veut identifier les clients atypiques s√©par√©ment\n‚Ä¢ Une approche hybride pourrait combiner les deux m√©thodes",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#questions-de-r√©flexion",
    "href": "seance10.html#questions-de-r√©flexion",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "7. Questions de R√©flexion",
    "text": "7. Questions de R√©flexion\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nDans l‚Äôanalyse des clients du centre commercial, quelles actions marketing pourriez-vous recommander pour chaque segment identifi√© ?\n\n\n\n\n\n\n\n\nNoteR√©ponse Question 1\n\n\n\n\n\nAnalyse des segments et recommandations marketing:\nCluster 0 - Clients Moyens (Segment Mainstream)\n\nProfil: √Çge moyen (30-45 ans), revenu moyen (50-70k$), d√©penses mod√©r√©es\nTaille estim√©e: ~40% de la client√®le\nActions recommand√©es:\n\nProgrammes de fid√©lit√© avec r√©compenses progressives\nPromotions r√©guli√®res sur des produits de consommation courante\nCommunication √©quilibr√©e entre qualit√© et prix\nCampagnes saisonni√®res cibl√©es\nCross-selling sur produits compl√©mentaires\n\n\nCluster 1 - Jeunes D√©pensiers (Segment Premium Jeune)\n\nProfil: Jeunes (18-30 ans), revenu mod√©r√© (30-50k$), score de d√©pense √©lev√© (&gt;60)\nTaille estim√©e: ~25% de la client√®le\nActions recommand√©es:\n\nMarketing digital et r√©seaux sociaux intensif\nLancements de nouveaux produits tendance\n√âv√©nements exclusifs et exp√©riences immersives\nProgrammes de parrainage avec r√©compenses imm√©diates\nOffres ‚Äúacheter maintenant, payer plus tard‚Äù\nCollaboration avec influenceurs\nCollections capsules et √©ditions limit√©es\n\n\nCluster 2 - Seniors √âconomes (Segment Conservateur Ais√©)\n\nProfil: √Çge √©lev√© (&gt;50 ans), revenu √©lev√© (70-90k$), d√©penses faibles (&lt;40)\nTaille estim√©e: ~35% de la client√®le\nActions recommand√©es:\n\nMise en avant du rapport qualit√©-prix\nService client premium et personnalis√©\nProgrammes de points avec avantages √† long terme\nCommunication par email et courrier traditionnel\nOffres exclusives sur des produits durables et de qualit√©\nConseils personnalis√©s et service apr√®s-vente renforc√©\n√âv√©nements VIP en petit comit√©\n\n\nStrat√©gie globale recommand√©e:\n\nPersonnalisation des campagnes par segment\nA/B testing des messages marketing par cluster\nOptimisation de l‚Äôassortiment produit par profil\nFormation du personnel √† la reconnaissance des profils\nMesure du ROI par segment pour allocation budg√©taire optimale\n\n\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nQuand choisiriez-vous PCA vs t-SNE pour la visualisation des clusters ? Justifiez avec des exemples concrets.\n\n\n\n\n\n\n\n\nNoteR√©ponse Question 2\n\n\n\n\n\nComparaison PCA vs t-SNE pour la visualisation de clusters:\nChoisir PCA quand:\n\nInterpr√©tabilit√© requise\n\nExemple: Rapport pour la direction n√©cessitant de comprendre quelles variables contribuent aux axes\nLes composantes principales sont des combinaisons lin√©aires interpr√©tables\nOn peut expliquer ‚ÄúPC1 repr√©sente 45% de la variance et combine principalement le revenu et l‚Äô√©ducation‚Äù\n\nAnalyse de la variance\n\nExemple: D√©terminer combien de dimensions conserver\nPermet de quantifier l‚Äôinformation perdue: ‚Äú2 composantes capturent 78% de la variance‚Äù\nUtile pour la r√©duction de dimension avant clustering\n\nDatasets de taille moyenne √† grande\n\nExemple: 10,000+ observations\nPCA est beaucoup plus rapide (complexit√© lin√©aire vs quadratique)\nScalabilit√© pour les applications en production\n\nStabilit√© et reproductibilit√©\n\nExemple: Dashboards actualis√©s quotidiennement\nPCA donne toujours le m√™me r√©sultat (d√©terministe)\nt-SNE peut varier √† chaque ex√©cution\n\nRelations lin√©aires √† pr√©server\n\nExemple: Variables √©conomiques corr√©l√©es lin√©airement\nPCA pr√©serve les distances globales\nMeilleur pour comprendre la structure g√©n√©rale\n\n\nChoisir t-SNE quand:\n\nVisualisation pure pour exploration\n\nExemple: Premi√®re exploration d‚Äôun dataset complexe\nR√©v√®le des structures non-lin√©aires cach√©es\nMeilleur pour ‚Äúvoir‚Äù les groupements naturels\n\nStructures non-lin√©aires complexes\n\nExemple: Donn√©es d‚Äôimages, de textes, ou g√©nomiques\nt-SNE peut ‚Äúd√©rouler‚Äù des manifolds non-lin√©aires\nCas o√π PCA montre un nuage de points uniforme\n\nPr√©servation des voisinages locaux\n\nExemple: Analyse de sous-populations fines\nt-SNE garde ensemble les points similaires\nMeilleur pour identifier des micro-clusters\n\nDatasets de petite √† moyenne taille\n\nExemple: &lt;5,000 observations\nLe co√ªt computationnel reste acceptable\nPermet d‚Äôoptimiser les hyperparam√®tres (perplexity)\n\nPr√©sentation/communication visuelle\n\nExemple: Publications scientifiques, pr√©sentations\nSouvent plus ‚Äúimpressionnant‚Äù visuellement\nClusters plus clairement s√©par√©s\n\n\nApproche recommand√©e - Utiliser les DEUX:\n# Strat√©gie optimale pour un projet r√©el\n# 1. PCA d'abord pour comprendre\npca = PCA(n_components=0.95)  # 95% de variance\ndata_pca = pca.fit_transform(data_scaled)\nprint(f\"Dimensions r√©duites de {data.shape[1]} √† {data_pca.shape[1]}\")\n\n# 2. PCA pour le clustering\nkmeans = KMeans(n_clusters=k)\nclusters = kmeans.fit_predict(data_pca)\n\n# 3. PCA pour visualisation interpr√©table\npca_2d = PCA(n_components=2)\nviz_pca = pca_2d.fit_transform(data_scaled)\n\n# 4. t-SNE pour visualisation exploratoire\ntsne_2d = TSNE(n_components=2, perplexity=30)\nviz_tsne = tsne_2d.fit_transform(data_scaled)\n\n# 5. Comparaison visuelle\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.scatter(viz_pca[:, 0], viz_pca[:, 1], c=clusters)\nax1.set_title('PCA - Variance pr√©serv√©e')\nax2.scatter(viz_tsne[:, 0], viz_tsne[:, 1], c=clusters)\nax2.set_title('t-SNE - Voisinages pr√©serv√©s')\nCas pratiques concrets:\n\n\n\n\n\n\n\n\nSituation\nChoix\nRaison\n\n\n\n\nSegmentation clients bancaires (50k clients)\nPCA\nScalabilit√© + interpr√©tabilit√© pour r√©gulation\n\n\nExploration de donn√©es g√©n√©tiques (500 √©chantillons)\nt-SNE\nStructures biologiques non-lin√©aires\n\n\nDashboard temps r√©el e-commerce\nPCA\nRapidit√© + reproductibilit√©\n\n\nPublication recherche (clustering cellules)\nLes deux\nPCA pour m√©thode, t-SNE pour figures\n\n\nR√©duction avant ML (100k lignes)\nPCA\nPerformance computationnelle\n\n\n\nErreurs √† √©viter:\n\n‚ùå Utiliser t-SNE pour des donn√©es tr√®s high-dimensional sans pr√©-r√©duction PCA\n‚ùå Interpr√©ter les distances absolues dans t-SNE (seuls les voisinages comptent)\n‚ùå Utiliser PCA sur donn√©es non-normalis√©es\n‚ùå Fixer perplexity=30 sans tester d‚Äôautres valeurs pour t-SNE\n‚ùå Utiliser t-SNE en production sans consid√©rer le temps de calcul\n\n\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nProposez une m√©trique business pour √©valuer l‚Äôefficacit√© du clustering au-del√† des m√©triques techniques.\n\n\n\n\n\n\n\n\nNoteR√©ponse Question 3\n\n\n\n\n\nM√©triques Business pour √âvaluer l‚ÄôEfficacit√© du Clustering\nLes m√©triques techniques (silhouette score, inertie) mesurent la qualit√© math√©matique, mais pas l‚Äôimpact business. Voici des m√©triques orient√©es valeur:\n\n1. AUGMENTATION DU TAUX DE CONVERSION PAR SEGMENT\nD√©finition:\nTaux conversion post-segmentation - Taux conversion baseline\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √ó 100\n           Taux conversion baseline\nExemple concret:\nBaseline (pas de segmentation): 2.5% conversion\nApr√®s segmentation et marketing cibl√©:\n- Segment Premium: 5.2% (+108%)\n- Segment √âconome: 3.1% (+24%)\n- Segment Moyen: 2.8% (+12%)\n\nM√©trique globale: +45% conversion moyenne pond√©r√©e\nAvantages: - Mesure directe de l‚Äôimpact financier - Facile √† communiquer aux stakeholders - Comparable dans le temps\n\n2. CUSTOMER LIFETIME VALUE (CLV) PAR SEGMENT\nD√©finition:\nCLV_segment = (Revenu moyen par achat √ó Fr√©quence d'achat √ó Dur√©e de vie client)\n              - (Co√ªt acquisition + Co√ªt service)\nApplication:\n# Calcul apr√®s 6 mois de campagnes segment√©es\nsegments_clv = {\n    'Cluster 0': {\n        'CLV': 1250‚Ç¨,\n        'Co√ªt acquisition': 45‚Ç¨,\n        'ROI': 27.8\n    },\n    'Cluster 1': {\n        'CLV': 2100‚Ç¨,\n        'Co√ªt acquisition': 85‚Ç¨,\n        'ROI': 24.7\n    },\n    'Cluster 2': {\n        'CLV': 890‚Ç¨,\n        'Co√ªt acquisition': 35‚Ç¨,\n        'ROI': 25.4\n    }\n}\n\n# M√©trique: CLV pond√©r√© total vs approche non-segment√©e\nCLV_improvement = (weighted_avg_clv_segmented - clv_baseline) / clv_baseline\nKPI d‚Äô√©valuation: - CLV moyen par segment &gt; CLV baseline - Variance du CLV entre segments (plus √©lev√©e = meilleure diff√©renciation) - Budget marketing optimis√© selon CLV/segment\n\n3. TAUX DE R√âTENTION DIFF√âRENTIEL\nD√©finition:\nR√©tention_segment(t) = Clients actifs en t / Clients actifs en t-1\n\nM√©trique: Diff√©rence de r√©tention entre segments vs approche globale\nTableau de bord:\n\n\n\n\n\n\n\n\n\n\nSegment\nR√©tention M+3\nR√©tention M+6\nR√©tention M+12\nAm√©lioration vs baseline\n\n\n\n\nPremium Jeune\n92%\n85%\n78%\n+15%\n\n\nConservateurs\n95%\n91%\n88%\n+22%\n\n\nMainstream\n88%\n79%\n71%\n+8%\n\n\nBaseline\n82%\n73%\n65%\n-\n\n\n\nM√©trique synth√©tique:\nScore_Efficacit√©_R√©tention = Œ£(R√©tention_segment √ó Poids_segment) - R√©tention_baseline\n\n4. EFFICACIT√â OP√âRATIONNELLE DES CAMPAGNES\nD√©finition:\nCo√ªt par Acquisition (CPA) par segment\nROI marketing = (Revenu g√©n√©r√© - Co√ªt campagne) / Co√ªt campagne\nExemple d‚Äô√©valuation:\nCampagne Email Marketing:\n\nSans segmentation:\n- Envois: 100,000\n- Taux ouverture: 18%\n- Conversions: 450\n- CPA: 22‚Ç¨\n\nAvec segmentation (3 messages adapt√©s):\n- Segment A: 35,000 envois, 28% ouverture, 280 conversions, CPA: 15‚Ç¨\n- Segment B: 40,000 envois, 22% ouverture, 200 conversions, CPA: 18‚Ç¨\n- Segment C: 25,000 envois, 32% ouverture, 220 conversions, CPA: 12‚Ç¨\n\nTotal conversions: 700 (+55%)\nCPA moyen pond√©r√©: 15.2‚Ç¨ (-31%)\n\nM√©trique: Efficacit√© = (700-450)/450 √ó (22-15.2)/22 = +90% d'efficacit√©\n\n5. INDICE DE STABILIT√â DES SEGMENTS (ISS)\nD√©finition: Mesure si les segments restent coh√©rents dans le temps (crucial pour strat√©gie long-terme)\ndef indice_stabilite_segment(labels_t1, labels_t2):\n    \"\"\"\n    Mesure la stabilit√©: clients restent-ils dans leur segment?\n    \"\"\"\n    # Matrice de transition\n    transition_matrix = pd.crosstab(labels_t1, labels_t2, normalize='index')\n    \n    # Stabilit√© = moyenne des probabilit√©s diagonales\n    stabilite = np.mean(np.diag(transition_matrix))\n    \n    return stabilite\n\n# Exemple\nstabilite_3_mois = 0.87  # 87% des clients restent dans leur segment\nstabilite_6_mois = 0.82\nstabilite_12_mois = 0.76\n\n# M√©trique: Si stabilit√© &lt; 0.7, les segments ne sont pas fiables\nInterpr√©tation: - ISS &gt; 0.80: Excellente stabilit√©, segments bien d√©finis - ISS 0.60-0.80: Stabilit√© acceptable, ajustements mineurs - ISS &lt; 0.60: Segments peu fiables, revoir la segmentation\n\n6. SCORE DE DIFF√âRENCIATION ACTIONNABLE\nD√©finition: Les segments doivent √™tre suffisamment diff√©rents pour justifier des actions distinctes\ndef score_differenciation_business(segments_data):\n    \"\"\"\n    Mesure si les segments justifient des strat√©gies diff√©rentes\n    \"\"\"\n    scores = []\n    \n    # 1. Diff√©rence de comportement d'achat\n    purchase_variance = segments_data.groupby('segment')['purchase_frequency'].var()\n    scores.append(normalize(purchase_variance.mean()))\n    \n    # 2. Diff√©rence de pr√©f√©rences produits\n    product_affinity_diff = calculate_product_affinity_distance(segments_data)\n    scores.append(normalize(product_affinity_diff))\n    \n    # 3. Diff√©rence de sensibilit√© prix\n    price_sensitivity_diff = calculate_price_elasticity_diff(segments_data)\n    scores.append(normalize(price_sensitivity_diff))\n    \n    # 4. Diff√©rence de canaux pr√©f√©r√©s\n    channel_preference_diff = calculate_channel_divergence(segments_data)\n    scores.append(normalize(channel_preference_diff))\n    \n    # Score final (0-100)\n    return np.mean(scores) * 100\n\n# Interpr√©tation:\n# Score &gt; 70: Segments tr√®s diff√©renci√©s, strat√©gies distinctes justifi√©es\n# Score 40-70: Diff√©renciation mod√©r√©e, personnalisation partielle\n# Score &lt; 40: Segments trop similaires, clustering peu utile\n\n7. M√âTRIQUE COMPOSITE: BUSINESS VALUE SCORE (BVS)\nFormule int√©gr√©e:\nBVS = w1√ó(Lift_Conversion) + w2√ó(Am√©lioration_CLV) + w3√ó(R√©duction_CPA) \n      + w4√ó(Stabilit√©) + w5√ó(Diff√©renciation)\n\nAvec: Œ£wi = 1 (pond√©rations selon priorit√©s business)\nExemple de calcul:\n# Pond√©rations pour un e-commerce\nweights = {\n    'conversion_lift': 0.30,      # Priorit√© maximale\n    'clv_improvement': 0.25,\n    'cpa_reduction': 0.20,\n    'stability': 0.15,\n    'differentiation': 0.10\n}\n\nmetrics = {\n    'conversion_lift': 0.45,      # +45%\n    'clv_improvement': 0.32,      # +32%\n    'cpa_reduction': 0.28,        # -28% (normalis√© positivement)\n    'stability': 0.82,            # ISS = 0.82\n    'differentiation': 0.73       # Score = 73/100\n}\n\nBVS = sum(weights[k] * metrics[k] for k in weights.keys())\n# BVS = 0.456 ‚Üí Score de 45.6/100\n\n# Interpr√©tation:\n# BVS &gt; 0.60: Clustering tr√®s efficace\n# BVS 0.40-0.60: Clustering efficace\n# BVS &lt; 0.40: Revoir la segmentation\n\nDASHBOARD DE SUIVI RECOMMAND√â:\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n‚ïë                 √âVALUATION BUSINESS DU CLUSTERING - Q1 2025      ‚ïë\n‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n‚ïë Business Value Score (BVS):                        47.2/100  [‚úì] ‚ïë\n‚ïë                                                                  ‚ïë\n‚ïë M√©triques D√©taill√©es:                                            ‚ïë\n‚ïë ‚îú‚îÄ Conversion Lift:                                +38%     [‚úì]  ‚ïë\n‚ïë ‚îú‚îÄ CLV Am√©lioration:                               +28%     [‚úì]  ‚ïë\n‚ïë ‚îú‚îÄ CPA R√©duction:                                  -22%     [‚úì]  ‚ïë\n‚ïë ‚îú‚îÄ Indice Stabilit√© (6 mois):                     0.79      [‚úì]  ‚ïë\n‚ïë ‚îî‚îÄ Score Diff√©renciation:                          68/100   [‚úì]  ‚ïë\n‚ïë                                                                  ‚ïë\n‚ïë ROI Global Clustering:                             324%          ‚ïë\n‚ïë Co√ªt impl√©mentation:                               45K‚Ç¨          ‚ïë\n‚ïë Gain annuel estim√©:                                146K‚Ç¨         ‚ïë\n‚ïë                                                                  ‚ïë\n‚ïë Recommandation:                     ‚úÖ Poursuivre et optimiser   ‚ïë     \n‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\nCONCLUSION:\nLa meilleure approche combine: 1. M√©trique primaire: Conversion Lift ou CLV (selon objectif business) 2. M√©trique secondaire: Efficacit√© op√©rationnelle (CPA, ROI marketing) 3. M√©trique de contr√¥le: Stabilit√© et diff√©renciation\nR√®gle d‚Äôor: Si le clustering n‚Äôam√©liore pas au moins une m√©trique business de 15-20% sur 3-6 mois, il faut revoir la segmentation ou son utilisation op√©rationnelle.",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#r√©sum√©-et-bonnes-pratiques",
    "href": "seance10.html#r√©sum√©-et-bonnes-pratiques",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "8. R√©sum√© et Bonnes Pratiques",
    "text": "8. R√©sum√© et Bonnes Pratiques\n\n\n\n\n\n\nImportantChecklist des √©tapes d‚Äôun projet de clustering\n\n\n\n‚úÖ 1. Compr√©hension du probl√®me m√©tier - Quel est l‚Äôobjectif business ? - Comment les clusters seront-ils utilis√©s ?\n‚úÖ 2. Exploration et pr√©traitement - Analyse des distributions - Traitement des valeurs manquantes - Normalisation/standardisation\n‚úÖ 3. D√©termination du nombre de clusters - M√©thode du coude - Score silhouette - Analyse de stabilit√©\n‚úÖ 4. Application des algorithmes - Test de plusieurs m√©thodes - Ajustement des hyperparam√®tres - Validation des r√©sultats\n‚úÖ 5. √âvaluation et interpr√©tation - M√©triques internes (silhouette, etc.) - Visualisation (PCA, t-SNE) - Profilage des clusters - Interpr√©tation m√©tier\n‚úÖ 6. D√©ploiement et monitoring - Documentation des segments - Mise √† jour p√©riodique - Suivi de la stabilit√© des clusters",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  },
  {
    "objectID": "seance10.html#ressources-compl√©mentaires",
    "href": "seance10.html#ressources-compl√©mentaires",
    "title": "S√©ance 10: TP4 - Clustering & R√©duction de Dimension",
    "section": "9. Ressources Compl√©mentaires",
    "text": "9. Ressources Compl√©mentaires\n\nScikit-learn Clustering Guide\nInteractive Clustering Visualization\nPCA vs t-SNE Explained\nCustomer Segmentation Case Study\n\n\nFichiers √† rendre: 1. Notebook Jupyter complet avec code et commentaires 2. Rapport d‚Äôanalyse (1-2 pages) incluant : - M√©thodologie choisie - R√©sultats obtenus - Visualisations cl√©s - Recommandations m√©tier",
    "crumbs": [
      "Partie 1: Machine Learning Fondamental",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>S√©ance 10: TP4 - Clustering & R√©duction de Dimension</span>"
    ]
  }
]