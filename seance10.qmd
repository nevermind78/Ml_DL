# S√©ance 10: TP4 - Clustering & R√©duction de Dimension

::: {.callout-note icon=false}
## Informations de la s√©ance
- **Type**: TP
- **Dur√©e**: 2h
- **Objectifs**: Obj9, Obj10
:::

## 1. Objectifs du TP

Dans ce TP, vous allez :

1. Appliquer diff√©rentes m√©thodes de clustering sur des datasets r√©els
2. Utiliser des techniques pour d√©terminer le nombre optimal de clusters
3. Visualiser et interpr√©ter les r√©sultats de clustering
4. Utiliser PCA pour am√©liorer l'analyse et la visualisation
5. Interpr√©ter les r√©sultats dans un contexte m√©tier

## 2. Pr√©paration de l'Environnement

```{python}
#| echo: true
#| eval: true

# Importations n√©cessaires
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.manifold import TSNE
from scipy.cluster.hierarchy import dendrogram, linkage

# Configuration des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
%matplotlib inline

print("‚úÖ Environnement pr√™t!")
```

## 3. Dataset 1: Mall Customers Segmentation

### 3.1 Chargement et Exploration

```{python}
#| echo: true
#| eval: true

# Dataset: Caract√©ristiques de clients d'un centre commercial
# Source: https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python

# Cr√©ation d'un dataset synth√©tique pour l'exemple
np.random.seed(42)
n_samples = 300

# G√©n√©ration de donn√©es
age = np.random.normal(35, 10, n_samples).clip(18, 70)
annual_income = np.random.normal(60, 20, n_samples).clip(15, 140)
spending_score = np.random.normal(50, 25, n_samples).clip(1, 100)

# Cr√©ation de clusters artificiels
# Cluster 1: Jeunes d√©pensiers
mask1 = (age < 30) & (spending_score > 60)
annual_income[mask1] = np.random.normal(40, 5, mask1.sum()).clip(30, 50)

# Cluster 2: Seniors √©conomes
mask2 = (age > 50) & (spending_score < 40)
annual_income[mask2] = np.random.normal(70, 10, mask2.sum()).clip(60, 90)

# DataFrame
mall_data = pd.DataFrame({
    'Age': age,
    'Annual_Income_k': annual_income,
    'Spending_Score': spending_score
})

# Affichage des premi√®res lignes
print("üìä Dataset Mall Customers:")
print(f"Dimensions: {mall_data.shape}")
print("\nPremi√®res lignes:")
print(mall_data.head())
print("\nStatistiques descriptives:")
print(mall_data.describe())

# Visualisation 3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(mall_data['Age'], 
                     mall_data['Annual_Income_k'], 
                     mall_data['Spending_Score'],
                     c='blue', alpha=0.6, edgecolors='w', s=50)
ax.set_xlabel('Age')
ax.set_ylabel('Revenu Annuel (k$)')
ax.set_zlabel('Score de D√©pense')
ax.set_title('Distribution des Clients - 3D')
plt.tight_layout()
plt.show()
```

### 3.2 Pr√©traitement des Donn√©es

```{python}
#| echo: true
#| eval: true

# Normalisation des donn√©es
scaler = StandardScaler()
mall_scaled = scaler.fit_transform(mall_data)

print("‚úÖ Donn√©es normalis√©es (moyenne=0, √©cart-type=1)")
print(f"Moyennes apr√®s normalisation: {mall_scaled.mean(axis=0).round(2)}")
print(f"√âcarts-types apr√®s normalisation: {mall_scaled.std(axis=0).round(2)}")
```

### 3.3 D√©termination du Nombre Optimal de Clusters

```{python}
#| echo: true
#| eval: true

# M√©thode du coude (Elbow Method)
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(mall_scaled)
    inertias.append(kmeans.inertia_)
    
    if k > 1:  # silhouette_score n√©cessite au moins 2 clusters
        score = silhouette_score(mall_scaled, kmeans.labels_)
        silhouette_scores.append(score)

# Graphique Elbow Method
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Courbe d'inertie
ax1.plot(K_range, inertias, 'bo-')
ax1.set_xlabel('Nombre de clusters (k)')
ax1.set_ylabel('Inertie')
ax1.set_title('M√©thode du Coude')
ax1.grid(True)

# Score silhouette
ax2.plot(range(2, 11), silhouette_scores, 'go-')
ax2.set_xlabel('Nombre de clusters (k)')
ax2.set_ylabel('Score Silhouette')
ax2.set_title('Score Silhouette par k')
ax2.grid(True)

plt.tight_layout()
plt.show()

print("üìà Analyse:")
print(f"Inertie pour k=3: {inertias[1]:.2f}")
print(f"Inertie pour k=4: {inertias[2]:.2f}")
print(f"Silhouette pour k=3: {silhouette_scores[1]:.3f}")
print(f"Silhouette pour k=4: {silhouette_scores[2]:.3f}")
```

### 3.4 Application de k-means

```{python}
#| echo: true
#| eval: true

# Clustering avec k=3 (choisi d'apr√®s l'analyse)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
mall_data['Cluster_kmeans'] = kmeans.fit_predict(mall_scaled)

# Affichage des r√©sultats
print("üéØ R√©sultats du clustering k-means (k=3):")
print(f"Taille des clusters: {np.bincount(mall_data['Cluster_kmeans'])}")

# Caract√©ristiques par cluster
cluster_stats = mall_data.groupby('Cluster_kmeans').agg({
    'Age': ['mean', 'std', 'count'],
    'Annual_Income_k': ['mean', 'std'],
    'Spending_Score': ['mean', 'std']
}).round(2)

print("\nüìä Statistiques par cluster:")
print(cluster_stats)

# Interpr√©tation m√©tier
print("\nüí° Interpr√©tation m√©tier:")
print("Cluster 0: Clients moyens (√¢ge et revenu moyens, d√©penses moyennes)")
print("Cluster 1: Jeunes d√©pensiers (√¢ge jeune, revenu mod√©r√©, d√©penses √©lev√©es)")
print("Cluster 2: Seniors √©conomes (√¢ge √©lev√©, revenu √©lev√©, d√©penses faibles)")
```

### 3.5 Visualisation avec PCA

```{python}
#| echo: true
#| eval: true

# R√©duction √† 2D avec PCA pour visualisation
pca = PCA(n_components=2)
mall_pca = pca.fit_transform(mall_scaled)

# Ajout des composantes principales au DataFrame
mall_data['PCA1'] = mall_pca[:, 0]
mall_data['PCA2'] = mall_pca[:, 1]

print(f"üìâ Variance expliqu√©e par PCA: {pca.explained_variance_ratio_.round(3)}")
print(f"üìä Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}")

# Visualisation des clusters avec PCA
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
scatter = plt.scatter(mall_data['PCA1'], mall_data['PCA2'], 
                     c=mall_data['Cluster_kmeans'], cmap='viridis', 
                     alpha=0.7, s=50)
plt.xlabel(f'Premi√®re Composante Principale ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'Deuxi√®me Composante Principale ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('Clusters k-means visualis√©s avec PCA')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)

# Visualisation t-SNE (comparaison)
plt.subplot(1, 2, 2)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
mall_tsne = tsne.fit_transform(mall_scaled)

plt.scatter(mall_tsne[:, 0], mall_tsne[:, 1], 
           c=mall_data['Cluster_kmeans'], cmap='viridis', 
           alpha=0.7, s=50)
plt.xlabel('t-SNE 1')
plt.ylabel('t-SNE 2')
plt.title('Clusters k-means visualis√©s avec t-SNE')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 4. Dataset 2: Clustering Hi√©rarchique sur Donn√©es de Fleurs

### 4.1 Chargement et Exploration

```{python}
#| echo: true
#| eval: true

# Chargement du dataset Iris (sans utiliser les labels pour l'apprentissage non supervis√©)
iris = datasets.load_iris()
iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)

print("üå∏ Dataset Iris (sans les labels):")
print(f"Dimensions: {iris_data.shape}")
print("\nDescription:")
print(iris_data.describe())

# Matrice de corr√©lation
plt.figure(figsize=(8, 6))
sns.heatmap(iris_data.corr(), annot=True, cmap='coolwarm', center=0)
plt.title('Matrice de Corr√©lation - Dataset Iris')
plt.tight_layout()
plt.show()
```

### 4.2 Clustering Hi√©rarchique

```{python}
#| echo: true
#| eval: true

# Normalisation
iris_scaled = StandardScaler().fit_transform(iris_data)

# Clustering hi√©rarchique agglom√©ratif
agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')
iris_labels = agg_clustering.fit_predict(iris_scaled)

# Dendrogramme
plt.figure(figsize=(12, 5))

# Sous-√©chantillon pour le dendrogramme (pour lisibilit√©)
linkage_matrix = linkage(iris_scaled[:50], method='ward')

plt.subplot(1, 2, 1)
dendrogram(linkage_matrix, truncate_mode='level', p=5)
plt.xlabel('Indices des √©chantillons')
plt.ylabel('Distance')
plt.title('Dendrogramme - Clustering Hi√©rarchique')
plt.grid(True, alpha=0.3)

# Visualisation avec PCA
plt.subplot(1, 2, 2)
iris_pca = PCA(n_components=2).fit_transform(iris_scaled)
plt.scatter(iris_pca[:, 0], iris_pca[:, 1], c=iris_labels, cmap='tab20c', s=50)
plt.xlabel('Premi√®re Composante Principale')
plt.ylabel('Deuxi√®me Composante Principale')
plt.title('Clusters Hi√©rarchiques - Visualisation PCA')
plt.colorbar(label='Cluster')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# √âvaluation
print("üìä √âvaluation du clustering hi√©rarchique:")
print(f"Silhouette Score: {silhouette_score(iris_scaled, iris_labels):.3f}")
print(f"Calinski-Harabasz Score: {calinski_harabasz_score(iris_scaled, iris_labels):.2f}")
print(f"Davies-Bouldin Score: {davies_bouldin_score(iris_scaled, iris_labels):.3f}")
```

## 5. Comparaison des M√©thodes de Clustering

```{python}
#| echo: true
#| eval: true

# Test de diff√©rentes m√©thodes sur le dataset Iris
methods = {
    'K-means (k=3)': KMeans(n_clusters=3, random_state=42, n_init=10),
    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
    'Agglomerative (Ward)': AgglomerativeClustering(n_clusters=3, linkage='ward'),
    'Agglomerative (Average)': AgglomerativeClustering(n_clusters=3, linkage='average')
}

results = []

for name, model in methods.items():
    labels = model.fit_predict(iris_scaled)
    
    if len(set(labels)) > 1:  # Au moins 2 clusters
        silhouette = silhouette_score(iris_scaled, labels)
        n_clusters = len(set(labels))
    else:
        silhouette = np.nan
        n_clusters = len(set(labels))
    
    results.append({
        'M√©thode': name,
        'Nombre de clusters': n_clusters,
        'Silhouette Score': silhouette
    })

results_df = pd.DataFrame(results)
print("üìã Comparaison des m√©thodes de clustering:")
print(results_df.to_string(index=False))
```

## 6. Exercice Pratique Guid√©

::: {.callout-warning icon=false}
## Exercice 1: Dataset Wine
1. Chargez le dataset Wine de scikit-learn (`datasets.load_wine()`)
2. Normalisez les donn√©es
3. D√©terminez le nombre optimal de clusters avec la m√©thode du coude et le silhouette score
4. Appliquez k-means avec le k optimal
5. Visualisez les clusters avec PCA
6. Interpr√©tez les r√©sultats en termes de caract√©ristiques des vins
:::
::: {.callout-note collapse="true"}
### Solution Exercice 1

```{python}
#| echo: true
#| eval: true

print("üç∑ Dataset Wine - Analyse compl√®te")
print("="*60)

# 1. Chargement
wine = datasets.load_wine()
wine_data = pd.DataFrame(wine.data, columns=wine.feature_names)

print(f"\nüìä Dimensions: {wine_data.shape}")
print(f"Nombre de classes originales: {len(np.unique(wine.target))}")
print(f"\nCaract√©ristiques: {wine.feature_names}")

# Exploration initiale
print("\nüìà Statistiques descriptives:")
print(wine_data.describe())

# 2. Normalisation
wine_scaled = StandardScaler().fit_transform(wine_data)
print("\n‚úÖ Donn√©es normalis√©es")

# 3. D√©termination du nombre optimal de clusters
print("\nüîç D√©termination du nombre optimal de clusters...")

inertias_wine = []
silhouette_scores_wine = []
K_range = range(2, 11)

for k in K_range:
    kmeans_wine = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans_wine.fit(wine_scaled)
    inertias_wine.append(kmeans_wine.inertia_)
    
    score = silhouette_score(wine_scaled, kmeans_wine.labels_)
    silhouette_scores_wine.append(score)

# Visualisation
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Courbe d'inertie
ax1.plot(K_range, inertias_wine, 'bo-', linewidth=2, markersize=8)
ax1.set_xlabel('Nombre de clusters (k)', fontsize=12)
ax1.set_ylabel('Inertie', fontsize=12)
ax1.set_title('M√©thode du Coude - Dataset Wine', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.axvline(x=3, color='r', linestyle='--', alpha=0.5, label='k optimal sugg√©r√©')
ax1.legend()

# Score silhouette
ax2.plot(K_range, silhouette_scores_wine, 'go-', linewidth=2, markersize=8)
ax2.set_xlabel('Nombre de clusters (k)', fontsize=12)
ax2.set_ylabel('Score Silhouette', fontsize=12)
ax2.set_title('Score Silhouette - Dataset Wine', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.axvline(x=3, color='r', linestyle='--', alpha=0.5, label='k optimal sugg√©r√©')
ax2.legend()

plt.tight_layout()
plt.show()

# Analyse des scores
print("\nüìä Analyse des m√©triques:")
for i, k in enumerate(K_range):
    print(f"k={k}: Inertie={inertias_wine[i]:.2f}, Silhouette={silhouette_scores_wine[i]:.3f}")

# Identification du k optimal
optimal_k = K_range[np.argmax(silhouette_scores_wine)]
print(f"\nüéØ Nombre optimal de clusters sugg√©r√©: k={optimal_k}")

# 4. Application de k-means avec k optimal
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
wine_clusters = kmeans_final.fit_predict(wine_scaled)

print(f"\n‚úÖ Clustering effectu√© avec k={optimal_k}")
print(f"Taille des clusters: {np.bincount(wine_clusters)}")

# Ajout des clusters au DataFrame
wine_data['Cluster'] = wine_clusters

# Statistiques par cluster
print("\nüìä Caract√©ristiques moyennes par cluster:")
cluster_profiles = wine_data.groupby('Cluster').mean()
print(cluster_profiles.round(2))

# 5. Visualisation avec PCA
pca_wine = PCA(n_components=2)
wine_pca = pca_wine.fit_transform(wine_scaled)

print(f"\nüìâ Variance expliqu√©e par PCA:")
print(f"PC1: {pca_wine.explained_variance_ratio_[0]:.2%}")
print(f"PC2: {pca_wine.explained_variance_ratio_[1]:.2%}")
print(f"Total: {sum(pca_wine.explained_variance_ratio_):.2%}")

# Visualisation
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Clusters identifi√©s
scatter1 = axes[0].scatter(wine_pca[:, 0], wine_pca[:, 1], 
                          c=wine_clusters, cmap='viridis', 
                          s=100, alpha=0.7, edgecolors='black', linewidth=0.5)
axes[0].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})', fontsize=12)
axes[0].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})', fontsize=12)
axes[0].set_title('Clusters identifi√©s - k-means', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)
plt.colorbar(scatter1, ax=axes[0], label='Cluster')

# Vraies classes (pour comparaison)
scatter2 = axes[1].scatter(wine_pca[:, 0], wine_pca[:, 1], 
                          c=wine.target, cmap='plasma', 
                          s=100, alpha=0.7, edgecolors='black', linewidth=0.5)
axes[1].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})', fontsize=12)
axes[1].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})', fontsize=12)
axes[1].set_title('Vraies classes (r√©f√©rence)', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)
plt.colorbar(scatter2, ax=axes[1], label='Classe r√©elle')

plt.tight_layout()
plt.show()

# 6. Interpr√©tation en termes de caract√©ristiques des vins
print("\nüçá Interpr√©tation m√©tier - Profils des clusters de vins:")
print("="*60)

for cluster_id in range(optimal_k):
    cluster_mask = wine_data['Cluster'] == cluster_id
    cluster_subset = wine_data[cluster_mask]
    
    print(f"\nüç∑ CLUSTER {cluster_id} ({cluster_mask.sum()} vins):")
    print("-" * 60)
    
    # Caract√©ristiques principales
    top_features = cluster_profiles.loc[cluster_id].nlargest(5)
    print(f"Caract√©ristiques dominantes:")
    for feat, val in top_features.items():
        if feat != 'Cluster':
            print(f"  ‚Ä¢ {feat}: {val:.2f}")
    
    # Interpr√©tation qualitative
    alcohol = cluster_profiles.loc[cluster_id, 'alcohol']
    color_intensity = cluster_profiles.loc[cluster_id, 'color_intensity']
    flavanoids = cluster_profiles.loc[cluster_id, 'flavanoids']
    
    print(f"\nProfil g√©n√©ral:")
    if alcohol > 13:
        print(f"  ‚Ä¢ Teneur en alcool √©lev√©e ({alcohol:.1f}%)")
    elif alcohol < 12:
        print(f"  ‚Ä¢ Teneur en alcool mod√©r√©e ({alcohol:.1f}%)")
    else:
        print(f"  ‚Ä¢ Teneur en alcool moyenne ({alcohol:.1f}%)")
    
    if color_intensity > 5:
        print(f"  ‚Ä¢ Couleur intense ({color_intensity:.1f})")
    else:
        print(f"  ‚Ä¢ Couleur l√©g√®re ({color_intensity:.1f})")
    
    if flavanoids > 2.5:
        print(f"  ‚Ä¢ Riche en flavono√Ødes ({flavanoids:.1f})")
    else:
        print(f"  ‚Ä¢ Pauvre en flavono√Ødes ({flavanoids:.1f})")

# M√©triques de qualit√© du clustering
print("\nüìä √âvaluation de la qualit√© du clustering:")
print(f"Silhouette Score: {silhouette_score(wine_scaled, wine_clusters):.3f}")
print(f"Calinski-Harabasz Score: {calinski_harabasz_score(wine_scaled, wine_clusters):.2f}")
print(f"Davies-Bouldin Score: {davies_bouldin_score(wine_scaled, wine_clusters):.3f}")
print("\nüí° Note: Un bon silhouette score > 0.5, plus il est proche de 1, mieux c'est")
```
:::

::: {.callout-warning icon=true}
## Exercice 2: Clustering DBSCAN
1. Sur le dataset Mall Customers, testez DBSCAN avec diff√©rents param√®tres
2. Comparez les r√©sultats avec k-means
3. Visualisez les clusters obtenus
4. Identifiez les points consid√©r√©s comme bruit (-1)
5. Analysez les avantages/inconv√©nients de DBSCAN pour ce dataset
:::

::: {.callout-note collapse="true"}
### Solution Exercice 2

```{python}
#| echo: true
#| eval: true

print("üõçÔ∏è Clustering DBSCAN sur Mall Customers")
print("="*60)

# Test de diff√©rents param√®tres DBSCAN
eps_values = [0.3, 0.5, 0.7, 1.0]
min_samples_values = [3, 5, 10]

print("\nüîç Test de diff√©rentes configurations DBSCAN:")
print("-" * 60)

best_config = {'eps': None, 'min_samples': None, 'score': -1, 'n_clusters': 0}
dbscan_results = []

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(mall_scaled)
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        # Calcul du silhouette score (si au moins 2 clusters)
        if n_clusters >= 2:
            # Exclure les points de bruit pour le calcul du score
            mask = labels != -1
            if mask.sum() > 0:
                score = silhouette_score(mall_scaled[mask], labels[mask])
            else:
                score = -1
        else:
            score = -1
        
        dbscan_results.append({
            'eps': eps,
            'min_samples': min_samples,
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'silhouette': score
        })
        
        print(f"eps={eps}, min_samples={min_samples}: "
              f"{n_clusters} clusters, {n_noise} points de bruit, "
              f"silhouette={score:.3f}")
        
        if score > best_config['score'] and n_clusters > 0:
            best_config = {
                'eps': eps,
                'min_samples': min_samples,
                'score': score,
                'n_clusters': n_clusters,
                'labels': labels
            }

print(f"\nüéØ Meilleure configuration: eps={best_config['eps']}, "
      f"min_samples={best_config['min_samples']}")

# Application de DBSCAN avec les meilleurs param√®tres
dbscan_best = DBSCAN(eps=best_config['eps'], min_samples=best_config['min_samples'])
mall_data['Cluster_DBSCAN'] = dbscan_best.fit_predict(mall_scaled)

# Analyse des r√©sultats
n_clusters_dbscan = len(set(mall_data['Cluster_DBSCAN'])) - (1 if -1 in mall_data['Cluster_DBSCAN'].values else 0)
n_noise_dbscan = (mall_data['Cluster_DBSCAN'] == -1).sum()

print(f"\nüìä R√©sultats DBSCAN:")
print(f"Nombre de clusters: {n_clusters_dbscan}")
print(f"Nombre de points de bruit: {n_noise_dbscan} ({n_noise_dbscan/len(mall_data)*100:.1f}%)")

# Statistiques par cluster (sans le bruit)
print("\nüìä Taille des clusters (hors bruit):")
cluster_counts = mall_data[mall_data['Cluster_DBSCAN'] != -1]['Cluster_DBSCAN'].value_counts().sort_index()
for cluster_id, count in cluster_counts.items():
    print(f"Cluster {cluster_id}: {count} clients")

# Comparaison avec k-means
print("\n‚öñÔ∏è Comparaison K-means vs DBSCAN:")
print("-" * 60)
print(f"K-means:")
print(f"  ‚Ä¢ Nombre de clusters: 3 (pr√©d√©fini)")
print(f"  ‚Ä¢ Silhouette score: {silhouette_score(mall_scaled, mall_data['Cluster_kmeans']):.3f}")
print(f"  ‚Ä¢ Tous les points assign√©s")

if n_clusters_dbscan >= 2:
    mask_dbscan = mall_data['Cluster_DBSCAN'] != -1
    score_dbscan = silhouette_score(mall_scaled[mask_dbscan], 
                                    mall_data.loc[mask_dbscan, 'Cluster_DBSCAN'])
    print(f"\nDBSCAN:")
    print(f"  ‚Ä¢ Nombre de clusters: {n_clusters_dbscan} (automatique)")
    print(f"  ‚Ä¢ Silhouette score: {score_dbscan:.3f}")
    print(f"  ‚Ä¢ Points de bruit: {n_noise_dbscan}")

# Visualisation comparative
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# K-means - 2D (Age vs Income)
ax1 = axes[0, 0]
scatter1 = ax1.scatter(mall_data['Age'], mall_data['Annual_Income_k'],
                      c=mall_data['Cluster_kmeans'], cmap='viridis',
                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
ax1.set_xlabel('Age', fontsize=12)
ax1.set_ylabel('Revenu Annuel (k$)', fontsize=12)
ax1.set_title('K-means: Age vs Revenu', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3)
plt.colorbar(scatter1, ax=ax1, label='Cluster')

# DBSCAN - 2D (Age vs Income)
ax2 = axes[0, 1]
scatter2 = ax2.scatter(mall_data['Age'], mall_data['Annual_Income_k'],
                      c=mall_data['Cluster_DBSCAN'], cmap='viridis',
                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
ax2.set_xlabel('Age', fontsize=12)
ax2.set_ylabel('Revenu Annuel (k$)', fontsize=12)
ax2.set_title('DBSCAN: Age vs Revenu (points noirs = bruit)', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3)
plt.colorbar(scatter2, ax=ax2, label='Cluster')

# K-means - PCA
ax3 = axes[1, 0]
scatter3 = ax3.scatter(mall_data['PCA1'], mall_data['PCA2'],
                      c=mall_data['Cluster_kmeans'], cmap='viridis',
                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
ax3.set_xlabel('PCA1', fontsize=12)
ax3.set_ylabel('PCA2', fontsize=12)
ax3.set_title('K-means: Visualisation PCA', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3)
plt.colorbar(scatter3, ax=ax3, label='Cluster')

# DBSCAN - PCA
ax4 = axes[1, 1]
scatter4 = ax4.scatter(mall_data['PCA1'], mall_data['PCA2'],
                      c=mall_data['Cluster_DBSCAN'], cmap='viridis',
                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
ax4.set_xlabel('PCA1', fontsize=12)
ax4.set_ylabel('PCA2', fontsize=12)
ax4.set_title('DBSCAN: Visualisation PCA (points noirs = bruit)', fontsize=14, fontweight='bold')
ax4.grid(True, alpha=0.3)
plt.colorbar(scatter4, ax=ax4, label='Cluster')

plt.tight_layout()
plt.show()

# Analyse des points de bruit
if n_noise_dbscan > 0:
    noise_points = mall_data[mall_data['Cluster_DBSCAN'] == -1]
    print("\nüîç Analyse des points de bruit (outliers):")
    print("-" * 60)
    print(f"Nombre: {len(noise_points)}")
    print("\nCaract√©ristiques moyennes des outliers:")
    print(noise_points[['Age', 'Annual_Income_k', 'Spending_Score']].describe())
    
    print("\nüí° Interpr√©tation:")
    print("Les points de bruit repr√©sentent des clients atypiques qui ne correspondent")
    print("√† aucun segment majeur - ils peuvent √™tre des cas particuliers √† traiter")
    print("individuellement en marketing.")

# Profils des clusters DBSCAN
print("\nüìä Profils des clusters DBSCAN:")
print("-" * 60)
for cluster_id in sorted(mall_data['Cluster_DBSCAN'].unique()):
    if cluster_id != -1:
        cluster_data = mall_data[mall_data['Cluster_DBSCAN'] == cluster_id]
        print(f"\nCluster {cluster_id} ({len(cluster_data)} clients):")
        print(f"  ‚Ä¢ Age moyen: {cluster_data['Age'].mean():.1f} ans")
        print(f"  ‚Ä¢ Revenu moyen: {cluster_data['Annual_Income_k'].mean():.1f}k$")
        print(f"  ‚Ä¢ Score de d√©pense moyen: {cluster_data['Spending_Score'].mean():.1f}")

# Avantages et inconv√©nients
print("\n‚úÖ Avantages de DBSCAN pour ce dataset:")
print("-" * 60)
print("1. D√©tection automatique du nombre de clusters")
print("2. Identification des outliers (points atypiques)")
print("3. Capacit√© √† d√©tecter des clusters de formes non-sph√©riques")
print("4. Robustesse face au bruit dans les donn√©es")

print("\n‚ùå Inconv√©nients de DBSCAN pour ce dataset:")
print("-" * 60)
print("1. Sensibilit√© aux param√®tres eps et min_samples")
print("2. Difficult√© avec des clusters de densit√©s variables")
print("3. Certains clients sont exclus (marqu√©s comme bruit)")
print("4. Moins intuitif pour la segmentation marketing traditionnelle")

print("\nüéØ Recommandation:")
print("Pour ce dataset de segmentation client:")
print("‚Ä¢ K-means est pr√©f√©rable si on veut assigner TOUS les clients √† un segment")
print("‚Ä¢ DBSCAN est utile si on veut identifier les clients atypiques s√©par√©ment")
print("‚Ä¢ Une approche hybride pourrait combiner les deux m√©thodes")
```
:::

## 7. Questions de R√©flexion

::: {.callout-note icon=false}
## Question 1
Dans l'analyse des clients du centre commercial, quelles actions marketing pourriez-vous recommander pour chaque segment identifi√© ?
:::

::: {.callout-note collapse="true"}
## R√©ponse Question 1

**Analyse des segments et recommandations marketing:**

**Cluster 0 - Clients Moyens (Segment Mainstream)**

- **Profil**: √Çge moyen (30-45 ans), revenu moyen (50-70k$), d√©penses mod√©r√©es
- **Taille estim√©e**: ~40% de la client√®le
- **Actions recommand√©es**:
  - Programmes de fid√©lit√© avec r√©compenses progressives
  - Promotions r√©guli√®res sur des produits de consommation courante
  - Communication √©quilibr√©e entre qualit√© et prix
  - Campagnes saisonni√®res cibl√©es
  - Cross-selling sur produits compl√©mentaires

**Cluster 1 - Jeunes D√©pensiers (Segment Premium Jeune)**

- **Profil**: Jeunes (18-30 ans), revenu mod√©r√© (30-50k$), score de d√©pense √©lev√© (>60)
- **Taille estim√©e**: ~25% de la client√®le
- **Actions recommand√©es**:
  - Marketing digital et r√©seaux sociaux intensif
  - Lancements de nouveaux produits tendance
  - √âv√©nements exclusifs et exp√©riences immersives
  - Programmes de parrainage avec r√©compenses imm√©diates
  - Offres "acheter maintenant, payer plus tard"
  - Collaboration avec influenceurs
  - Collections capsules et √©ditions limit√©es

**Cluster 2 - Seniors √âconomes (Segment Conservateur Ais√©)**

- **Profil**: √Çge √©lev√© (>50 ans), revenu √©lev√© (70-90k$), d√©penses faibles (<40)
- **Taille estim√©e**: ~35% de la client√®le
- **Actions recommand√©es**:
  - Mise en avant du rapport qualit√©-prix
  - Service client premium et personnalis√©
  - Programmes de points avec avantages √† long terme
  - Communication par email et courrier traditionnel
  - Offres exclusives sur des produits durables et de qualit√©
  - Conseils personnalis√©s et service apr√®s-vente renforc√©
  - √âv√©nements VIP en petit comit√©

**Strat√©gie globale recommand√©e**:

- Personnalisation des campagnes par segment
- A/B testing des messages marketing par cluster
- Optimisation de l'assortiment produit par profil
- Formation du personnel √† la reconnaissance des profils
- Mesure du ROI par segment pour allocation budg√©taire optimale
:::

::: {.callout-note icon=false}
## Question 2
Quand choisiriez-vous PCA vs t-SNE pour la visualisation des clusters ? Justifiez avec des exemples concrets.
:::

::: {.callout-note collapse="true"}
## R√©ponse Question 2

**Comparaison PCA vs t-SNE pour la visualisation de clusters:**

**Choisir PCA quand:**

1. **Interpr√©tabilit√© requise**

   - Exemple: Rapport pour la direction n√©cessitant de comprendre quelles variables contribuent aux axes
   - Les composantes principales sont des combinaisons lin√©aires interpr√©tables
   - On peut expliquer "PC1 repr√©sente 45% de la variance et combine principalement le revenu et l'√©ducation"

2. **Analyse de la variance**

   - Exemple: D√©terminer combien de dimensions conserver
   - Permet de quantifier l'information perdue: "2 composantes capturent 78% de la variance"
   - Utile pour la r√©duction de dimension avant clustering

3. **Datasets de taille moyenne √† grande**

   - Exemple: 10,000+ observations
   - PCA est beaucoup plus rapide (complexit√© lin√©aire vs quadratique)
   - Scalabilit√© pour les applications en production

4. **Stabilit√© et reproductibilit√©**

   - Exemple: Dashboards actualis√©s quotidiennement
   - PCA donne toujours le m√™me r√©sultat (d√©terministe)
   - t-SNE peut varier √† chaque ex√©cution

5. **Relations lin√©aires √† pr√©server**

   - Exemple: Variables √©conomiques corr√©l√©es lin√©airement
   - PCA pr√©serve les distances globales
   - Meilleur pour comprendre la structure g√©n√©rale

**Choisir t-SNE quand:**

1. **Visualisation pure pour exploration**
   - Exemple: Premi√®re exploration d'un dataset complexe
   - R√©v√®le des structures non-lin√©aires cach√©es
   - Meilleur pour "voir" les groupements naturels

2. **Structures non-lin√©aires complexes**

   - Exemple: Donn√©es d'images, de textes, ou g√©nomiques
   - t-SNE peut "d√©rouler" des manifolds non-lin√©aires
   - Cas o√π PCA montre un nuage de points uniforme

3. **Pr√©servation des voisinages locaux**

   - Exemple: Analyse de sous-populations fines
   - t-SNE garde ensemble les points similaires
   - Meilleur pour identifier des micro-clusters

4. **Datasets de petite √† moyenne taille**

   - Exemple: <5,000 observations
   - Le co√ªt computationnel reste acceptable
   - Permet d'optimiser les hyperparam√®tres (perplexity)

5. **Pr√©sentation/communication visuelle**

   - Exemple: Publications scientifiques, pr√©sentations
   - Souvent plus "impressionnant" visuellement
   - Clusters plus clairement s√©par√©s

**Approche recommand√©e - Utiliser les DEUX:**

```python
# Strat√©gie optimale pour un projet r√©el
# 1. PCA d'abord pour comprendre
pca = PCA(n_components=0.95)  # 95% de variance
data_pca = pca.fit_transform(data_scaled)
print(f"Dimensions r√©duites de {data.shape[1]} √† {data_pca.shape[1]}")

# 2. PCA pour le clustering
kmeans = KMeans(n_clusters=k)
clusters = kmeans.fit_predict(data_pca)

# 3. PCA pour visualisation interpr√©table
pca_2d = PCA(n_components=2)
viz_pca = pca_2d.fit_transform(data_scaled)

# 4. t-SNE pour visualisation exploratoire
tsne_2d = TSNE(n_components=2, perplexity=30)
viz_tsne = tsne_2d.fit_transform(data_scaled)

# 5. Comparaison visuelle
fig, (ax1, ax2) = plt.subplots(1, 2)
ax1.scatter(viz_pca[:, 0], viz_pca[:, 1], c=clusters)
ax1.set_title('PCA - Variance pr√©serv√©e')
ax2.scatter(viz_tsne[:, 0], viz_tsne[:, 1], c=clusters)
ax2.set_title('t-SNE - Voisinages pr√©serv√©s')
```

**Cas pratiques concrets:**

| Situation | Choix | Raison |
|-----------|-------|--------|
| Segmentation clients bancaires (50k clients) | PCA | Scalabilit√© + interpr√©tabilit√© pour r√©gulation |
| Exploration de donn√©es g√©n√©tiques (500 √©chantillons) | t-SNE | Structures biologiques non-lin√©aires |
| Dashboard temps r√©el e-commerce | PCA | Rapidit√© + reproductibilit√© |
| Publication recherche (clustering cellules) | Les deux | PCA pour m√©thode, t-SNE pour figures |
| R√©duction avant ML (100k lignes) | PCA | Performance computationnelle |

**Erreurs √† √©viter:**

- ‚ùå Utiliser t-SNE pour des donn√©es tr√®s high-dimensional sans pr√©-r√©duction PCA
- ‚ùå Interpr√©ter les distances absolues dans t-SNE (seuls les voisinages comptent)
- ‚ùå Utiliser PCA sur donn√©es non-normalis√©es
- ‚ùå Fixer perplexity=30 sans tester d'autres valeurs pour t-SNE
- ‚ùå Utiliser t-SNE en production sans consid√©rer le temps de calcul
:::

::: {.callout-note icon=false}
## Question 3
Proposez une m√©trique business pour √©valuer l'efficacit√© du clustering au-del√† des m√©triques techniques.
:::

::: {.callout-note collapse="true"}
## R√©ponse Question 3

**M√©triques Business pour √âvaluer l'Efficacit√© du Clustering**

Les m√©triques techniques (silhouette score, inertie) mesurent la qualit√© math√©matique, mais pas l'impact business. Voici des m√©triques orient√©es valeur:

---

**1. AUGMENTATION DU TAUX DE CONVERSION PAR SEGMENT**

**D√©finition:**
```
Taux conversion post-segmentation - Taux conversion baseline
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √ó 100
           Taux conversion baseline
```

**Exemple concret:**

```
Baseline (pas de segmentation): 2.5% conversion
Apr√®s segmentation et marketing cibl√©:
- Segment Premium: 5.2% (+108%)
- Segment √âconome: 3.1% (+24%)
- Segment Moyen: 2.8% (+12%)

M√©trique globale: +45% conversion moyenne pond√©r√©e
```

**Avantages:**
- Mesure directe de l'impact financier
- Facile √† communiquer aux stakeholders
- Comparable dans le temps

---

**2. CUSTOMER LIFETIME VALUE (CLV) PAR SEGMENT**

**D√©finition:**
```
CLV_segment = (Revenu moyen par achat √ó Fr√©quence d'achat √ó Dur√©e de vie client)
              - (Co√ªt acquisition + Co√ªt service)
```

**Application:**
```python
# Calcul apr√®s 6 mois de campagnes segment√©es
segments_clv = {
    'Cluster 0': {
        'CLV': 1250‚Ç¨,
        'Co√ªt acquisition': 45‚Ç¨,
        'ROI': 27.8
    },
    'Cluster 1': {
        'CLV': 2100‚Ç¨,
        'Co√ªt acquisition': 85‚Ç¨,
        'ROI': 24.7
    },
    'Cluster 2': {
        'CLV': 890‚Ç¨,
        'Co√ªt acquisition': 35‚Ç¨,
        'ROI': 25.4
    }
}

# M√©trique: CLV pond√©r√© total vs approche non-segment√©e
CLV_improvement = (weighted_avg_clv_segmented - clv_baseline) / clv_baseline
```

**KPI d'√©valuation:**
- CLV moyen par segment > CLV baseline
- Variance du CLV entre segments (plus √©lev√©e = meilleure diff√©renciation)
- Budget marketing optimis√© selon CLV/segment

---

**3. TAUX DE R√âTENTION DIFF√âRENTIEL**

**D√©finition:**
```
R√©tention_segment(t) = Clients actifs en t / Clients actifs en t-1

M√©trique: Diff√©rence de r√©tention entre segments vs approche globale
```

**Tableau de bord:**


| Segment        | R√©tention M+3 | R√©tention M+6 | R√©tention M+12 | Am√©lioration vs baseline |
|----------------|---------------|---------------|----------------|--------------------------|
| Premium Jeune  | 92%           | 85%           | 78%            | +15%                     |
| Conservateurs  | 95%           | 91%           | 88%            | +22%                     |
| Mainstream     | 88%           | 79%           | 71%            | +8%                      |
| Baseline       | 82%           | 73%           | 65%            | -                        |



**M√©trique synth√©tique:**
```
Score_Efficacit√©_R√©tention = Œ£(R√©tention_segment √ó Poids_segment) - R√©tention_baseline
```

---

**4. EFFICACIT√â OP√âRATIONNELLE DES CAMPAGNES**

**D√©finition:**
```
Co√ªt par Acquisition (CPA) par segment
ROI marketing = (Revenu g√©n√©r√© - Co√ªt campagne) / Co√ªt campagne
```

**Exemple d'√©valuation:**
```
Campagne Email Marketing:

Sans segmentation:
- Envois: 100,000
- Taux ouverture: 18%
- Conversions: 450
- CPA: 22‚Ç¨

Avec segmentation (3 messages adapt√©s):
- Segment A: 35,000 envois, 28% ouverture, 280 conversions, CPA: 15‚Ç¨
- Segment B: 40,000 envois, 22% ouverture, 200 conversions, CPA: 18‚Ç¨
- Segment C: 25,000 envois, 32% ouverture, 220 conversions, CPA: 12‚Ç¨

Total conversions: 700 (+55%)
CPA moyen pond√©r√©: 15.2‚Ç¨ (-31%)

M√©trique: Efficacit√© = (700-450)/450 √ó (22-15.2)/22 = +90% d'efficacit√©
```

---

**5. INDICE DE STABILIT√â DES SEGMENTS (ISS)**

**D√©finition:**
Mesure si les segments restent coh√©rents dans le temps (crucial pour strat√©gie long-terme)

```python
def indice_stabilite_segment(labels_t1, labels_t2):
    """
    Mesure la stabilit√©: clients restent-ils dans leur segment?
    """
    # Matrice de transition
    transition_matrix = pd.crosstab(labels_t1, labels_t2, normalize='index')
    
    # Stabilit√© = moyenne des probabilit√©s diagonales
    stabilite = np.mean(np.diag(transition_matrix))
    
    return stabilite

# Exemple
stabilite_3_mois = 0.87  # 87% des clients restent dans leur segment
stabilite_6_mois = 0.82
stabilite_12_mois = 0.76

# M√©trique: Si stabilit√© < 0.7, les segments ne sont pas fiables
```

**Interpr√©tation:**
- ISS > 0.80: Excellente stabilit√©, segments bien d√©finis
- ISS 0.60-0.80: Stabilit√© acceptable, ajustements mineurs
- ISS < 0.60: Segments peu fiables, revoir la segmentation

---

**6. SCORE DE DIFF√âRENCIATION ACTIONNABLE**

**D√©finition:**
Les segments doivent √™tre suffisamment diff√©rents pour justifier des actions distinctes

```python
def score_differenciation_business(segments_data):
    """
    Mesure si les segments justifient des strat√©gies diff√©rentes
    """
    scores = []
    
    # 1. Diff√©rence de comportement d'achat
    purchase_variance = segments_data.groupby('segment')['purchase_frequency'].var()
    scores.append(normalize(purchase_variance.mean()))
    
    # 2. Diff√©rence de pr√©f√©rences produits
    product_affinity_diff = calculate_product_affinity_distance(segments_data)
    scores.append(normalize(product_affinity_diff))
    
    # 3. Diff√©rence de sensibilit√© prix
    price_sensitivity_diff = calculate_price_elasticity_diff(segments_data)
    scores.append(normalize(price_sensitivity_diff))
    
    # 4. Diff√©rence de canaux pr√©f√©r√©s
    channel_preference_diff = calculate_channel_divergence(segments_data)
    scores.append(normalize(channel_preference_diff))
    
    # Score final (0-100)
    return np.mean(scores) * 100

# Interpr√©tation:
# Score > 70: Segments tr√®s diff√©renci√©s, strat√©gies distinctes justifi√©es
# Score 40-70: Diff√©renciation mod√©r√©e, personnalisation partielle
# Score < 40: Segments trop similaires, clustering peu utile
```

---

**7. M√âTRIQUE COMPOSITE: BUSINESS VALUE SCORE (BVS)**

**Formule int√©gr√©e:**
```
BVS = w1√ó(Lift_Conversion) + w2√ó(Am√©lioration_CLV) + w3√ó(R√©duction_CPA) 
      + w4√ó(Stabilit√©) + w5√ó(Diff√©renciation)

Avec: Œ£wi = 1 (pond√©rations selon priorit√©s business)
```

**Exemple de calcul:**
```python
# Pond√©rations pour un e-commerce
weights = {
    'conversion_lift': 0.30,      # Priorit√© maximale
    'clv_improvement': 0.25,
    'cpa_reduction': 0.20,
    'stability': 0.15,
    'differentiation': 0.10
}

metrics = {
    'conversion_lift': 0.45,      # +45%
    'clv_improvement': 0.32,      # +32%
    'cpa_reduction': 0.28,        # -28% (normalis√© positivement)
    'stability': 0.82,            # ISS = 0.82
    'differentiation': 0.73       # Score = 73/100
}

BVS = sum(weights[k] * metrics[k] for k in weights.keys())
# BVS = 0.456 ‚Üí Score de 45.6/100

# Interpr√©tation:
# BVS > 0.60: Clustering tr√®s efficace
# BVS 0.40-0.60: Clustering efficace
# BVS < 0.40: Revoir la segmentation
```

---

**DASHBOARD DE SUIVI RECOMMAND√â:**

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                 √âVALUATION BUSINESS DU CLUSTERING - Q1 2025      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Business Value Score (BVS):                        47.2/100  [‚úì] ‚ïë
‚ïë                                                                  ‚ïë
‚ïë M√©triques D√©taill√©es:                                            ‚ïë
‚ïë ‚îú‚îÄ Conversion Lift:                                +38%     [‚úì]  ‚ïë
‚ïë ‚îú‚îÄ CLV Am√©lioration:                               +28%     [‚úì]  ‚ïë
‚ïë ‚îú‚îÄ CPA R√©duction:                                  -22%     [‚úì]  ‚ïë
‚ïë ‚îú‚îÄ Indice Stabilit√© (6 mois):                     0.79      [‚úì]  ‚ïë
‚ïë ‚îî‚îÄ Score Diff√©renciation:                          68/100   [‚úì]  ‚ïë
‚ïë                                                                  ‚ïë
‚ïë ROI Global Clustering:                             324%          ‚ïë
‚ïë Co√ªt impl√©mentation:                               45K‚Ç¨          ‚ïë
‚ïë Gain annuel estim√©:                                146K‚Ç¨         ‚ïë
‚ïë                                                                  ‚ïë
‚ïë Recommandation:                     ‚úÖ Poursuivre et optimiser   ‚ïë     
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

---

**CONCLUSION:**

La meilleure approche combine:
1. **M√©trique primaire**: Conversion Lift ou CLV (selon objectif business)
2. **M√©trique secondaire**: Efficacit√© op√©rationnelle (CPA, ROI marketing)
3. **M√©trique de contr√¥le**: Stabilit√© et diff√©renciation

**R√®gle d'or**: Si le clustering n'am√©liore pas au moins une m√©trique business de 15-20% sur 3-6 mois, il faut revoir la segmentation ou son utilisation op√©rationnelle.
:::

## 8. R√©sum√© et Bonnes Pratiques

::: {.callout-important icon=false}
## Checklist des √©tapes d'un projet de clustering

‚úÖ **1. Compr√©hension du probl√®me m√©tier**
   - Quel est l'objectif business ?
   - Comment les clusters seront-ils utilis√©s ?

‚úÖ **2. Exploration et pr√©traitement**
   - Analyse des distributions
   - Traitement des valeurs manquantes
   - Normalisation/standardisation

‚úÖ **3. D√©termination du nombre de clusters**
   - M√©thode du coude
   - Score silhouette
   - Analyse de stabilit√©

‚úÖ **4. Application des algorithmes**
   - Test de plusieurs m√©thodes
   - Ajustement des hyperparam√®tres
   - Validation des r√©sultats

‚úÖ **5. √âvaluation et interpr√©tation**
   - M√©triques internes (silhouette, etc.)
   - Visualisation (PCA, t-SNE)
   - Profilage des clusters
   - Interpr√©tation m√©tier

‚úÖ **6. D√©ploiement et monitoring**
   - Documentation des segments
   - Mise √† jour p√©riodique
   - Suivi de la stabilit√© des clusters
:::

## 9. Ressources Compl√©mentaires

1. [Scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html)
2. [Interactive Clustering Visualization](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)
3. [PCA vs t-SNE Explained](https://towardsdatascience.com/pca-vs-t-sne-257d2b9cc7cb)
4. [Customer Segmentation Case Study](https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3)

---

**Fichiers √† rendre**:
1. Notebook Jupyter complet avec code et commentaires
2. Rapport d'analyse (1-2 pages) incluant :
   - M√©thodologie choisie
   - R√©sultats obtenus
   - Visualisations cl√©s
   - Recommandations m√©tier