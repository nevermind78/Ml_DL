{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Séance 4: TD1 - Modèles de Classification de Base\n",
        "\n",
        "::: {.callout-note icon=false}\n",
        "## Informations de la séance\n",
        "- **Type**: Travaux Dirigés\n",
        "- **Durée**: 2h\n",
        "- **Objectifs**: Obj4, Obj6\n",
        ":::\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Ce TD vous permet d'approfondir votre compréhension théorique et pratique des principaux algorithmes de classification. Vous allez travailler sur des exercices conceptuels et des problèmes appliqués.\n",
        "\n",
        "## Partie 1: Arbres de Décision\n",
        "\n",
        "### Exercice 1.1: Construction d'un Arbre\n",
        "\n",
        "Considérez le dataset suivant pour prédire si un client va acheter un ordinateur:\n",
        "\n",
        "| Age | Revenu | Étudiant | Crédit | Achète |\n",
        "|-----|--------|----------|--------|--------|\n",
        "| Jeune | Élevé | Non | Excellent | Non |\n",
        "| Jeune | Élevé | Non | Excellent | Non |\n",
        "| Moyen | Élevé | Non | Excellent | Oui |\n",
        "| Senior | Moyen | Non | Excellent | Oui |\n",
        "| Senior | Faible | Oui | Excellent | Oui |\n",
        "| Senior | Faible | Oui | Bon | Non |\n",
        "| Moyen | Faible | Oui | Bon | Oui |\n",
        "| Jeune | Moyen | Non | Excellent | Non |\n",
        "| Jeune | Faible | Oui | Excellent | Oui |\n",
        "| Senior | Moyen | Oui | Excellent | Oui |\n",
        "| Jeune | Moyen | Oui | Bon | Oui |\n",
        "| Moyen | Moyen | Non | Bon | Oui |\n",
        "| Moyen | Élevé | Oui | Excellent | Oui |\n",
        "| Senior | Moyen | Non | Bon | Non |\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Calculez l'entropie initiale du dataset\n",
        "2. Calculez le gain d'information pour chaque attribut (Age, Revenu, Étudiant, Crédit)\n",
        "3. Quel attribut sera choisi comme racine de l'arbre ?\n",
        "4. Dessinez l'arbre de décision complet\n",
        "\n",
        "::: {.callout-tip collapse=\"true\"}\n",
        "## Rappels - Formules\n",
        "\n",
        "**Entropie:**\n",
        "$$H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
        "\n",
        "**Gain d'Information:**\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
        "\n",
        "où:\n",
        "- $S$ = ensemble de données\n",
        "- $A$ = attribut\n",
        "- $c$ = nombre de classes\n",
        "- $p_i$ = proportion de la classe $i$\n",
        "- $S_v$ = sous-ensemble où $A = v$\n",
        ":::\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 1.1"
      ],
      "id": "a18658b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Données\n",
        "data = [\n",
        "    ('Jeune', 'Élevé', 'Non', 'Excellent', 'Non'),\n",
        "    ('Jeune', 'Élevé', 'Non', 'Excellent', 'Non'),\n",
        "    ('Moyen', 'Élevé', 'Non', 'Excellent', 'Oui'),\n",
        "    ('Senior', 'Moyen', 'Non', 'Excellent', 'Oui'),\n",
        "    ('Senior', 'Faible', 'Oui', 'Excellent', 'Oui'),\n",
        "    ('Senior', 'Faible', 'Oui', 'Bon', 'Non'),\n",
        "    ('Moyen', 'Faible', 'Oui', 'Bon', 'Oui'),\n",
        "    ('Jeune', 'Moyen', 'Non', 'Excellent', 'Non'),\n",
        "    ('Jeune', 'Faible', 'Oui', 'Excellent', 'Oui'),\n",
        "    ('Senior', 'Moyen', 'Oui', 'Excellent', 'Oui'),\n",
        "    ('Jeune', 'Moyen', 'Oui', 'Bon', 'Oui'),\n",
        "    ('Moyen', 'Moyen', 'Non', 'Bon', 'Oui'),\n",
        "    ('Moyen', 'Élevé', 'Oui', 'Excellent', 'Oui'),\n",
        "    ('Senior', 'Moyen', 'Non', 'Bon', 'Non')\n",
        "]\n",
        "\n",
        "def entropy(labels):\n",
        "    \"\"\"Calcule l'entropie\"\"\"\n",
        "    counter = Counter(labels)\n",
        "    total = len(labels)\n",
        "    ent = 0\n",
        "    for count in counter.values():\n",
        "        p = count / total\n",
        "        if p > 0:\n",
        "            ent -= p * np.log2(p)\n",
        "    return ent\n",
        "\n",
        "def information_gain(data, attr_idx):\n",
        "    \"\"\"Calcule le gain d'information\"\"\"\n",
        "    # Entropie initiale\n",
        "    labels = [row[-1] for row in data]\n",
        "    h_s = entropy(labels)\n",
        "    \n",
        "    # Partition par attribut\n",
        "    partitions = {}\n",
        "    for row in data:\n",
        "        attr_value = row[attr_idx]\n",
        "        if attr_value not in partitions:\n",
        "            partitions[attr_value] = []\n",
        "        partitions[attr_value].append(row[-1])\n",
        "    \n",
        "    # Entropie pondérée\n",
        "    h_s_a = 0\n",
        "    total = len(data)\n",
        "    for partition_labels in partitions.values():\n",
        "        p = len(partition_labels) / total\n",
        "        h_s_a += p * entropy(partition_labels)\n",
        "    \n",
        "    return h_s - h_s_a\n",
        "\n",
        "# 1. Entropie initiale\n",
        "labels = [row[-1] for row in data]\n",
        "print(f\"1. Entropie initiale: {entropy(labels):.4f}\")\n",
        "\n",
        "# 2. Gain d'information pour chaque attribut\n",
        "attributes = ['Age', 'Revenu', 'Étudiant', 'Crédit']\n",
        "print(\"\\n2. Gain d'information:\")\n",
        "gains = {}\n",
        "for idx, attr in enumerate(attributes):\n",
        "    ig = information_gain(data, idx)\n",
        "    gains[attr] = ig\n",
        "    print(f\"   {attr}: {ig:.4f}\")\n",
        "\n",
        "# 3. Meilleur attribut\n",
        "best_attr = max(gains, key=gains.get)\n",
        "print(f\"\\n3. Attribut racine: {best_attr} (IG = {gains[best_attr]:.4f})\")\n",
        "\n",
        "# 4. L'arbre complet nécessiterait une implémentation récursive\n",
        "print(\"\\n4. Arbre de décision (structure simplifiée):\")\n",
        "print(\"\"\"\n",
        "         Age?\n",
        "        /    |    \\\\\n",
        "    Jeune  Moyen  Senior\n",
        "      |      |      |\n",
        "    [Classes selon données]\n",
        "\"\"\")"
      ],
      "id": "165eaff0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Réponses:**\n",
        "\n",
        "1. Entropie initiale $\\approx$ 0.940\n",
        "2. Gains d'information:\n",
        "   - Age: ~0.246\n",
        "   - Revenu: ~0.029\n",
        "   - Étudiant: ~0.151\n",
        "   - Crédit: ~0.048\n",
        "3. **Age** sera choisi comme racine (gain le plus élevé)\n",
        ":::\n",
        "\n",
        "### Exercice 1.2: Overfitting dans les Arbres\n",
        "\n",
        "**Question:** Expliquez pourquoi un arbre de décision sans contraintes (profondeur illimitée) tend à faire de l'overfitting.\n",
        "\n",
        "**Proposez 3 méthodes pour limiter l'overfitting dans les arbres de décision.**\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 1.2\n",
        "\n",
        "**Pourquoi l'overfitting ?**\n",
        "\n",
        "Un arbre sans contraintes va créer des branches jusqu'à ce que chaque feuille soit \"pure\" (contient une seule classe). Cela signifie:\n",
        "- L'arbre mémorise les données d'entraînement, y compris le bruit\n",
        "- Il crée des règles très spécifiques qui ne généralisent pas\n",
        "- La complexité du modèle est trop élevée par rapport aux données\n",
        "\n",
        "**3 méthodes pour limiter l'overfitting:**\n",
        "\n",
        "1. **Pré-élagage (Pre-pruning)**:\n",
        "   - `max_depth`: Limiter la profondeur maximale\n",
        "   - `min_samples_split`: Nombre minimum d'échantillons pour diviser un nœud\n",
        "   - `min_samples_leaf`: Nombre minimum d'échantillons dans une feuille\n",
        "   - `max_leaf_nodes`: Nombre maximum de feuilles\n",
        "\n",
        "2. **Post-élagage (Post-pruning)**:\n",
        "   - Construire l'arbre complet\n",
        "   - Élaguer les branches qui n'apportent pas assez d'amélioration\n",
        "   - Utiliser un ensemble de validation pour guider l'élagage\n",
        "\n",
        "3. **Ensemble Methods**:\n",
        "   - Random Forest: moyenne de plusieurs arbres\n",
        "   - Gradient Boosting: construction itérative d'arbres\n",
        "   - Bagging: bootstrap + agrégation"
      ],
      "id": "90f5e34a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Exemple pratique\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Arbre avec overfitting\n",
        "tree_overfit = DecisionTreeClassifier()  # Pas de contraintes\n",
        "\n",
        "# Arbre régularisé\n",
        "tree_regularized = DecisionTreeClassifier(\n",
        "    max_depth=5,              # Profondeur max\n",
        "    min_samples_split=10,     # Min échantillons pour split\n",
        "    min_samples_leaf=5,       # Min échantillons par feuille\n",
        "    max_leaf_nodes=20         # Max feuilles\n",
        ")"
      ],
      "id": "640c3f9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Partie 2: Régression Logistique\n",
        "\n",
        "### Exercice 2.1: Intuition Probabiliste\n",
        "\n",
        "Considérez le modèle de régression logistique suivant pour prédire l'admission à l'université:\n",
        "\n",
        "$$P(admission=1|score) = \\frac{1}{1 + e^{-(0.05 \\times score - 3)}}$$\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Quelle est la probabilité d'admission pour un score de 60?\n",
        "2. Quelle est la probabilité d'admission pour un score de 80?\n",
        "3. Quel score donne une probabilité d'admission de 50%?\n",
        "4. Interprétez le coefficient 0.05\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 2.1"
      ],
      "id": "18882efe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Fonction sigmoïde\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def prob_admission(score):\n",
        "    \"\"\"Probabilité d'admission\"\"\"\n",
        "    z = 0.05 * score - 3\n",
        "    return sigmoid(z)\n",
        "\n",
        "# 1. Probabilité pour score = 60\n",
        "p_60 = prob_admission(60)\n",
        "print(f\"1. P(admission|score=60) = {p_60:.4f} = {p_60*100:.2f}%\")\n",
        "\n",
        "# 2. Probabilité pour score = 80\n",
        "p_80 = prob_admission(80)\n",
        "print(f\"2. P(admission|score=80) = {p_80:.4f} = {p_80*100:.2f}%\")\n",
        "\n",
        "# 3. Score pour P = 0.5\n",
        "# 0.5 = 1/(1 + e^(-(0.05*score - 3)))\n",
        "# => 0.05*score - 3 = 0\n",
        "# => score = 60\n",
        "score_50 = 3 / 0.05\n",
        "print(f\"3. Score pour P=50%: {score_50}\")\n",
        "\n",
        "# 4. Interprétation du coefficient\n",
        "print(f\"\\n4. Coefficient 0.05:\")\n",
        "print(f\"   - Augmenter le score de 1 point augmente z de 0.05\")\n",
        "print(f\"   - Cela augmente les log-odds de 0.05\")\n",
        "print(f\"   - Odds ratio = e^0.05 = {np.exp(0.05):.4f}\")\n",
        "\n",
        "# Visualisation\n",
        "scores = np.linspace(0, 100, 1000)\n",
        "probs = [prob_admission(s) for s in scores]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(scores, probs, linewidth=2)\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='P = 0.5')\n",
        "plt.axvline(x=60, color='r', linestyle='--', alpha=0.7, label='Score = 60')\n",
        "plt.scatter([60, 80], [p_60, p_80], color='red', s=100, zorder=5)\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Probabilité d\\'admission')\n",
        "plt.title('Régression Logistique - Admission à l\\'université')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "8e32073a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Réponses:**\n",
        "\n",
        "1. P(admission|score=60) = 0.50 = 50%\n",
        "2. P(admission|score=80) = 0.73 = 73%\n",
        "3. Score pour P=50%: **60**\n",
        "4. Le coefficient 0.05 indique qu'augmenter le score de 1 point multiplie les odds d'admission par e^0.05 $\\approx$ 1.051 (5.1% d'augmentation)\n",
        ":::\n",
        "\n",
        "### Exercice 2.2: Régression Logistique Multiclasse\n",
        "\n",
        "Expliquez comment adapter la régression logistique pour un problème multiclasse (ex: 3 classes). Quelles sont les deux approches principales?\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 2.2\n",
        "\n",
        "**Deux approches pour la classification multiclasse:**\n",
        "\n",
        "### 1. One-vs-Rest (OvR) ou One-vs-All (OvA)\n",
        "\n",
        "**Principe:**\n",
        "- Entraîner K modèles binaires (K = nombre de classes)\n",
        "- Chaque modèle sépare une classe vs toutes les autres\n",
        "- Prédiction: choisir la classe avec la probabilité la plus élevée\n",
        "\n",
        "**Exemple avec 3 classes:**\n",
        "\n",
        "- Modèle 1: Classe A vs (B, C)\n",
        "- Modèle 2: Classe B vs (A, C)\n",
        "- Modèle 3: Classe C vs (A, B)"
      ],
      "id": "1990faa8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# One-vs-Rest (par défaut dans scikit-learn)\n",
        "ovr_model = LogisticRegression(multi_class='ovr')\n",
        "ovr_model.fit(X_train, y_train)"
      ],
      "id": "d5cda587",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Softmax (ou Multinomial)\n",
        "\n",
        "**Principe:**\n",
        "- Un seul modèle qui produit K probabilités (une par classe)\n",
        "- Utilise la fonction softmax au lieu de sigmoid\n",
        "- Les probabilités somment à 1\n",
        "\n",
        "**Fonction Softmax:**\n",
        "$$P(y=k|X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
        "\n",
        "où $z_k = w_k^T X + b_k$"
      ],
      "id": "8fbab5be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Softmax / Multinomial\n",
        "softmax_model = LogisticRegression(multi_class='multinomial')\n",
        "softmax_model.fit(X_train, y_train)"
      ],
      "id": "bfa325f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Comparaison:**\n",
        "\n",
        "| Critère | One-vs-Rest | Softmax |\n",
        "|---------|-------------|---------|\n",
        "| Nombre de modèles | K modèles | 1 modèle |\n",
        "| Probabilités | Peuvent dépasser 1 (total) | Somment à 1 |\n",
        "| Entraînement | Plus rapide | Plus lent |\n",
        "| Performance | Généralement similaire | Légèrement meilleur |\n",
        "| Calibration | Moins bonne | Meilleure |\n",
        "\n",
        ":::\n",
        "\n",
        "## Partie 3: k-Nearest Neighbors\n",
        "\n",
        "### Exercice 3.1: Distance et Voisinage\n",
        "\n",
        "Considérez les points suivants dans un espace 2D:\n",
        "\n",
        "| Point | x1 | x2 | Classe |\n",
        "|-------|----|----|--------|\n",
        "| A | 2 | 3 | Rouge |\n",
        "| B | 3 | 4 | Rouge |\n",
        "| C | 5 | 6 | Bleu |\n",
        "| D | 5 | 4 | Bleu |\n",
        "| E | 7 | 8 | Bleu |\n",
        "\n",
        "Nouveau point: **P (4, 5)**\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Calculez la distance euclidienne entre P et chaque point\n",
        "2. Avec k=3, quelle classe sera prédite pour P?\n",
        "3. Que se passerait-il avec k=5?\n",
        "4. Pourquoi est-il important de normaliser les données avant d'utiliser k-NN?\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 3.1"
      ],
      "id": "7d3d43d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Points\n",
        "points = {\n",
        "    'A': ([2, 3], 'Rouge'),\n",
        "    'B': ([3, 4], 'Rouge'),\n",
        "    'C': ([5, 6], 'Bleu'),\n",
        "    'D': ([5, 4], 'Bleu'),\n",
        "    'E': ([7, 8], 'Bleu')\n",
        "}\n",
        "\n",
        "# Nouveau point\n",
        "P = np.array([4, 5])\n",
        "\n",
        "# 1. Distances euclidiennes\n",
        "print(\"1. Distances euclidiennes:\")\n",
        "distances = {}\n",
        "for name, (coords, classe) in points.items():\n",
        "    dist = np.sqrt(np.sum((np.array(coords) - P)**2))\n",
        "    distances[name] = (dist, classe)\n",
        "    print(f\"   P → {name}: {dist:.4f} (classe: {classe})\")\n",
        "\n",
        "# 2. k=3\n",
        "print(\"\\n2. k=3:\")\n",
        "sorted_distances = sorted(distances.items(), key=lambda x: x[1][0])\n",
        "k3_neighbors = sorted_distances[:3]\n",
        "k3_classes = [classe for _, (_, classe) in k3_neighbors]\n",
        "k3_prediction = Counter(k3_classes).most_common(1)[0][0]\n",
        "print(f\"   3 voisins les plus proches: {[n[0] for n in k3_neighbors]}\")\n",
        "print(f\"   Classes: {k3_classes}\")\n",
        "print(f\"   Prédiction: {k3_prediction}\")\n",
        "\n",
        "# 3. k=5\n",
        "print(\"\\n3. k=5:\")\n",
        "k5_neighbors = sorted_distances[:5]\n",
        "k5_classes = [classe for _, (_, classe) in k5_neighbors]\n",
        "k5_prediction = Counter(k5_classes).most_common(1)[0][0]\n",
        "print(f\"   5 voisins les plus proches: {[n[0] for n in k5_neighbors]}\")\n",
        "print(f\"   Classes: {k5_classes}\")\n",
        "print(f\"   Prédiction: {k5_prediction}\")\n",
        "\n",
        "# 4. Importance de la normalisation\n",
        "print(\"\\n4. Importance de la normalisation:\")\n",
        "print(\"   Sans normalisation, une feature avec une grande échelle\")\n",
        "print(\"   dominera le calcul de distance.\")\n",
        "print(\"\\n   Exemple:\")\n",
        "print(\"   - Feature 1 (âge): 20-80 → échelle ~60\")\n",
        "print(\"   - Feature 2 (revenu): 20000-100000 → échelle ~80000\")\n",
        "print(\"   → La distance sera dominée par le revenu!\")"
      ],
      "id": "bcd8925d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Réponses:**\n",
        "\n",
        "1. Distances:\n",
        "   - P → A: 2.828\n",
        "   - P → B: 1.414 (plus proche)\n",
        "   - P → C: 1.414 (plus proche)\n",
        "   - P → D: 1.414 (plus proche)\n",
        "   - P → E: 4.243\n",
        "\n",
        "2. Avec k=3: Les 3 voisins sont B (Rouge), C (Bleu), D (Bleu)\n",
        "   - Vote: 1 Rouge, 2 Bleus\n",
        "   - **Prédiction: Bleu**\n",
        "\n",
        "3. Avec k=5: Tous les points\n",
        "   - Vote: 2 Rouges, 3 Bleus\n",
        "   - **Prédiction: Bleu** (même résultat)\n",
        "\n",
        "4. **Normalisation importante** car:\n",
        "   - Les features avec de grandes valeurs dominent le calcul de distance\n",
        "   - Exemple: Si une feature est en milliers et l'autre en dizaines, la première écrasera la seconde\n",
        "   - Solution: StandardScaler ou MinMaxScaler\n",
        ":::\n",
        "\n",
        "## Partie 4: Naive Bayes\n",
        "\n",
        "### Exercice 4.1: Application du Théorème de Bayes\n",
        "\n",
        "Dataset pour classification de courriels:\n",
        "\n",
        "| Courriel | \"gratuit\" | \"argent\" | \"viagra\" | Classe |\n",
        "|----------|-----------|----------|----------|--------|\n",
        "| 1 | Oui | Non | Non | Spam |\n",
        "| 2 | Oui | Oui | Oui | Spam |\n",
        "| 3 | Non | Non | Non | Ham |\n",
        "| 4 | Non | Non | Non | Ham |\n",
        "| 5 | Oui | Oui | Non | Spam |\n",
        "\n",
        "Nouveau courriel contient: **\"gratuit\"** et **\"argent\"**\n",
        "\n",
        "**Calculez P(Spam | gratuit, argent) et P(Ham | gratuit, argent)**\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 4.1"
      ],
      "id": "a850f0a7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "# Données\n",
        "emails = [\n",
        "    {'gratuit': 1, 'argent': 0, 'viagra': 0, 'classe': 'Spam'},\n",
        "    {'gratuit': 1, 'argent': 1, 'viagra': 1, 'classe': 'Spam'},\n",
        "    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n",
        "    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n",
        "    {'gratuit': 1, 'argent': 1, 'viagra': 0, 'classe': 'Spam'},\n",
        "]\n",
        "\n",
        "# Probabilités a priori\n",
        "n_spam = sum(1 for e in emails if e['classe'] == 'Spam')\n",
        "n_ham = sum(1 for e in emails if e['classe'] == 'Ham')\n",
        "total = len(emails)\n",
        "\n",
        "p_spam = n_spam / total\n",
        "p_ham = n_ham / total\n",
        "\n",
        "print(\"Probabilités a priori:\")\n",
        "print(f\"P(Spam) = {n_spam}/{total} = {p_spam}\")\n",
        "print(f\"P(Ham) = {n_ham}/{total} = {p_ham}\")\n",
        "\n",
        "# Probabilités conditionnelles pour Spam\n",
        "spam_emails = [e for e in emails if e['classe'] == 'Spam']\n",
        "p_gratuit_spam = sum(e['gratuit'] for e in spam_emails) / n_spam\n",
        "p_argent_spam = sum(e['argent'] for e in spam_emails) / n_spam\n",
        "\n",
        "print(f\"\\nP(gratuit|Spam) = {p_gratuit_spam}\")\n",
        "print(f\"P(argent|Spam) = {p_argent_spam}\")\n",
        "\n",
        "# Probabilités conditionnelles pour Ham\n",
        "ham_emails = [e for e in emails if e['classe'] == 'Ham']\n",
        "p_gratuit_ham = sum(e['gratuit'] for e in ham_emails) / n_ham\n",
        "p_argent_ham = sum(e['argent'] for e in ham_emails) / n_ham\n",
        "\n",
        "print(f\"\\nP(gratuit|Ham) = {p_gratuit_ham}\")\n",
        "print(f\"P(argent|Ham) = {p_argent_ham}\")\n",
        "\n",
        "# Calcul Naive Bayes (hypothèse d'indépendance)\n",
        "# P(Spam | gratuit, argent) $\\propto$ P(gratuit|Spam) * P(argent|Spam) * P(Spam)\n",
        "numerator_spam = p_gratuit_spam * p_argent_spam * p_spam\n",
        "numerator_ham = p_gratuit_ham * p_argent_ham * p_ham\n",
        "\n",
        "# Normalisation\n",
        "p_spam_given_words = numerator_spam / (numerator_spam + numerator_ham)\n",
        "p_ham_given_words = numerator_ham / (numerator_spam + numerator_ham)\n",
        "\n",
        "print(f\"\\nRésultats:\")\n",
        "print(f\"P(Spam | gratuit, argent) = {p_spam_given_words:.4f}\")\n",
        "print(f\"P(Ham | gratuit, argent) = {p_ham_given_words:.4f}\")\n",
        "print(f\"\\nPrédiction: {'Spam' if p_spam_given_words > p_ham_given_words else 'Ham'}\")"
      ],
      "id": "f2612025",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Résolution manuelle:**\n",
        "\n",
        "**Étape 1: Probabilités a priori**\n",
        "\n",
        "- P(Spam) = 3/5 = 0.6\n",
        "- P(Ham) = 2/5 = 0.4\n",
        "\n",
        "**Étape 2: Probabilités conditionnelles**\n",
        "\n",
        "Pour Spam:\n",
        "- P(gratuit|Spam) = 3/3 = 1.0\n",
        "- P(argent|Spam) = 2/3 $\\approx$ 0.67\n",
        "\n",
        "Pour Ham:\n",
        "- P(gratuit|Ham) = 0/2 = 0\n",
        "- P(argent|Ham) = 0/2 = 0\n",
        "\n",
        "**Étape 3: Application de Bayes**\n",
        "\n",
        "P(Spam | gratuit, argent) $\\propto$ 1.0 × 0.67 × 0.6 = 0.4\n",
        "P(Ham | gratuit, argent) $\\propto$ 0 × 0 × 0.4 = 0\n",
        "\n",
        "**Prédiction: Spam** (avec 100% de confiance)\n",
        ":::\n",
        "\n",
        "## Partie 5: Gradient Boosting (XGBoost/LightGBM)\n",
        "\n",
        "### Exercice 5.1: Comprendre le Boosting\n",
        "\n",
        "**Questions conceptuelles:**\n",
        "\n",
        "1. Quelle est la différence fondamentale entre Random Forest (Bagging) et Gradient Boosting?\n",
        "2. Pourquoi le Gradient Boosting est-il plus sensible à l'overfitting que Random Forest?\n",
        "3. Quels sont les 3 hyperparamètres les plus importants à ajuster pour XGBoost/LightGBM?\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 5.1\n",
        "\n",
        "**1. Différence Bagging vs Boosting:**\n",
        "\n",
        "| Aspect | Random Forest (Bagging) | Gradient Boosting |\n",
        "|--------|------------------------|-------------------|\n",
        "| **Construction** | Parallèle (arbres indépendants) | Séquentielle (arbres dépendants) |\n",
        "| **Objectif** | Réduire la variance | Réduire le biais |\n",
        "| **Données** | Bootstrap (échantillonnage) | Totalité des données |\n",
        "| **Poids** | Tous arbres égaux | Arbres pondérés |\n",
        "| **Prédiction** | Moyenne simple | Somme pondérée |\n",
        "| **Focus** | Erreurs aléatoires | Erreurs résiduelles |\n",
        "\n",
        "\n",
        "**Diagramme mermaid (conversion échouée):**\n",
        "```\n",
        "graph LR\n",
        "    A[Random Forest] --> B[Arbre 1]\n",
        "    A --> C[Arbre 2]\n",
        "    A --> D[Arbre N]\n",
        "    B --> E[V...\n",
        "```\n",
        "\n",
        "\n",
        "**2. Sensibilité à l'overfitting:**\n",
        "\n",
        "Gradient Boosting est plus sensible car:\n",
        "- Chaque arbre se concentre sur les erreurs précédentes\n",
        "- Risque d'apprendre le bruit si trop d'itérations\n",
        "- Peut \"mémoriser\" les cas difficiles du train set\n",
        "- Pas de randomisation par défaut (contrairement à RF)\n",
        "\n",
        "**Solutions:**\n",
        "- Limiter le nombre d'arbres (`n_estimators`)\n",
        "- Réduire le taux d'apprentissage (`learning_rate`)\n",
        "- Limiter la profondeur (`max_depth`)\n",
        "- Early stopping avec validation set\n",
        "\n",
        "**3. Hyperparamètres clés:**"
      ],
      "id": "0e5b6053"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(\n",
        "    # 1. Nombre d'arbres\n",
        "    n_estimators=100,  # Plus = meilleur mais risque overfitting\n",
        "    \n",
        "    # 2. Taux d'apprentissage\n",
        "    learning_rate=0.1,  # Plus faible = besoin de plus d'arbres\n",
        "                        # Typage: 0.01-0.3\n",
        "    \n",
        "    # 3. Profondeur maximale\n",
        "    max_depth=6,  # Plus profond = plus complexe\n",
        "                  # Typique: 3-10\n",
        "    \n",
        "    # Bonus importants:\n",
        "    subsample=0.8,      # Échantillonnage des données (0.5-1.0)\n",
        "    colsample_bytree=0.8,  # Échantillonnage des features\n",
        "    min_child_weight=1,    # Régularisation\n",
        "    \n",
        "    random_state=42\n",
        ")"
      ],
      "id": "293f55eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Recommandations de tuning:**\n",
        "\n",
        "1. **Commencer avec:**\n",
        "\n",
        "   - `learning_rate=0.1`\n",
        "   - `max_depth=6`\n",
        "   - `n_estimators=100`\n",
        "\n",
        "2. **Puis optimiser:**\n",
        "\n",
        "   - Augmenter `n_estimators` + réduire `learning_rate`\n",
        "   - Ajuster `max_depth` (3-10)\n",
        "   - Ajouter régularisation (`subsample`, `colsample_bytree`)\n",
        "\n",
        "3. **Utiliser early stopping:**"
      ],
      "id": "0ef022ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    early_stopping_rounds=10,  # Stop si pas d'amélioration\n",
        "    verbose=False\n",
        ")"
      ],
      "id": "e7520150",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Exercices Récapitulatifs\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Exercice Final: Choix d'Algorithme\n",
        "\n",
        "Pour chacun des scénarios suivants, recommandez un algorithme et justifiez:\n",
        "\n",
        "1. **Diagnostic médical** (interprétabilité cruciale, 1000 patients, 20 features)\n",
        "2. **Détection de fraude** (millions de transactions, temps réel, déséquilibre 99/1)\n",
        "3. **Classification d'images** (50000 images, haute dimension, GPU disponible)\n",
        "4. **Prédiction de churn** (10000 clients, features mixtes, besoin de probabilités calibrées)\n",
        "5. **Classification de textes** (emails spam, 100000 emails, features = mots)\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "\n",
        "<function_calls>\n",
        "<invoke name=\"artifacts\">\n",
        "<parameter name=\"command\">update</parameter>\n",
        "<parameter name=\"id\">seance4</parameter>\n",
        "<parameter name=\"old_str\">5. **Classification de textes** (emails spam</parameter>\n",
        "<parameter name=\"new_str\">5. **Classification de textes** (emails spam, 100000 emails, features = mots)\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice Final\n",
        "\n",
        "**1. Diagnostic médical:**\n",
        "\n",
        "- **Recommandation**: Decision Tree ou Régression Logistique\n",
        "- **Justification**:\n",
        "\n",
        "  - Interprétabilité essentielle pour les médecins\n",
        "  - Dataset de taille modérée\n",
        "  - Besoin de comprendre les règles de décision\n",
        "  - Alternative: Random Forest + feature importance\n",
        "\n",
        "**2. Détection de fraude:**\n",
        "\n",
        "- **Recommandation**: XGBoost/LightGBM\n",
        "\n",
        "- **Justification**:\n",
        "\n",
        "  - Excellent avec classes déséquilibrées (paramètre `scale_pos_weight`)\n",
        "  - Très rapide en prédiction (important pour temps réel)\n",
        "  - Gère bien les grandes données\n",
        "  - Robuste et performant\n",
        "  - Peut utiliser early stopping\n",
        "\n",
        "**3. Classification d'images:**\n",
        "\n",
        "- **Recommandation**: CNN (Deep Learning) - hors scope pour l'instant\n",
        "- **Justification actuelle avec ML classique**:\n",
        "\n",
        "  - Random Forest avec features extraites (HOG, SIFT)\n",
        "  - SVM avec kernel RBF\n",
        "  - Mais performances limitées vs Deep Learning\n",
        "\n",
        "**4. Prédiction de churn:**\n",
        "\n",
        "- **Recommandation**: Régression Logistique ou Gradient Boosting\n",
        "- **Justification**:\n",
        "\n",
        "  - Logistic Regression: probabilités bien calibrées, interprétable\n",
        "  - Gradient Boosting: meilleures performances, feature importance\n",
        "  - Dataset de taille moyenne\n",
        "  - Features mixtes gérées par les deux\n",
        "\n",
        "**5. Classification de textes:**\n",
        "\n",
        "- **Recommandation**: Naive Bayes (Multinomial)\n",
        "- **Justification**:\n",
        "\n",
        "  - Très performant pour la classification de texte\n",
        "  - Rapide à entraîner et prédire\n",
        "  - Gère bien les grandes dimensions (nombreux mots)\n",
        "  - Probabilités natives\n",
        "  - Alternative: Régression Logistique\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Résumé du TD\n",
        "\n",
        "::: {.callout-important icon=false}\n",
        "## Points clés à retenir\n",
        "\n",
        "### Algorithmes et leurs forces\n",
        "\n",
        "1. **Decision Tree**\n",
        "   - $\\checkmark$ Interprétable, visuel\n",
        "   - X Overfitting, instable\n",
        "\n",
        "2. **Random Forest**\n",
        "   - $\\checkmark$ Robuste, performant\n",
        "   - X Moins interprétable, mémoire\n",
        "\n",
        "3. **SVM**\n",
        "   - $\\checkmark$ Excellent en haute dimension\n",
        "   - X Lent, difficile à interpréter\n",
        "\n",
        "4. **Naive Bayes**\n",
        "   - $\\checkmark$ Rapide, bon pour texte\n",
        "   - X Hypothèse d'indépendance forte\n",
        "\n",
        "5. **Régression Logistique**\n",
        "   - $\\checkmark$ Probabilités calibrées, interprétable\n",
        "   - X Assume linéarité\n",
        "\n",
        "6. **k-NN**\n",
        "   - $\\checkmark$ Simple, pas de training\n",
        "   - X Lent en prédiction, besoin normalisation\n",
        "\n",
        "7. **Gradient Boosting**\n",
        "   - $\\checkmark$ Très performant, gère déséquilibre\n",
        "   - X Sensible overfitting, plus complexe\n",
        ":::\n",
        "\n",
        "## Pour le Prochain Cours\n",
        "\n",
        "Préparez-vous pour le **TD2 sur les Critères d'Évaluation** où nous approfondirons:\n",
        "- Matrice de confusion\n",
        "- Precision, Recall, F1-score\n",
        "- Courbe ROC et AUC\n",
        "- Choix de métriques selon le contexte\n",
        "\n",
        "## Ressources Complémentaires\n",
        "\n",
        "1. [Scikit-learn: Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
        "2. [StatQuest: Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n",
        "3. [XGBoost Documentation](https://xgboost.readthedocs.io/)</parameter>\n"
      ],
      "id": "29281aab"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "test_env",
      "language": "python",
      "display_name": "Python (test_env)",
      "path": "C:\\Users\\abdal\\AppData\\Roaming\\jupyter\\kernels\\test_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}