# Séance 2: Apprentissage Supervisé - Classification

::: {.callout-note icon=false}
## Informations de la séance
- **Type**: Cours
- **Durée**: 2h
- **Objectifs**: Obj5, Obj6
:::

## 1. Introduction à la Classification

### 1.1 Définition

La **classification** est une tâche d'apprentissage supervisé où l'objectif est de prédire une **classe** ou **catégorie** discrète à partir de caractéristiques d'entrée.

::: {.callout-note}
## Exemple
**Entrée**: Caractéristiques d'un email (mots, expéditeur, longueur, etc.)  
**Sortie**: Classe = "Spam" ou "Non Spam"
:::

### 1.2 Différence Classification vs Régression

| Caractéristique | Classification | Régression |
|----------------|----------------|------------|
| **Sortie** | Catégorie discrète | Valeur continue |
| **Exemple** | Spam/Non spam | Prix d'une maison |
| **Métrique** | Accuracy, F1-score | MAE, RMSE |
| **Fonction** | Probabilité → Classe | Valeur numérique |

## 2. Types de Classification

### 2.1 Classification Binaire

Deux classes possibles: 0 ou 1, Vrai ou Faux, Positif ou Négatif

**Exemples**:

- Détection de spam (spam/non spam)
- Diagnostic médical (malade/sain)
- Détection de fraude (fraude/légitime)
- Approbation de crédit (approuvé/rejeté)

```{python}
#| echo: true
#| eval: false

# Exemple: Classification binaire
y_binary = [0, 1, 1, 0, 1, 0, 0, 1]  # 0 = négatif, 1 = positif
```

### 2.2 Classification Multi-classes

Plus de deux classes **mutuellement exclusives** (une seule classe par instance)

**Exemples**:

- Reconnaissance de chiffres manuscrits (0-9 = 10 classes)
- Classification de fleurs Iris (Setosa, Versicolor, Virginica)
- Catégorisation d'articles (Sport, Politique, Économie, Culture)

```{python}
#| echo: true
#| eval: false

# Exemple: Classification multi-classes
y_multiclass = [0, 1, 2, 1, 0, 2, 1]  # 3 classes: 0, 1, 2
```

### 2.3 Classification Multi-label

Plusieurs classes **simultanées** possibles pour une instance

**Exemples**:

- Étiquetage de photos (peut contenir: personne, chien, extérieur)
- Catégorisation de films (peut être: Action, Comédie, Drame)
- Analyse de sentiments multiple (joie + surprise)

```{python}
#| echo: true
#| eval: false

# Exemple: Classification multi-label
y_multilabel = [
    [1, 0, 1],  # instance a les labels 0 et 2
    [0, 1, 1],  # instance a les labels 1 et 2
    [1, 1, 0]   # instance a les labels 0 et 1
]
```

## 3. Algorithmes de Classification

### 3.1 Arbre de Décision (Decision Tree)

Modèle qui prend des décisions basées sur des questions successives.

#### Principe

L'arbre divise l'espace des caractéristiques en régions par des questions binaires.


**Diagramme :**
```{mermaid}
graph TD
    A[Age > 30?] -->|Oui| B[Revenu > 50k?]
    A -->|Non| C[Étudiant?]
    B -->|Oui| D[Approuvé ]
    B -->|Non| E[Rejeté ]
    C -->|Oui| F[Rejeté ]
    C -->|Non| G[Approuvé ]
```


#### Avantages
- Facile à interpréter et visualiser
- Pas besoin de normalisation des données
- Gère les données non linéaires
- Gère les variables catégorielles et numériques

#### Inconvénients
- Tendance à l'overfitting
- Instable (petits changements de données → arbre différent)
- Biais vers les classes majoritaires

```{python}
#| echo: true
#| eval: false

from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Chargement des données
iris = load_iris()
X, y = iris.data, iris.target

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Entraînement
clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# Prédiction
y_pred = clf.predict(X_test)
print(f"Accuracy: {clf.score(X_test, y_test):.2f}")
```

### 3.2 Random Forest
Ensemble d'arbres de décision qui votent ensemble.

#### Principe
1. Créer N arbres sur des sous-ensembles aléatoires de données
2. Chaque arbre vote pour une classe
3. Prédiction finale = vote majoritaire

#### Avantages
- Très performant et robuste
- Réduit l'overfitting par rapport à un arbre unique
- Gère bien les grandes dimensions
- Donne l'importance des features

#### Inconvénients
- Moins interprétable qu'un arbre unique
- Plus lent à entraîner et prédire
- Mémoire importante

```{python}
#| echo: true
#| eval: false

from sklearn.ensemble import RandomForestClassifier

# Entraînement
rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)
rf.fit(X_train, y_train)

# Prédiction
y_pred = rf.predict(X_test)
print(f"Accuracy: {rf.score(X_test, y_test):.2f}")

# Importance des features
importances = rf.feature_importances_
for i, imp in enumerate(importances):
    print(f"Feature {iris.feature_names[i]}: {imp:.3f}")
```

### 3.3 Support Vector Machine (SVM)

Trouve l'hyperplan optimal qui sépare les classes avec la marge maximale.

#### Principe

- **Marge**: distance entre l'hyperplan et les points les plus proches (vecteurs de support)
- **Objectif**: Maximiser cette marge
- **Kernel trick**: Permet de gérer des données non linéairement séparables

```{python}
#| echo: true
#| eval: false

from sklearn.svm import SVC

# SVM linéaire
svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train, y_train)

# SVM avec kernel RBF (pour données non linéaires)
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')
svm_rbf.fit(X_train, y_train)

print(f"SVM Linear Accuracy: {svm_linear.score(X_test, y_test):.2f}")
print(f"SVM RBF Accuracy: {svm_rbf.score(X_test, y_test):.2f}")
```

#### Avantages
- Très efficace en haute dimension
- Robuste aux outliers
- Versatile (différents kernels)

#### Inconvénients
- Lent sur de grandes données
- Difficile à interpréter
- Sensible au choix des hyperparamètres

### 3.4 Naïve Bayes

Basé sur le théorème de Bayes avec hypothèse d'indépendance des features.

#### Principe - Théorème de Bayes

$$P(y|X) = \frac{P(X|y) \cdot P(y)}{P(X)}$$

Où:

- $P(y|X)$ = probabilité de la classe $y$ sachant les features $X$ (posterior)
- $P(X|y)$ = vraisemblance
- $P(y)$ = probabilité a priori de la classe
- $P(X)$ = évidence (constante)

#### Hypothèse "Naïve"

Les features sont **indépendantes** conditionnellement à la classe:

$$P(X|y) = P(x_1|y) \cdot P(x_2|y) \cdot ... \cdot P(x_n|y)$$

```{python}
#| echo: true
#| eval: false

from sklearn.naive_bayes import GaussianNB, MultinomialNB

# Gaussian Naive Bayes (pour features continues)
gnb = GaussianNB()
gnb.fit(X_train, y_train)

print(f"Gaussian NB Accuracy: {gnb.score(X_test, y_test):.2f}")

# Multinomial NB (pour comptages, ex: mots dans un texte)
# mnb = MultinomialNB()
# mnb.fit(X_train_counts, y_train)
```

#### Avantages
- Très rapide (entraînement et prédiction)
- Fonctionne bien avec peu de données
- Excellent pour la classification de texte

#### Inconvénients
- Hypothèse d'indépendance rarement vraie
- Performance limitée si hypothèse violée

### 3.5 Régression Logistique

**Attention**: Malgré son nom, c'est un algorithme de **classification** !

#### Principe

Modèle linéaire qui utilise la fonction sigmoïde pour produire des probabilités.

**Fonction sigmoïde**:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Modèle**:
$$P(y=1|X) = \sigma(w^T X + b) = \frac{1}{1 + e^{-(w^T X + b)}}$$

#### Interprétation Probabiliste

- Sortie $\in [0, 1]$ : probabilité d'appartenance à la classe positive
- Si $P(y=1|X) \geq 0.5$ → prédiction = classe 1
- Si $P(y=1|X) < 0.5$ → prédiction = classe 0

```{python}
#| echo: true
#| eval: false

from sklearn.linear_model import LogisticRegression

# Entraînement
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Prédiction de classes
y_pred = log_reg.predict(X_test)

# Prédiction de probabilités
y_proba = log_reg.predict_proba(X_test)

print(f"Accuracy: {log_reg.score(X_test, y_test):.2f}")
print(f"\nPremière prédiction:")
print(f"  Probabilités: {y_proba[0]}")
print(f"  Classe prédite: {y_pred[0]}")
```

#### Avantages
- Simple et interprétable
- Donne des probabilités (utile pour la prise de décision)
- Peu de paramètres à ajuster
- Fonctionne bien sur données linéairement séparables

#### Inconvénients
- Assume une relation linéaire
- Sensible aux outliers
- Nécessite feature engineering pour les relations non linéaires

### 3.6 k-Nearest Neighbors (k-NN)

Classification basée sur la proximité avec les voisins.

#### Principe
1. Calculer la distance entre le nouveau point et tous les points d'entraînement
2. Sélectionner les k points les plus proches
3. Vote majoritaire parmi ces k voisins

```{python}
#| echo: true
#| eval: false

from sklearn.neighbors import KNeighborsClassifier

# k=5 voisins
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

print(f"k-NN Accuracy: {knn.score(X_test, y_test):.2f}")
```

#### Avantages
- Simple et intuitif
- Pas d'entraînement (lazy learning)
- Fonctionne bien pour des frontières complexes

#### Inconvénients
- Lent pour la prédiction (calcule toutes les distances)
- Sensible à l'échelle des features (nécessite normalisation)
- Curse of dimensionality (mauvais en haute dimension)

## 4. Critères d'Évaluation (Aperçu)

### 4.1 Métriques Principales

- **Accuracy**: Proportion de prédictions correctes
- **Precision**: Proportion de vrais positifs parmi les prédictions positives
- **Recall**: Proportion de vrais positifs parmi les cas réellement positifs
- **F1-Score**: Moyenne harmonique de Precision et Recall

::: {.callout-important}
Ces métriques seront détaillées en profondeur dans la **Séance 5 - TD2**
:::

### 4.2 Exemple Simple

```{python}
#| echo: true
#| eval: false

from sklearn.metrics import accuracy_score, classification_report

# Calcul de l'accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Rapport complet
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

## 5. Exemple Complet: Comparaison d'Algorithmes

```{python}
#| echo: true
#| eval: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Création d'un dataset synthétique
X, y = make_classification(
    n_samples=1000, 
    n_features=2, 
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=42
)

# Split et normalisation
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Entraînement de plusieurs modèles
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

models = {
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Naive Bayes': GaussianNB(),
    'k-NN': KNeighborsClassifier(n_neighbors=5)
}

# Comparaison des performances
results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    score = model.score(X_test_scaled, y_test)
    results[name] = score
    print(f"{name:20s}: {score:.4f}")

# Visualisation
plt.figure(figsize=(10, 6))
plt.barh(list(results.keys()), list(results.values()))
plt.xlabel('Accuracy')
plt.title('Comparaison des Algorithmes de Classification')
plt.xlim([0, 1])
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
```

## Résumé de la Séance

::: {.callout-important icon=false}
## Points clés à retenir

1. **Classification** = prédire une catégorie discrète
2. **Types**: Binaire, Multi-classes, Multi-label
3. **Algorithmes principaux**:
   - Decision Tree: interprétable mais tendance à l'overfitting
   - Random Forest: robuste et performant
   - SVM: excellent en haute dimension
   - Naïve Bayes: rapide, bon pour le texte
   - Régression Logistique: simple, interprétable, probabiliste
   - k-NN: simple mais coûteux en prédiction
4. **Choix du modèle** dépend de: taille des données, interprétabilité, performance, ressources
5. **Évaluation** avec métriques appropriées (détails en TD2)
:::

## Exercices

::: {.callout-warning icon=false}
## Exercice 1
Implémentez une régression logistique sur le dataset Iris et analysez les coefficients appris. Que représentent-ils?
:::

::: {.callout-warning icon=false}
## Exercice 2
Comparez les performances de Decision Tree vs Random Forest sur le dataset digits de sklearn. Expliquez les différences observées.
:::

::: {.callout-warning icon=false}
## Exercice 3
Pour un problème de détection de fraude bancaire, quel algorithme recommanderiez-vous et pourquoi? Considérez les aspects: interprétabilité, temps réel, déséquilibre des classes.
:::

## Lectures Complémentaires

1. Géron, A. (2019) - Chapitre 3: Classification
2. Scikit-learn Documentation: [Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html)
3. [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)