{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Séance 5: TD2 - Critères d'Évaluation\n",
        "\n",
        "::: {.callout-note icon=false}\n",
        "## Informations de la séance\n",
        "- **Type**: Travaux Dirigés\n",
        "- **Durée**: 2h\n",
        "- **Objectifs**: Obj4\n",
        ":::\n",
        "\n",
        "## Introduction\n",
        "\n",
        "L'évaluation correcte d'un modèle de classification est cruciale. Choisir la mauvaise métrique peut conduire à des conclusions erronées et à des modèles inadaptés en production. Dans ce TD, nous allons explorer en profondeur les différentes métriques d'évaluation.\n",
        "\n",
        "## 1. La Matrice de Confusion\n",
        "\n",
        "### 1.1 Définition et Structure\n",
        "\n",
        "La **matrice de confusion** est un tableau qui visualise les performances d'un modèle de classification en comparant les prédictions aux vraies valeurs.\n",
        "\n",
        "Pour un problème **binaire**:\n",
        "\n",
        "```\n",
        "                    Prédiction\n",
        "                 Négatif  Positif\n",
        "Réalité  Négatif    TN       FP\n",
        "         Positif    FN       TP\n",
        "```\n",
        "\n",
        "Où:\n",
        "\n",
        "- **TP (True Positive)**: Vrais Positifs - correctement classifiés comme positifs\n",
        "- **TN (True Negative)**: Vrais Négatifs - correctement classifiés comme négatifs\n",
        "- **FP (False Positive)**: Faux Positifs - incorrectement classifiés comme positifs (Erreur Type I)\n",
        "- **FN (False Negative)**: Faux Négatifs - incorrectement classifiés comme négatifs (Erreur Type II)\n",
        "\n",
        "::: {.callout-tip}\n",
        "## Mnémotechnique\n",
        "- **Type I (FP)**: Fausse alarme - \"On crie au loup alors qu'il n'y a pas de loup\"\n",
        "- **Type II (FN)**: Manque - \"On ne voit pas le loup alors qu'il est là\"\n",
        ":::\n",
        "\n",
        "### 1.2 Exemple Concret: Détection de Maladie\n",
        "\n",
        "Considérez un test médical pour une maladie:\n",
        "\n",
        "| Patient | Vraie Classe | Prédiction | Résultat |\n",
        "|---------|--------------|------------|----------|\n",
        "| 1 | Malade | Malade | TP $\\checkmark$ |\n",
        "| 2 | Malade | Sain | FN X (Dangereux!) |\n",
        "| 3 | Sain | Malade | FP X (Fausse alarme) |\n",
        "| 4 | Sain | Sain | TN $\\checkmark$ |\n",
        "| 5 | Malade | Malade | TP $\\checkmark$ |\n",
        "| 6 | Sain | Sain | TN $\\checkmark$ |\n",
        "| 7 | Malade | Sain | FN X (Dangereux!) |\n",
        "| 8 | Sain | Malade | FP X (Fausse alarme) |\n",
        "\n",
        "Matrice de confusion:\n",
        "\n",
        "```\n",
        "              Prédiction\n",
        "           Sain  Malade\n",
        "Réalité Sain   2      2      (4 sains)\n",
        "      Malade   2      2      (4 malades)\n",
        "```\n",
        "\n",
        "- **TP = 2**: Malades correctement détectés\n",
        "- **TN = 2**: Sains correctement identifiés\n",
        "- **FP = 2**: Sains diagnostiqués malades (traitement inutile)\n",
        "- **FN = 2**: Malades non détectés (très dangereux!)\n",
        "\n",
        "### Exercice 1.1: Construction de Matrice de Confusion\n",
        "\n",
        "Soit les prédictions suivantes pour un détecteur de spam (1 = Spam, 0 = Ham):\n",
        "\n",
        "```python\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
        "```\n",
        "\n",
        "**Questions:**\n",
        "1. Construisez la matrice de confusion\n",
        "2. Calculez TP, TN, FP, FN\n",
        "3. Interprétez chaque type d'erreur dans ce contexte\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 1.1"
      ],
      "id": "1640ca68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
        "y_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0])\n",
        "\n",
        "# 1. Matrice de confusion\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"1. Matrice de Confusion:\")\n",
        "print(cm)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Ham (0)', 'Spam (1)'],\n",
        "            yticklabels=['Ham (0)', 'Spam (1)'])\n",
        "plt.ylabel('Vraie Classe')\n",
        "plt.xlabel('Prédiction')\n",
        "plt.title('Matrice de Confusion - Détection de Spam')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Calcul manuel\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\n2. Valeurs:\")\n",
        "print(f\"TP (Vrais Positifs) = {tp}\")\n",
        "print(f\"TN (Vrais Négatifs) = {tn}\")\n",
        "print(f\"FP (Faux Positifs) = {fp}\")\n",
        "print(f\"FN (Faux Négatifs) = {fn}\")\n",
        "\n",
        "# Vérification manuelle\n",
        "print(\"\\n3. Interprétation:\")\n",
        "print(f\"TP = {tp}: Spams correctement détectés\")\n",
        "print(f\"TN = {tn}: Hams correctement identifiés\")\n",
        "print(f\"FP = {fp}: Hams classés comme spam (vont en indésirables)\")\n",
        "print(f\"FN = {fn}: Spams non détectés (arrivent en boîte de réception)\")\n",
        "\n",
        "# Analyse détaillée\n",
        "print(\"\\nAnalyse détaillée des prédictions:\")\n",
        "for i, (true, pred) in enumerate(zip(y_true, y_pred)):\n",
        "    status = \"\"\n",
        "    if true == 1 and pred == 1:\n",
        "        status = \"TP $\\checkmark$\"\n",
        "    elif true == 0 and pred == 0:\n",
        "        status = \"TN $\\checkmark$\"\n",
        "    elif true == 0 and pred == 1:\n",
        "        status = \"FP X\"\n",
        "    elif true == 1 and pred == 0:\n",
        "        status = \"FN X\"\n",
        "    \n",
        "    true_label = \"Spam\" if true == 1 else \"Ham\"\n",
        "    pred_label = \"Spam\" if pred == 1 else \"Ham\"\n",
        "    print(f\"Email {i+1}: Vrai={true_label}, Prédit={pred_label} → {status}\")"
      ],
      "id": "8b812774",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Réponses:**\n",
        "\n",
        "1. Matrice: `[[4, 1], [1, 4]]`\n",
        "2. TP=4, TN=4, FP=1, FN=1\n",
        "3. Interprétation:\n",
        "\n",
        "   - **FP (1 email)**: Email légitime envoyé dans spam (utilisateur peut manquer info importante)\n",
        "   - **FN (1 email)**: Spam non détecté dans boîte réception (nuisance mineure)\n",
        ":::\n",
        "\n",
        "## 2. Métriques Dérivées\n",
        "\n",
        "### 2.1 Accuracy (Exactitude)\n",
        "\n",
        "**Définition**: Proportion de prédictions correctes\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$"
      ],
      "id": "4cb63ef7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "# ou manuellement:\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)"
      ],
      "id": "e16669ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-warning}\n",
        "## Piège de l'Accuracy!\n",
        "\n",
        "L'accuracy peut être **trompeuse** avec des classes déséquilibrées!\n",
        "\n",
        "**Exemple**: Détection de fraude\n",
        "\n",
        "- 990 transactions légitimes, 10 frauduleuses\n",
        "- Modèle naïf qui prédit toujours \"légitime\"\n",
        "- Accuracy = 990/1000 = **99%** \n",
        "- Mais 0% de fraudes détectées!\n",
        ":::\n",
        "\n",
        "### 2.2 Precision (Précision)\n",
        "\n",
        "**Définition**: Proportion de prédictions positives qui sont correctes\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "**Question répondue**: \"Parmi tous les cas prédits positifs, combien le sont vraiment?\"\n",
        "\n",
        "**Interprétation**:\n",
        "\n",
        "- Haute précision → Peu de faux positifs\n",
        "- Important quand le coût d'un FP est élevé"
      ],
      "id": "863a50aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "# ou manuellement:\n",
        "precision = tp / (tp + fp)"
      ],
      "id": "b3ea6e89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exemples où la Precision est cruciale**:\n",
        "\n",
        "1. **Recommandation de produits**: Ne pas recommander des produits non pertinents\n",
        "2. **Filtrage spam**: Ne pas mettre d'emails importants dans spam\n",
        "3. **Détection de visages**: Ne pas identifier de faux visages\n",
        "\n",
        "### 2.3 Recall (Rappel) ou Sensitivity (Sensibilité)\n",
        "\n",
        "**Définition**: Proportion de vrais positifs correctement identifiés\n",
        "\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "**Question répondue**: \"Parmi tous les cas réellement positifs, combien ai-je détectés?\"\n",
        "\n",
        "**Interprétation**:\n",
        "- Haut recall → Peu de faux négatifs\n",
        "- Important quand le coût d'un FN est élevé"
      ],
      "id": "22df184c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "recall = recall_score(y_true, y_pred)\n",
        "# ou manuellement:\n",
        "recall = tp / (tp + fn)"
      ],
      "id": "19063d8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exemples où le Recall est crucial**:\n",
        "\n",
        "1. **Détection de cancer**: Ne manquer aucun malade\n",
        "2. **Détection de fraude**: Détecter toutes les fraudes\n",
        "3. **Systèmes de sécurité**: Ne rater aucune menace\n",
        "\n",
        "### 2.4 Compromis Precision-Recall\n",
        "\n",
        "Il existe généralement un **compromis** entre Precision et Recall:\n",
        "\n",
        "\n",
        "**Diagramme mermaid (conversion échouée):**\n",
        "```\n",
        "graph LR\n",
        "    A[Seuil bas<br/>0.3] --> B[Haute Recall<br/>Basse Precision]\n",
        "    C[Seuil moyen<br/>0.5]...\n",
        "```\n",
        "\n",
        "\n",
        "**Exemple concret**:"
      ],
      "id": "2d5b7e55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Modèle de détection de maladie\n",
        "# Proba patient malade: 0.6\n",
        "\n",
        "# Seuil = 0.5: Prédit malade → Haute recall\n",
        "# Seuil = 0.8: Prédit sain → Haute precision (mais manque des cas)"
      ],
      "id": "f4e1ff16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 F1-Score\n",
        "\n",
        "**Définition**: Moyenne harmonique de Precision et Recall\n",
        "\n",
        "$$F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "**Pourquoi moyenne harmonique?**\n",
        "\n",
        "- Pénalise les déséquilibres\n",
        "- Si Precision=100% et Recall=1%, F1 $\\approx$ 2% (pas 50.5%)"
      ],
      "id": "b86b06d4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "# ou manuellement:\n",
        "f1 = 2 * (precision * recall) / (precision + recall)"
      ],
      "id": "ed30f060",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Quand utiliser F1?**\n",
        "- Besoin d'équilibre entre Precision et Recall\n",
        "- Classes déséquilibrées\n",
        "- Vouloir une seule métrique résumée\n",
        "\n",
        "### 2.6 Autres Métriques\n",
        "\n",
        "**Specificity (Spécificité)**:\n",
        "\n",
        "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
        "Proportion de vrais négatifs correctement identifiés\n",
        "\n",
        "**F-Beta Score**:\n",
        "\n",
        "$$F_\\beta = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "- $\\beta > 1$: Favorise le Recall\n",
        "- $\\beta < 1$: Favorise la Precision\n",
        "\n",
        "### Exercice 2.1: Calcul de Métriques\n",
        "\n",
        "Soit la matrice de confusion suivante pour un détecteur de tumeurs malignes:\n",
        "\n",
        "```\n",
        "              Prédiction\n",
        "           Bénigne  Maligne\n",
        "Réalité Bénigne    850      50\n",
        "       Maligne     20       80\n",
        "```\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Calculez Accuracy, Precision, Recall, F1-Score\n",
        "2. Quelle métrique est la plus importante dans ce contexte? Pourquoi?\n",
        "3. Le modèle est-il satisfaisant? Justifiez.\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 2.1"
      ],
      "id": "f40a4c0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Matrice de confusion\n",
        "#          Bénigne  Maligne\n",
        "# Bénigne    850      50\n",
        "# Maligne     20      80\n",
        "\n",
        "tn = 850  # Bénigne correctement identifié\n",
        "fp = 50   # Bénigne prédit Maligne\n",
        "fn = 20   # Maligne prédit Bénigne\n",
        "tp = 80   # Maligne correctement identifié\n",
        "\n",
        "total = tn + fp + fn + tp\n",
        "\n",
        "# 1. Calcul des métriques\n",
        "accuracy = (tp + tn) / total\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(\"1. Métriques calculées:\")\n",
        "print(f\"Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Precision:   {precision:.4f} ({precision*100:.2f}%)\")\n",
        "print(f\"Recall:      {recall:.4f} ({recall*100:.2f}%)\")\n",
        "print(f\"F1-Score:    {f1:.4f} ({f1*100:.2f}%)\")\n",
        "print(f\"Specificity: {specificity:.4f} ({specificity*100:.2f}%)\")\n",
        "\n",
        "# 2. Métrique la plus importante\n",
        "print(\"\\n2. Métrique la plus importante: RECALL\")\n",
        "print(\"   Raison: En médical, ne pas détecter un cancer (FN) est\")\n",
        "print(\"   beaucoup plus grave qu'une fausse alarme (FP).\")\n",
        "print(f\"   Actuellement, Recall = {recall:.2%} signifie que\")\n",
        "print(f\"   {fn} cancers sur {tp+fn} ne sont pas détectés!\")\n",
        "\n",
        "# 3. Évaluation\n",
        "print(\"\\n3. Évaluation du modèle:\")\n",
        "print(f\"   X Recall de {recall:.2%} est INSUFFISANT\")\n",
        "print(f\"   X 20% de cancers manqués = inacceptable\")\n",
        "print(f\"   $\\checkmark$ Precision de {precision:.2%} est correcte\")\n",
        "print(f\"   $\\checkmark$ Specificity de {specificity:.2%} est bonne\")\n",
        "print(\"\\n   CONCLUSION: Modèle à améliorer!\")\n",
        "print(\"   Recommandation: Abaisser le seuil de décision pour\")\n",
        "print(\"   augmenter le Recall, même au prix de la Precision.\")\n",
        "\n",
        "# Impact en nombre de patients\n",
        "print(f\"\\nImpact sur 1000 patients:\")\n",
        "print(f\"   - 20 cancers NON DÉTECTÉS (FN) ← CRITIQUE\")\n",
        "print(f\"   - 50 fausses alarmes (FP) ← Acceptable (examens complémentaires)\")"
      ],
      "id": "3ef05a35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Réponses:**\n",
        "\n",
        "1. **Métriques:**\n",
        "   - Accuracy: 93.00%\n",
        "   - Precision: 61.54%\n",
        "   - Recall: 80.00%\n",
        "   - F1-Score: 69.57%\n",
        "   - Specificity: 94.44%\n",
        "\n",
        "2. **Métrique importante: RECALL**\n",
        "   - Un cancer non détecté (FN) peut être mortel\n",
        "   - Une fausse alarme (FP) → examens supplémentaires (acceptable)\n",
        "   - Donc: mieux vaut trop détecter que pas assez\n",
        "\n",
        "3. **Évaluation:**\n",
        "   - X Recall de 80% **insuffisant** (20% de cancers manqués)\n",
        "   - $\\checkmark$ Specificity correcte\n",
        "   - **Conclusion**: Modèle dangereux en l'état\n",
        "   - **Action**: Réduire le seuil pour augmenter Recall\n",
        ":::\n",
        "\n",
        "## 3. Courbe ROC et AUC\n",
        "\n",
        "### 3.1 Courbe ROC (Receiver Operating Characteristic)\n",
        "\n",
        "La **courbe ROC** visualise les performances d'un classificateur binaire en variant le seuil de décision.\n",
        "\n",
        "**Axes:**\n",
        "\n",
        "- **X**: False Positive Rate (FPR) = $\\frac{FP}{FP + TN}$ = 1 - Specificity\n",
        "- **Y**: True Positive Rate (TPR) = $\\frac{TP}{TP + FN}$ = Recall"
      ],
      "id": "f13a07a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Obtenir les probabilités (pas les classes)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculer les points de la courbe ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "\n",
        "# Tracer la courbe\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, linewidth=2, label='ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.5)')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Recall)')\n",
        "plt.title('Courbe ROC')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "58d18987",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interprétation des points:**\n",
        "\n",
        "- **Point (0, 0)**: Tout prédit négatif (seuil = 1.0)\n",
        "- **Point (1, 1)**: Tout prédit positif (seuil = 0.0)\n",
        "- **Point (0, 1)**: Classificateur parfait\n",
        "- **Diagonale**: Classificateur aléatoire\n",
        "\n",
        "### 3.2 AUC (Area Under Curve)\n",
        "\n",
        "L'**AUC** est l'aire sous la courbe ROC.\n",
        "\n",
        "**Interprétation:**\n",
        "\n",
        "- **AUC = 1.0**: Classificateur parfait\n",
        "- **AUC = 0.9 - 1.0**: Excellent\n",
        "- **AUC = 0.8 - 0.9**: Très bon\n",
        "- **AUC = 0.7 - 0.8**: Bon\n",
        "- **AUC = 0.6 - 0.7**: Médiocre\n",
        "- **AUC = 0.5**: Aléatoire (inutile)\n",
        "- **AUC < 0.5**: Pire qu'aléatoire (inverser les prédictions!)"
      ],
      "id": "2f16cf0d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Calcul de l'AUC\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"AUC Score: {auc:.4f}\")"
      ],
      "id": "a7e2f6c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Avantages de l'AUC:**\n",
        "\n",
        "- Indépendant du seuil de décision\n",
        "- Robuste aux classes déséquilibrées\n",
        "- Facile à interpréter (une seule valeur)\n",
        "\n",
        "**Signification probabiliste:**\n",
        "\n",
        "AUC = probabilité qu'un exemple positif aléatoire ait un score plus élevé qu'un exemple négatif aléatoire\n",
        "\n",
        "### Exercice 3.1: Analyse de Courbe ROC\n",
        "\n",
        "Vous avez trois modèles avec les AUC suivants:\n",
        "\n",
        "- Modèle A: AUC = 0.95\n",
        "- Modèle B: AUC = 0.75\n",
        "- Modèle C: AUC = 0.52\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Classez les modèles par performance\n",
        "2. Quel modèle choisiriez-vous pour détecter des fraudes bancaires?\n",
        "3. Dans quel cas le Modèle C pourrait-il être utile?\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 3.1"
      ],
      "id": "773e7b0c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Simulation de courbes ROC\n",
        "np.random.seed(42)\n",
        "\n",
        "# Modèle A (excellent)\n",
        "fpr_a = np.linspace(0, 1, 100)\n",
        "tpr_a = np.power(fpr_a, 0.2)  # Courbe très au-dessus de la diagonale\n",
        "\n",
        "# Modèle B (bon)\n",
        "fpr_b = np.linspace(0, 1, 100)\n",
        "tpr_b = np.power(fpr_b, 0.6)\n",
        "\n",
        "# Modèle C (quasi-aléatoire)\n",
        "fpr_c = np.linspace(0, 1, 100)\n",
        "tpr_c = fpr_c + np.random.normal(0, 0.05, 100)\n",
        "tpr_c = np.clip(tpr_c, 0, 1)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_a, tpr_a, linewidth=2, label=f'Modèle A (AUC=0.95)')\n",
        "plt.plot(fpr_b, tpr_b, linewidth=2, label=f'Modèle B (AUC=0.75)')\n",
        "plt.plot(fpr_c, tpr_c, linewidth=2, label=f'Modèle C (AUC=0.52)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Aléatoire (AUC=0.5)')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Comparaison des Courbes ROC')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"1. Classement par performance:\")\n",
        "print(\"   1er: Modèle A (AUC=0.95) - Excellent\")\n",
        "print(\"   2e:  Modèle B (AUC=0.75) - Bon\")\n",
        "print(\"   3e:  Modèle C (AUC=0.52) - Quasi-aléatoire\")\n",
        "\n",
        "print(\"\\n2. Pour détecter des fraudes:\")\n",
        "print(\"   → Modèle A (AUC=0.95)\")\n",
        "print(\"   Raison: Meilleure capacité à distinguer fraude/légitime\")\n",
        "print(\"   Permet d'ajuster le seuil selon coût FP vs FN\")\n",
        "\n",
        "print(\"\\n3. Utilité du Modèle C:\")\n",
        "print(\"   → Quasiment aucune!\")\n",
        "print(\"   AUC=0.52 $\\approx$ tirage à pile ou face\")\n",
        "print(\"   SAUF: Si on inverse ses prédictions → AUC=0.48\")\n",
        "print(\"   → Peut indiquer un bug dans le code/labels\")"
      ],
      "id": "177f06f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Réponses:**\n",
        "\n",
        "1. **Classement**: A > B > C\n",
        "\n",
        "   - A est excellent\n",
        "   - B est bon mais moins performant\n",
        "   - C est pratiquement inutile\n",
        "\n",
        "2. **Fraude bancaire: Modèle A**\n",
        "\n",
        "   - AUC élevé = meilleure discrimination\n",
        "   - Important car coûts élevés (fraudes manquées + fausses alarmes)\n",
        "   - Permet de choisir le seuil optimal selon les coûts métier\n",
        "\n",
        "3. **Utilité de C:**\n",
        "\n",
        "   - Pratiquement aucune (performance aléatoire)\n",
        "   - Pourrait indiquer un problème (bug, labels inversés, features non pertinentes)\n",
        "   - Si AUC < 0.5: inverser les prédictions pourrait aider!\n",
        ":::\n",
        "\n",
        "## 4. Choix de Métriques selon le Contexte\n",
        "\n",
        "### 4.1 Tableau de Décision\n",
        "\n",
        "| Contexte | Métrique Principale | Raison |\n",
        "|----------|-------------------|---------|\n",
        "| **Détection de spam** | Precision | Éviter de perdre emails importants (FP coûteux) |\n",
        "| **Détection de cancer** | Recall | Ne manquer aucun malade (FN critique) |\n",
        "| **Moteur de recherche** | Precision@K | Premiers résultats doivent être pertinents |\n",
        "| **Fraude bancaire** | F1 ou AUC | Équilibre entre détecter fraudes et éviter fausses alarmes |\n",
        "| **Système de recommandation** | Precision | Recommandations doivent être pertinentes |\n",
        "| **Détection d'intrusion réseau** | Recall | Ne rater aucune attaque (FN dangereux) |\n",
        "| **Diagnostic automatisé** | Recall + Precision | Les deux sont importants (vies humaines) |\n",
        "\n",
        "### 4.2 Classes Déséquilibrées\n",
        "\n",
        "Lorsque les classes sont **très déséquilibrées** (ex: 1% positifs, 99% négatifs):\n",
        "\n",
        "**❌ À ÉVITER:**\n",
        "\n",
        "- Accuracy (trompeuse)\n",
        "\n",
        "**✅ À UTILISER:**\n",
        "\n",
        "- Precision, Recall, F1-Score\n",
        "- AUC-ROC\n",
        "- Precision-Recall curve\n",
        "- Balanced Accuracy = $\\frac{\\text{Recall} + \\text{Specificity}}{2}$"
      ],
      "id": "f5d4469c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "# Accuracy normale (biaisée si déséquilibre)\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Balanced accuracy (moyenne de recall et specificity)\n",
        "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")"
      ],
      "id": "1755df35",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 4.1: Choix de Métrique\n",
        "\n",
        "Pour chaque scénario, choisissez la métrique la plus appropriée et justifiez:\n",
        "\n",
        "1. **Système de reconnaissance faciale pour déverrouillage de téléphone**\n",
        "2. **Filtre anti-spam Gmail**\n",
        "3. **Détection de COVID-19 par test rapide**\n",
        "4. **Système de recommandation Netflix**\n",
        "5. **Détection de transactions frauduleuses (0.1% de fraudes)**\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 4.1\n",
        "\n",
        "**1. Reconnaissance faciale pour téléphone:**\n",
        "- **Métrique**: **Precision** (prioritaire) + FPR faible\n",
        "- **Justification**:\n",
        "\n",
        "  - FP = Personne non autorisée accède au téléphone (GRAVE - sécurité)\n",
        "  - FN = Propriétaire doit retenter (MINEUR - inconvénient)\n",
        "  - Mieux vaut bloquer le propriétaire parfois que laisser entrer un intrus\n",
        "\n",
        "**2. Filtre anti-spam Gmail:**\n",
        "\n",
        "- **Métrique**: **Precision** (prioritaire)\n",
        "- **Justification**:\n",
        "\n",
        "  - FP = Email important dans spam (GRAVE - peut manquer info critique)\n",
        "  - FN = Spam en boîte de réception (MINEUR - simple nuisance)\n",
        "  - Gmail préfère laisser passer du spam que bloquer des emails légitimes\n",
        "\n",
        "**3. Détection COVID-19:**\n",
        "\n",
        "- **Métrique**: **Recall** (prioritaire)\n",
        "- **Justification**:\n",
        "  - FN = Malade non détecté → propage le virus (GRAVE - santé publique)\n",
        "  - FP = Personne saine isolée inutilement (MINEUR - test confirmatoire possible)\n",
        "  - Mieux vaut trop détecter que pas assez\n",
        "\n",
        "**4. Recommandation Netflix:**\n",
        "\n",
        "- **Métrique**: **Precision@K** (K = nombre de recommandations affichées)\n",
        "- **Justification**:\n",
        "  - Utilisateur voit seulement les K premières recommandations\n",
        "  - Celles-ci doivent être pertinentes pour satisfaction client\n",
        "  - Recall moins important (on ne peut pas tout recommander)\n",
        "\n",
        "**5. Fraude bancaire (0.1% de fraudes):**\n",
        "\n",
        "- **Métriques**: **F1-Score** + **AUC-ROC** + **Precision-Recall curve**\n",
        "- **Justification**:\n",
        "\n",
        "  - Classes très déséquilibrées → Accuracy inutile\n",
        "  - FP = Client bloqué à tort (coûteux - insatisfaction)\n",
        "  - FN = Fraude non détectée (très coûteux - pertes financières)\n",
        "  - Besoin d'équilibre → F1 ou optimiser selon coûts métier\n",
        "  - AUC pour comparer modèles indépendamment du seuil\n",
        ":::\n",
        "\n",
        "## 5. Exercice Récapitulatif Complet\n",
        "\n",
        "### Scénario: Détecteur de Défauts en Usine\n",
        "\n",
        "Une usine produit 10,000 pièces par jour. En moyenne, 100 pièces (1%) sont défectueuses.\n",
        "\n",
        "Vous avez développé un modèle de détection automatique qui donne les résultats suivants sur 1000 pièces de test:\n",
        "\n",
        "```\n",
        "              Prédiction\n",
        "           OK    Défectueux\n",
        "Réalité OK    970      20\n",
        "     Défect.   3       7\n",
        "```\n",
        "\n",
        "**Coûts:**\n",
        "\n",
        "- Pièce défectueuse non détectée (FN): **500€** (vendue puis retournée par client)\n",
        "- Pièce OK rejetée à tort (FP): **10€** (inspection manuelle inutile)\n",
        "- Inspection manuelle d'une pièce: **5€**\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Calculez toutes les métriques (Accuracy, Precision, Recall, F1, Specificity)\n",
        "2. Le modèle est-il acceptable d'un point de vue métier?\n",
        "3. Calculez le coût total journalier des erreurs\n",
        "4. Quelle métrique devriez-vous optimiser en priorité?\n",
        "5. Proposez une amélioration du modèle\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice Récapitulatif"
      ],
      "id": "89240dae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Matrice de confusion\n",
        "#           OK    Défect\n",
        "# OK       970      20\n",
        "# Défect     3       7\n",
        "\n",
        "tn, fp = 970, 20\n",
        "fn, tp = 3, 7\n",
        "\n",
        "total = tn + fp + fn + tp\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ANALYSE DU DÉTECTEUR DE DÉFAUTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Métriques\n",
        "print(\"\\n1. MÉTRIQUES DE PERFORMANCE:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "accuracy = (tp + tn) / total\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "specificity = tn / (tn + fp)\n",
        "fpr = fp / (fp + tn)\n",
        "fnr = fn / (fn + tp)\n",
        "\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1,\n",
        "    'Specificity': specificity,\n",
        "    'False Positive Rate': fpr,\n",
        "    'False Negative Rate': fnr\n",
        "}\n",
        "\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric:25s}: {value:.4f} ({value*100:6.2f}%)\")\n",
        "\n",
        "# 2. Acceptabilité métier\n",
        "print(\"\\n2. ACCEPTABILITÉ MÉTIER:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"X Recall = {recall:.2%} est TRÈS FAIBLE\")\n",
        "print(f\"  → Sur 10 défauts, seulement {tp} détectés, {fn} manqués!\")\n",
        "print(f\"  → {fnr:.2%} de défauts passent inaperçus\")\n",
        "print(f\"\\n$\\checkmark$ Precision = {precision:.2%} est acceptable\")\n",
        "print(f\"  → Peu de fausses alarmes\")\n",
        "print(f\"\\nX CONCLUSION: Modèle INACCEPTABLE\")\n",
        "print(f\"  → Trop de défauts non détectés atteignent les clients\")\n",
        "\n",
        "# 3. Coût journalier\n",
        "print(\"\\n3. COÛT TOTAL JOURNALIER:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Extrapolation à 10,000 pièces\n",
        "total_pieces = 10000\n",
        "defect_rate = 0.01\n",
        "total_defects = int(total_pieces * defect_rate)  # 100 défauts\n",
        "total_ok = total_pieces - total_defects  # 9900 OK\n",
        "\n",
        "# Calcul des erreurs attendues\n",
        "expected_fn = int(total_defects * fnr)  # Défauts non détectés\n",
        "expected_fp = int(total_ok * fpr)  # OK rejetés à tort\n",
        "\n",
        "cout_fn = expected_fn * 500  # Défauts non détectés\n",
        "cout_fp = expected_fp * 10   # OK rejetés\n",
        "cout_inspection = (expected_fn + expected_fp) * 0  # Pas d'inspection pour erreurs\n",
        "\n",
        "cout_total = cout_fn + cout_fp\n",
        "\n",
        "print(f\"Production journalière:     {total_pieces:,} pièces\")\n",
        "print(f\"Défauts réels:              {total_defects} pièces ({defect_rate:.1%})\")\n",
        "print(f\"\\nErreurs attendues:\")\n",
        "print(f\"  - FN (défauts manqués):   {expected_fn} × 500€ = {cout_fn:,}€\")\n",
        "print(f\"  - FP (OK rejetés):        {expected_fp} × 10€  = {cout_fp:,}€\")\n",
        "print(f\"\\nCoût total des erreurs:     {cout_total:,}€/jour\")\n",
        "print(f\"Coût mensuel (22 jours):    {cout_total*22:,}€/mois\")\n",
        "print(f\"Coût annuel (250 jours):    {cout_total*250:,}€/an\")\n",
        "\n",
        "# 4. Métrique à optimiser\n",
        "print(\"\\n4. MÉTRIQUE À OPTIMISER:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"→ RECALL (priorité absolue)\")\n",
        "print(\"\\nRaison: Le coût d'un FN (500€) est 50× plus élevé\")\n",
        "print(\"        que le coût d'un FP (10€)\")\n",
        "print(f\"\\nRatio coût FN/FP: {500/10:.0f}:1\")\n",
        "print(\"\\nStratégie: Maximiser le Recall, même au prix de\")\n",
        "print(\"           la Precision (plus de fausses alarmes acceptable)\")\n",
        "\n",
        "# 5. Amélioration\n",
        "print(\"\\n5. PROPOSITIONS D'AMÉLIORATION:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\\nA. Court terme (ajustement du seuil):\")\n",
        "print(\"   1. Abaisser le seuil de décision (ex: 0.5 → 0.2)\")\n",
        "print(\"   2. Objectif: Recall > 90%\")\n",
        "print(\"   3. Conséquence: Plus de FP (mais coût acceptable)\")\n",
        "\n",
        "# Simulation avec recall amélioré\n",
        "recall_target = 0.90\n",
        "fn_new = int(total_defects * (1 - recall_target))  # 10 défauts manqués\n",
        "# Supposons que Precision baisse à 20%\n",
        "precision_new = 0.20\n",
        "fp_new = int((total_defects * recall_target) / precision_new * (1 - precision_new))\n",
        "\n",
        "cout_fn_new = fn_new * 500\n",
        "cout_fp_new = fp_new * 10\n",
        "cout_total_new = cout_fn_new + cout_fp_new\n",
        "\n",
        "print(f\"\\n   Simulation avec Recall=90%, Precision=20%:\")\n",
        "print(f\"   - FN: {fn_new} × 500€ = {cout_fn_new:,}€\")\n",
        "print(f\"   - FP: {fp_new} × 10€ = {cout_fp_new:,}€\")\n",
        "print(f\"   - Coût total: {cout_total_new:,}€/jour\")\n",
        "print(f\"   - Économie: {cout_total - cout_total_new:,}€/jour\")\n",
        "print(f\"   - ROI annuel: {(cout_total - cout_total_new)*250:,}€\")\n",
        "\n",
        "print(\"\\nB. Moyen terme (amélioration du modèle):\")\n",
        "print(\"   1. Collecter plus de données (surtout défauts)\")\n",
        "print(\"   2. Feature engineering (nouvelles caractéristiques)\")\n",
        "print(\"   3. Essayer d'autres algorithmes (XGBoost, CNN)\")\n",
        "print(\"   4. Data augmentation pour la classe minoritaire\")\n",
        "\n",
        "print(\"\\nC. Long terme (approche hybride):\")\n",
        "print(\"   1. Modèle ML comme premier filtre (Recall élevé)\")\n",
        "print(\"   2. Inspection manuelle des cas suspects\")\n",
        "print(\"   3. Double vérification pour cas limites\")\n",
        "\n",
        "# Visualisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = np.array([[tn, fp], [fn, tp]])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['OK', 'Défectueux'],\n",
        "            yticklabels=['OK', 'Défectueux'])\n",
        "axes[0].set_ylabel('Vraie Classe')\n",
        "axes[0].set_xlabel('Prédiction')\n",
        "axes[0].set_title('Matrice de Confusion')\n",
        "\n",
        "# Comparaison coûts\n",
        "scenarios = ['Modèle Actuel\\n(Recall=70%)', \n",
        "             'Modèle Amélioré\\n(Recall=90%)']\n",
        "couts = [cout_total, cout_total_new]\n",
        "colors = ['#ff6b6b', '#51cf66']\n",
        "\n",
        "axes[1].bar(scenarios, couts, color=colors, alpha=0.7)\n",
        "axes[1].set_ylabel('Coût journalier (€)')\n",
        "axes[1].set_title('Comparaison des Coûts')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "for i, (scenario, cout) in enumerate(zip(scenarios, couts)):\n",
        "    axes[1].text(i, cout + 100, f'{cout:,}€', \n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tableau récapitulatif\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TABLEAU RÉCAPITULATIF\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Métrique': ['Recall', 'Precision', 'FN/jour', 'FP/jour', 'Coût/jour'],\n",
        "    'Actuel': [f'{recall:.1%}', f'{precision:.1%}', expected_fn, expected_fp, f'{cout_total:,}€'],\n",
        "    'Cible': ['90%', '20%', fn_new, fp_new, f'{cout_total_new:,}€'],\n",
        "    'Amélioration': ['↑ +20pp', '↓ -6pp', f'↓ -{expected_fn-fn_new}', f'↑ +{fp_new-expected_fp}', \n",
        "                     f'↓ -{cout_total-cout_total_new:,}€']\n",
        "})\n",
        "\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"=\" * 60)"
      ],
      "id": "5f7fd06b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Synthèse des réponses:**\n",
        "\n",
        "**1. Métriques:**\n",
        "\n",
        "- Accuracy: 97.70%\n",
        "- Precision: 25.93% (7 sur 27 prédictions de défauts sont correctes)\n",
        "- Recall: 70.00% (7 sur 10 vrais défauts détectés)\n",
        "- F1-Score: 38.89%\n",
        "- Specificity: 97.98%\n",
        "\n",
        "**2. Acceptabilité métier:**\n",
        "\n",
        "- ❌ **NON ACCEPTABLE**\n",
        "- Recall de 70% signifie 30% de défauts non détectés\n",
        "- Ces défauts atteignent les clients → réputation + coûts\n",
        "\n",
        "**3. Coût journalier:**\n",
        "\n",
        "- FN: 30 défauts × 500€ = **15,000€/jour**\n",
        "- FP: 200 pièces × 10€ = **2,000€/jour**\n",
        "- **Total: 17,000€/jour** ($\\approx$ 4.25M€/an!)\n",
        "\n",
        "**4. Métrique à optimiser:**\n",
        "\n",
        "- **RECALL** (priorité absolue)\n",
        "- Ratio coût 50:1 en faveur de l'augmentation du Recall\n",
        "- Accepter plus de FP pour réduire FN\n",
        "\n",
        "**5. Amélioration proposée:**\n",
        "\n",
        "- Abaisser seuil → Recall 90%+\n",
        "- Économie potentielle: ~10,000€/jour\n",
        "- ROI annuel: ~2.5M€\n",
        ":::\n",
        "\n",
        "## 6. Comparaison Critique des Résultats\n",
        "\n",
        "### Exercice 6.1: Analyse Comparative\n",
        "\n",
        "Trois data scientists ont entraîné des modèles pour détecter des défaillances machines:\n",
        "\n",
        "| Modèle | Accuracy | Precision | Recall | F1 | AUC |\n",
        "|--------|----------|-----------|--------|----|----|\n",
        "| A | 98% | 40% | 95% | 56% | 0.92 |\n",
        "| B | 95% | 80% | 70% | 75% | 0.88 |\n",
        "| C | 99% | 20% | 60% | 30% | 0.75 |\n",
        "\n",
        "**Contexte:** 2% de défaillances, coût FN = 10,000€, coût FP = 100€\n",
        "\n",
        "**Questions:**\n",
        "1. Quel modèle recommandez-vous?\n",
        "2. Justifiez votre choix avec calculs de coûts\n",
        "3. Que révèle l'Accuracy élevée du modèle C?\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Solution Exercice 6.1"
      ],
      "id": "17b6a890"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Données\n",
        "models_data = {\n",
        "    'Modèle': ['A', 'B', 'C'],\n",
        "    'Accuracy': [0.98, 0.95, 0.99],\n",
        "    'Precision': [0.40, 0.80, 0.20],\n",
        "    'Recall': [0.95, 0.70, 0.60],\n",
        "    'F1': [0.56, 0.75, 0.30],\n",
        "    'AUC': [0.92, 0.88, 0.75]\n",
        "}\n",
        "\n",
        "df_models = pd.DataFrame(models_data)\n",
        "\n",
        "# Contexte\n",
        "defect_rate = 0.02\n",
        "total_samples = 10000\n",
        "total_defects = int(total_samples * defect_rate)  # 200\n",
        "total_normal = total_samples - total_defects  # 9800\n",
        "\n",
        "cost_fn = 10000  # Coût défaillance non détectée\n",
        "cost_fp = 100    # Coût fausse alarme\n",
        "\n",
        "print(\"ANALYSE COMPARATIVE DES MODÈLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calcul des coûts pour chaque modèle\n",
        "results = []\n",
        "\n",
        "for idx, row in df_models.iterrows():\n",
        "    model = row['Modèle']\n",
        "    recall = row['Recall']\n",
        "    precision = row['Precision']\n",
        "    \n",
        "    # Calcul des erreurs\n",
        "    fn = int(total_defects * (1 - recall))  # Défauts manqués\n",
        "    tp = total_defects - fn  # Défauts détectés\n",
        "    \n",
        "    # De precision = TP / (TP + FP), on déduit FP\n",
        "    if precision > 0:\n",
        "        fp = int(tp / precision - tp)  # Fausses alarmes\n",
        "    else:\n",
        "        fp = 0\n",
        "    \n",
        "    # Coûts\n",
        "    cost_fn_total = fn * cost_fn\n",
        "    cost_fp_total = fp * cost_fp\n",
        "    cost_total = cost_fn_total + cost_fp_total\n",
        "    \n",
        "    results.append({\n",
        "        'Modèle': model,\n",
        "        'FN': fn,\n",
        "        'FP': fp,\n",
        "        'Coût FN': cost_fn_total,\n",
        "        'Coût FP': cost_fp_total,\n",
        "        'Coût Total': cost_total\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nModèle {model}:\")\n",
        "    print(f\"  Recall: {recall:.0%} → FN = {fn} défaillances manquées\")\n",
        "    print(f\"  Precision: {precision:.0%} → FP = {fp} fausses alarmes\")\n",
        "    print(f\"  Coût FN: {fn} × {cost_fn:,}€ = {cost_fn_total:,}€\")\n",
        "    print(f\"  Coût FP: {fp} × {cost_fp:,}€ = {cost_fp_total:,}€\")\n",
        "    print(f\"  COÛT TOTAL: {cost_total:,}€\")\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Meilleur modèle\n",
        "best_model = df_results.loc[df_results['Coût Total'].idxmin()]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"RECOMMANDATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"→ Modèle {best_model['Modèle']} (coût le plus faible)\")\n",
        "print(f\"\\nJustification:\")\n",
        "print(f\"  - Coût total minimal: {best_model['Coût Total']:,}€\")\n",
        "print(f\"  - Recall élevé ({df_models[df_models['Modèle']==best_model['Modèle']]['Recall'].values[0]:.0%}) critique car FN très coûteux\")\n",
        "print(f\"  - Le surcoût en FP est négligeable comparé aux économies en FN\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ANALYSE DU MODÈLE C\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Accuracy de 99% mais performances médiocres:\")\n",
        "print(\"  → Piège des classes déséquilibrées!\")\n",
        "print(f\"  → Avec 2% de défauts, prédire 'normal' partout\")\n",
        "print(f\"     donnerait déjà 98% d'accuracy\")\n",
        "print(\"  → Recall de 60% = 40% de défauts manqués (inacceptable)\")\n",
        "print(\"  → AUC faible (0.75) confirme faible capacité discriminative\")\n",
        "print(\"\\n  Conclusion: Accuracy est une métrique TROMPEUSE ici!\")\n",
        "\n",
        "# Visualisations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Graphique 1: Comparaison des coûts\n",
        "x = df_results['Modèle']\n",
        "width = 0.35\n",
        "\n",
        "x_pos = range(len(x))\n",
        "axes[0].bar([p - width/2 for p in x_pos], df_results['Coût FN'], \n",
        "           width, label='Coût FN', color='#ff6b6b', alpha=0.8)\n",
        "axes[0].bar([p + width/2 for p in x_pos], df_results['Coût FP'], \n",
        "           width, label='Coût FP', color='#ffa94d', alpha=0.8)\n",
        "\n",
        "axes[0].set_xlabel('Modèle')\n",
        "axes[0].set_ylabel('Coût (€)')\n",
        "axes[0].set_title('Comparaison des Coûts par Type d\\'Erreur')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(x)\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Ajouter coût total\n",
        "for i, (idx, row) in enumerate(df_results.iterrows()):\n",
        "    total = row['Coût Total']\n",
        "    axes[0].text(i, total + 5000, f'{total:,}€', \n",
        "                ha='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "# Graphique 2: Radar chart des métriques\n",
        "from math import pi\n",
        "\n",
        "categories = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
        "N = len(categories)\n",
        "\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "ax = plt.subplot(122, projection='polar')\n",
        "\n",
        "for idx, row in df_models.iterrows():\n",
        "    values = [row['Accuracy'], row['Precision'], row['Recall'], \n",
        "              row['F1'], row['AUC']]\n",
        "    values += values[:1]\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=f\"Modèle {row['Modèle']}\")\n",
        "    ax.fill(angles, values, alpha=0.15)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Comparaison Multidimensionnelle des Métriques', y=1.08)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Tableau final\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TABLEAU RÉCAPITULATIF\")\n",
        "print(\"=\" * 70)\n",
        "print(df_results.to_string(index=False))"
      ],
      "id": "92a765de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Réponses:**\n",
        "\n",
        "1. **Modèle recommandé: A**\n",
        "\n",
        "2. **Justification avec coûts:**\n",
        "   - Modèle A: 10 FN × 10,000€ + 475 FP × 100€ = **147,500€**\n",
        "   - Modèle B: 60 FN × 10,000€ + 175 FP × 100€ = **617,500€**\n",
        "   - Modèle C: 80 FN × 10,000€ + 600 FP × 100€ = **860,000€**\n",
        "   \n",
        "   Le Modèle A minimise le coût total malgré plus de FP\n",
        "\n",
        "3. **Accuracy élevée du Modèle C:**\n",
        "   - Piège classique des classes déséquilibrées!\n",
        "   - 99% accuracy car prédit surtout \"normal\"\n",
        "   - Mais rate 40% des défaillances (Recall = 60%)\n",
        "   - **Leçon**: Accuracy seule est trompeuse avec déséquilibre\n",
        ":::\n",
        "\n",
        "## Résumé de la Séance\n",
        "\n",
        "::: {.callout-important icon=false}\n",
        "## Points clés à retenir\n",
        "\n",
        "### 1. Matrice de Confusion\n",
        "- Fondation de toutes les métriques\n",
        "- TP, TN, FP, FN à bien comprendre\n",
        "- Visualisation claire des erreurs\n",
        "\n",
        "### 2. Métriques Principales\n",
        "- **Accuracy**: % correct (attention au déséquilibre!)\n",
        "- **Precision**: Fiabilité des prédictions positives\n",
        "- **Recall**: Couverture des vrais positifs\n",
        "- **F1-Score**: Équilibre Precision-Recall\n",
        "\n",
        "### 3. Compromis\n",
        "- Precision ↑ → Recall ↓ (généralement)\n",
        "- Choix selon le coût des erreurs\n",
        "- Ajustement du seuil de décision\n",
        "\n",
        "### 4. Courbe ROC et AUC\n",
        "- Évaluation indépendante du seuil\n",
        "- AUC = mesure globale de discrimination\n",
        "- Comparaison facile de modèles\n",
        "\n",
        "### 5. Choix Contextuel\n",
        "- **Pas de métrique universelle!**\n",
        "- Dépend du problème métier\n",
        "- Considérer les coûts réels des erreurs\n",
        "- Classes déséquilibrées → éviter Accuracy seule\n",
        "\n",
        "### 6. Règles d'Or\n",
        "1. Toujours regarder plusieurs métriques\n",
        "2. Privilégier Recall si FN coûteux\n",
        "3. Privilégier Precision si FP coûteux\n",
        "4. Utiliser AUC pour comparer modèles\n",
        "5. Valider avec coûts métier réels\n",
        ":::\n",
        "\n",
        "## Checklist de Maîtrise\n",
        "\n",
        "- [ ] Je sais construire et interpréter une matrice de confusion\n",
        "- [ ] Je comprends la différence entre Precision et Recall\n",
        "- [ ] Je peux expliquer pourquoi Accuracy peut être trompeuse\n",
        "- [ ] Je sais calculer manuellement les métriques de base\n",
        "- [ ] Je comprends le compromis Precision-Recall\n",
        "- [ ] Je sais interpréter une courbe ROC et l'AUC\n",
        "- [ ] Je peux choisir la métrique appropriée selon le contexte\n",
        "- [ ] Je comprends l'impact des classes déséquilibrées\n",
        "\n",
        "## Exercices Supplémentaires\n",
        "\n",
        "::: {.callout-warning icon=false}\n",
        "## Pour s'entraîner\n",
        "\n",
        "1. **Implémentez** toutes les métriques vues en Python sans Scikit-learn\n",
        "2. **Créez** une fonction qui recommande la meilleure métrique selon le contexte\n",
        "3. **Analysez** un dataset déséquilibré (ex: fraude, maladie rare) sur Kaggle\n",
        "4. **Comparez** plusieurs modèles sur le même problème avec toutes les métriques\n",
        "5. **Explorez** l'effet du seuil de décision sur Precision et Recall\n",
        ":::\n",
        "\n",
        "## Préparation Séance Suivante\n",
        "\n",
        "La **Séance 6 (TP2)** abordera:\n",
        "\n",
        "- Classification multi-classes\n",
        "- Optimisation d'hyperparamètres (GridSearchCV, RandomizedSearchCV)\n",
        "- Validation croisée\n",
        "- Comparaison avancée de modèles\n",
        "\n",
        "**À préparer:**\n",
        "\n",
        "- Relire les concepts de validation croisée\n",
        "- Installer scikit-learn à jour\n",
        "- Réfléchir aux hyperparamètres importants de chaque algorithme\n",
        "\n",
        "## Ressources Complémentaires\n",
        "\n",
        "1. [Scikit-learn: Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "2. [Google ML Crash Course: Classification](https://developers.google.com/machine-learning/crash-course/classification)\n",
        "3. [StatQuest: Sensitivity and Specificity](https://www.youtube.com/watch?v=vP06aMoz4v8)\n",
        "4. [Towards Data Science: Beyond Accuracy](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c)"
      ],
      "id": "21412c09"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "test_env",
      "language": "python",
      "display_name": "Python (test_env)",
      "path": "C:\\Users\\abdal\\AppData\\Roaming\\jupyter\\kernels\\test_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}