{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Séance 3: TP1 - Pipeline de Classification Binaire\n",
        "\n",
        "::: {.callout-note icon=false}\n",
        "## Informations de la séance\n",
        "- **Type**: Travaux Pratiques\n",
        "- **Durée**: 2h\n",
        "- **Objectifs**: Obj6, Obj7\n",
        "- **Dataset**: Titanic (prédiction de survie)\n",
        ":::\n",
        "\n",
        "## Objectifs du TP\n",
        "\n",
        "À la fin de ce TP, vous serez capable de:\n",
        "\n",
        "1. Charger et explorer un dataset\n",
        "2. Préparer les données pour l'apprentissage\n",
        "3. Créer un pipeline de prétraitement avec Scikit-learn\n",
        "4. Entraîner un modèle de classification binaire\n",
        "5. Évaluer les performances du modèle\n",
        "\n",
        "## 1. Configuration de l'Environnement"
      ],
      "id": "d641f777"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Installation des bibliothèques (si nécessaire)\n",
        "# !pip install scikit-learn pandas numpy matplotlib seaborn\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✓ Bibliothèques importées avec succès\")"
      ],
      "id": "e9f28c53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chargement et Exploration des Données\n",
        "\n",
        "### 2.1 Chargement du Dataset Titanic"
      ],
      "id": "cfbbc696"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Chargement depuis seaborn\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Affichage des premières lignes\n",
        "print(\"Aperçu des données:\")\n",
        "print(titanic.head())\n",
        "\n",
        "print(f\"\\nDimensions: {titanic.shape}\")\n",
        "print(f\"Colonnes: {titanic.columns.tolist()}\")"
      ],
      "id": "656f7548",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Exploration Initiale"
      ],
      "id": "4f08cba1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Informations générales\n",
        "print(\"Informations sur le dataset:\")\n",
        "print(titanic.info())\n",
        "\n",
        "print(\"\\nStatistiques descriptives:\")\n",
        "print(titanic.describe())\n",
        "\n",
        "# Vérification des valeurs manquantes\n",
        "print(\"\\nValeurs manquantes:\")\n",
        "print(titanic.isnull().sum())\n",
        "\n",
        "# Distribution de la variable cible\n",
        "print(\"\\nDistribution de la survie:\")\n",
        "print(titanic['survived'].value_counts())\n",
        "print(f\"\\nTaux de survie: {titanic['survived'].mean():.2%}\")"
      ],
      "id": "95fd09b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Visualisations Exploratoires"
      ],
      "id": "f604ebdf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Figure 1: Distribution de la survie\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Survie globale\n",
        "axes[0, 0].pie(\n",
        "    titanic['survived'].value_counts(), \n",
        "    labels=['Décédé', 'Survivant'],\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=['#ff6b6b', '#51cf66']\n",
        ")\n",
        "axes[0, 0].set_title('Distribution de la Survie')\n",
        "\n",
        "# Survie par sexe\n",
        "survival_by_sex = titanic.groupby(['sex', 'survived']).size().unstack()\n",
        "survival_by_sex.plot(kind='bar', ax=axes[0, 1], color=['#ff6b6b', '#51cf66'])\n",
        "axes[0, 1].set_title('Survie par Sexe')\n",
        "axes[0, 1].set_xlabel('Sexe')\n",
        "axes[0, 1].set_ylabel('Nombre de passagers')\n",
        "axes[0, 1].legend(['Décédé', 'Survivant'])\n",
        "axes[0, 1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Survie par classe\n",
        "survival_by_class = titanic.groupby(['pclass', 'survived']).size().unstack()\n",
        "survival_by_class.plot(kind='bar', ax=axes[1, 0], color=['#ff6b6b', '#51cf66'])\n",
        "axes[1, 0].set_title('Survie par Classe')\n",
        "axes[1, 0].set_xlabel('Classe')\n",
        "axes[1, 0].set_ylabel('Nombre de passagers')\n",
        "axes[1, 0].legend(['Décédé', 'Survivant'])\n",
        "\n",
        "# Distribution de l'âge\n",
        "axes[1, 1].hist(titanic[titanic['survived']==0]['age'].dropna(), \n",
        "                alpha=0.5, label='Décédé', bins=30, color='#ff6b6b')\n",
        "axes[1, 1].hist(titanic[titanic['survived']==1]['age'].dropna(), \n",
        "                alpha=0.5, label='Survivant', bins=30, color='#51cf66')\n",
        "axes[1, 1].set_title('Distribution de l\\'âge par survie')\n",
        "axes[1, 1].set_xlabel('Âge')\n",
        "axes[1, 1].set_ylabel('Fréquence')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "7eb903e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Préparation des Données\n",
        "\n",
        "### 3.1 Sélection des Features"
      ],
      "id": "b275720f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Sélection des colonnes pertinentes\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "target = 'survived'\n",
        "\n",
        "# Création du dataset de travail\n",
        "df = titanic[features + [target]].copy()\n",
        "\n",
        "print(f\"Dataset de travail: {df.shape}\")\n",
        "print(f\"\\nValeurs manquantes:\")\n",
        "print(df.isnull().sum())"
      ],
      "id": "ae9cfc28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Traitement des Valeurs Manquantes"
      ],
      "id": "cdd02224"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Stratégies de traitement\n",
        "# 1. Age: remplir avec la médiane\n",
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "\n",
        "# 2. Embarked: remplir avec le mode (valeur la plus fréquente)\n",
        "df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# 3. Fare: remplir avec la médiane (si manquant)\n",
        "df['fare'].fillna(df['fare'].median(), inplace=True)\n",
        "\n",
        "# Vérification\n",
        "print(\"Après traitement:\")\n",
        "print(df.isnull().sum())"
      ],
      "id": "c376181c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Encodage des Variables Catégorielles"
      ],
      "id": "96ac126b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Encodage de 'sex'\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "# Encodage de 'embarked' (One-Hot Encoding)\n",
        "df = pd.get_dummies(df, columns=['embarked'], prefix='embarked', drop_first=True)\n",
        "\n",
        "print(\"Dataset après encodage:\")\n",
        "print(df.head())\n",
        "print(f\"\\nNouvelles dimensions: {df.shape}\")"
      ],
      "id": "025c3866",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Séparation Features / Target"
      ],
      "id": "32a4cf81"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Séparation X (features) et y (target)\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"\\nFeatures utilisées:\\n{X.columns.tolist()}\")"
      ],
      "id": "d5d881dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Split Train/Validation/Test\n",
        "\n",
        "### 4.1 Split Train/Test"
      ],
      "id": "91712a7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Split 80/20\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2,      # 20% pour le test\n",
        "    random_state=42,    # reproductibilité\n",
        "    stratify=y          # préserver la distribution des classes\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape}\")\n",
        "print(f\"Test set:  {X_test.shape}\")\n",
        "\n",
        "# Vérification de la distribution\n",
        "print(f\"\\nDistribution train: {y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Distribution test:  {y_test.value_counts(normalize=True)}\")"
      ],
      "id": "cbb29c54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Split Train/Validation (optionnel)"
      ],
      "id": "a18869fa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Optionnel: créer un ensemble de validation\n",
        "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=0.2,  # 20% du train pour validation\n",
        "    random_state=42,\n",
        "    stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Train full: {X_train_full.shape}\")\n",
        "print(f\"Validation: {X_val.shape}\")\n",
        "print(f\"Test:       {X_test.shape}\")"
      ],
      "id": "cffa91fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Pipeline de Prétraitement et Entraînement\n",
        "\n",
        "### 5.1 Création du Pipeline"
      ],
      "id": "4001c26e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Pipeline: Standardisation + Modèle\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Étape 1: Standardisation\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))  # Étape 2: Modèle\n",
        "])\n",
        "\n",
        "print(\"Pipeline créé:\")\n",
        "print(pipeline)"
      ],
      "id": "5ec5402a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Entraînement du Modèle"
      ],
      "id": "478ff232"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Entraînement\n",
        "print(\"Entraînement en cours...\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(\"✓ Entraînement terminé\")\n",
        "\n",
        "# Prédictions\n",
        "y_train_pred = pipeline.predict(X_train)\n",
        "y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"✓ Prédictions effectuées\")"
      ],
      "id": "5f7e3dc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Évaluation Initiale\n",
        "\n",
        "### 6.1 Accuracy"
      ],
      "id": "af1cf77a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Calcul de l'accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Accuracy Train: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "print(f\"Accuracy Test:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Analyse de l'overfitting\n",
        "diff = train_accuracy - test_accuracy\n",
        "print(f\"\\nDifférence Train-Test: {diff:.4f}\")\n",
        "if diff < 0.05:\n",
        "    print(\"→ Bon équilibre biais-variance\")\n",
        "elif diff < 0.10:\n",
        "    print(\"→ Léger overfitting\")\n",
        "else:\n",
        "    print(\"→ Overfitting significatif\")"
      ],
      "id": "e049f9c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Matrice de Confusion"
      ],
      "id": "f3ce946d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Calcul de la matrice de confusion\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Décédé', 'Survivant'],\n",
        "            yticklabels=['Décédé', 'Survivant'])\n",
        "plt.title('Matrice de Confusion - Test Set')\n",
        "plt.ylabel('Vraie Classe')\n",
        "plt.xlabel('Classe Prédite')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interprétation\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nVrais Négatifs (TN):  {tn}\")\n",
        "print(f\"Faux Positifs (FP):   {fp}\")\n",
        "print(f\"Faux Négatifs (FN):   {fn}\")\n",
        "print(f\"Vrais Positifs (TP):  {tp}\")"
      ],
      "id": "7523a580",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Rapport de Classification"
      ],
      "id": "04f2ca0c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Rapport détaillé\n",
        "print(\"\\nRapport de Classification:\")\n",
        "print(classification_report(y_test, y_test_pred, \n",
        "                          target_names=['Décédé', 'Survivant']))"
      ],
      "id": "88a2a3e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparaison de Plusieurs Modèles"
      ],
      "id": "e3d093c7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Définition des modèles\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "}\n",
        "\n",
        "# Entraînement et évaluation\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    # Pipeline pour chaque modèle\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Entraînement\n",
        "    pipe.fit(X_train, y_train)\n",
        "    \n",
        "    # Évaluation\n",
        "    train_score = pipe.score(X_train, y_train)\n",
        "    test_score = pipe.score(X_test, y_test)\n",
        "    \n",
        "    results[name] = {\n",
        "        'train': train_score,\n",
        "        'test': test_score,\n",
        "        'diff': train_score - test_score\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Train Accuracy: {train_score:.4f}\")\n",
        "    print(f\"  Test Accuracy:  {test_score:.4f}\")\n",
        "    print(f\"  Différence:     {train_score - test_score:.4f}\")\n",
        "\n",
        "# Visualisation comparative\n",
        "df_results = pd.DataFrame(results).T\n",
        "df_results[['train', 'test']].plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Comparaison des Performances des Modèles')\n",
        "plt.xlabel('Modèle')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'])\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ae59d5bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analyse des Prédictions\n",
        "\n",
        "### 8.1 Exemples de Prédictions"
      ],
      "id": "ec306d72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Prédictions avec probabilités\n",
        "y_proba = pipeline.predict_proba(X_test)\n",
        "\n",
        "# Affichage de quelques exemples\n",
        "n_samples = 5\n",
        "indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
        "\n",
        "print(\"Exemples de prédictions:\\n\")\n",
        "for idx in indices:\n",
        "    actual = y_test.iloc[idx]\n",
        "    predicted = y_test_pred[idx]\n",
        "    proba = y_proba[idx]\n",
        "    \n",
        "    print(f\"Passager {idx}:\")\n",
        "    print(f\"  Vraie classe:     {'Survivant' if actual == 1 else 'Décédé'}\")\n",
        "    print(f\"  Prédiction:       {'Survivant' if predicted == 1 else 'Décédé'}\")\n",
        "    print(f\"  Probabilités:     Décédé={proba[0]:.2%}, Survivant={proba[1]:.2%}\")\n",
        "    print(f\"  Correct:          {'+' if actual == predicted else '+'}\")\n",
        "    print()"
      ],
      "id": "d365703e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Analyse des Erreurs"
      ],
      "id": "c255cc03"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "# Identification des erreurs\n",
        "errors = X_test[y_test != y_test_pred].copy()\n",
        "errors['actual'] = y_test[y_test != y_test_pred]\n",
        "errors['predicted'] = y_test_pred[y_test != y_test_pred]\n",
        "\n",
        "print(f\"Nombre d'erreurs: {len(errors)}\")\n",
        "print(f\"Taux d'erreur: {len(errors)/len(X_test):.2%}\")\n",
        "\n",
        "print(\"\\nQuelques erreurs:\")\n",
        "print(errors.head())\n",
        "\n",
        "# Analyse des caractéristiques des erreurs\n",
        "print(\"\\nCaractéristiques moyennes des erreurs vs correctes:\")\n",
        "correct = X_test[y_test == y_test_pred]\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Erreurs': errors.drop(['actual', 'predicted'], axis=1).mean(),\n",
        "    'Correctes': correct.mean()\n",
        "})\n",
        "print(comparison)"
      ],
      "id": "4bd5f38d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercices Pratiques\n",
        "\n",
        "::: {.callout-warning icon=false collapse=\"true\"}\n",
        "## Exercice 1: Feature Engineering\n",
        "\n",
        "Créez une nouvelle feature `family_size` = `sibsp` + `parch` + 1, puis ré-entraînez le modèle. La performance s'améliore-t-elle ?\n",
        "\n",
        "### Solution"
      ],
      "id": "05366f15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "# Création de la nouvelle feature\n",
        "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
        "\n",
        "# Refaire le split et l'entraînement\n",
        "X_new = df.drop('survived', axis=1)\n",
        "y_new = df['survived']\n",
        "\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
        "    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n",
        ")\n",
        "\n",
        "pipeline_new = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_new.fit(X_train_new, y_train_new)\n",
        "new_score = pipeline_new.score(X_test_new, y_test_new)\n",
        "\n",
        "print(f\"Accuracy avec family_size: {new_score:.4f}\")\n",
        "print(f\"Accuracy sans family_size: {test_accuracy:.4f}\")\n",
        "print(f\"Amélioration: {new_score - test_accuracy:.4f}\")"
      ],
      "id": "b682c6a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.callout-warning icon=false collapse=\"true\"}\n",
        "## Exercice 2: Optimisation des Hyperparamètres\n",
        "\n",
        "Testez différentes valeurs de `max_depth` pour le Decision Tree (3, 5, 7, 10, None). Quelle valeur donne les meilleures performances sur le test set ?\n",
        "\n",
        "### Solution"
      ],
      "id": "78c5ca9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "depths = [3, 5, 7, 10, None]\n",
        "results_depth = []\n",
        "\n",
        "for depth in depths:\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', DecisionTreeClassifier(max_depth=depth, random_state=42))\n",
        "    ])\n",
        "    \n",
        "    pipe.fit(X_train, y_train)\n",
        "    train_score = pipe.score(X_train, y_train)\n",
        "    test_score = pipe.score(X_test, y_test)\n",
        "    \n",
        "    results_depth.append({\n",
        "        'max_depth': depth,\n",
        "        'train': train_score,\n",
        "        'test': test_score,\n",
        "        'diff': train_score - test_score\n",
        "    })\n",
        "    \n",
        "df_depth = pd.DataFrame(results_depth)\n",
        "print(df_depth)\n",
        "\n",
        "# Meilleure valeur\n",
        "best_depth = df_depth.loc[df_depth['test'].idxmax(), 'max_depth']\n",
        "print(f\"\\nMeilleur max_depth: {best_depth}\")"
      ],
      "id": "2ece5bfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.callout-warning icon=false collapse=\"true\"}\n",
        "## Exercice 3: Analyse d'Importance\n",
        "\n",
        "Pour le Random Forest, affichez l'importance des features. Quelles sont les 3 features les plus importantes ?\n",
        "\n",
        "### Solution"
      ],
      "id": "6db45480"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "\n",
        "# Entraîner Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Importance des features\n",
        "importances = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': rf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Importance des features:\")\n",
        "print(importances)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importances['feature'], importances['importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Importance des Features - Random Forest')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTop 3 features:\")\n",
        "print(importances.head(3))"
      ],
      "id": "73f49a93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Résumé du TP\n",
        "\n",
        "::: {.callout-important icon=false}\n",
        "## Ce que vous avez appris\n",
        "\n",
        "1. **Chargement et exploration** de données avec pandas\n",
        "2. **Prétraitement** des données:\n",
        "   - Traitement des valeurs manquantes\n",
        "   - Encodage des variables catégorielles\n",
        "   - Standardisation\n",
        "3. **Pipeline Scikit-learn** pour automatiser le workflow\n",
        "4. **Split Train/Test** avec stratification\n",
        "5. **Entraînement et évaluation** de modèles de classification\n",
        "6. **Comparaison** de plusieurs algorithmes\n",
        "7. **Analyse des résultats** et des erreurs\n",
        ":::\n",
        "\n",
        "## Checklist de Validation\n",
        "\n",
        "- [ ] Dataset chargé et exploré\n",
        "- [ ] Valeurs manquantes traitées\n",
        "- [ ] Variables catégorielles encodées\n",
        "- [ ] Pipeline créé avec StandardScaler\n",
        "- [ ] Modèle entraîné avec succès\n",
        "- [ ] Accuracy calculée (train et test)\n",
        "- [ ] Matrice de confusion générée\n",
        "- [ ] Comparaison de plusieurs modèles effectuée\n",
        "- [ ] Analyse des erreurs réalisée\n",
        "\n",
        "## Pour Aller Plus Loin\n",
        "\n",
        "1. Testez d'autres features (titre extrait du nom, cabine, etc.)\n",
        "2. Expérimentez avec le seuil de décision (au lieu de 0.5)\n",
        "3. Utilisez la validation croisée (voir TP2)\n",
        "4. Essayez d'autres algorithmes (SVM, Gradient Boosting)"
      ],
      "id": "d49f298f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "test_env",
      "language": "python",
      "display_name": "Python (test_env)",
      "path": "C:\\Users\\abdal\\AppData\\Roaming\\jupyter\\kernels\\test_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}