# S√©ance 9: Apprentissage Non Supervis√©

::: {.callout-note icon=false}
## Informations de la s√©ance
- **Type**: Cours
- **Dur√©e**: 2h
- **Objectifs**: Obj8, Obj9
:::

## D√©finitions et Principes

L'**apprentissage non supervis√©** est un type d'apprentissage o√π le mod√®le apprend √† partir de **donn√©es non √©tiquet√©es**, sans r√©ponses connues.

**Objectif principal**: D√©couvrir des structures, des patterns ou des regroupements naturels dans les donn√©es.

::: {.callout-tip}
## Pourquoi l'apprentissage non supervis√© ?
- Les donn√©es √©tiquet√©es sont rares ou co√ªteuses √† obtenir
- Exploration de donn√©es inconnues
- R√©duction de dimension pour visualisation
- D√©tection d'anomalies
:::

## Clustering (Regroupement)

Le **clustering** consiste √† regrouper des donn√©es similaires dans des clusters (groupes).

### k-means

L'algorithme **k-means** est l'une des m√©thodes de clustering les plus populaires.

**Principe**:

1. Choisir k points initiaux (centro√Ødes)
2. Assigner chaque point au centro√Øde le plus proche
3. Recalculer les centro√Ødes (moyenne des points du cluster)
4. R√©p√©ter jusqu'√† convergence

```{python}
#| echo: true
#| eval: false

from sklearn.cluster import KMeans
import numpy as np

# Donn√©es non √©tiquet√©es
X = np.array([[1, 2], [1, 4], [1, 0],
              [10, 2], [10, 4], [10, 0]])

# Clustering avec k=2
kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(X)

print("Labels des clusters:", labels)
print("Centro√Ødes:", kmeans.cluster_centers_)
```

**Avantages**:

- Simple et rapide
- √âvolutif pour grands datasets
- R√©sultats faciles √† interpr√©ter

**Inconv√©nients**:

- N√©cessite de sp√©cifier k
- Sensible aux valeurs aberrantes
- Suppose des clusters sph√©riques et de taille similaire

### DBSCAN (Density-Based Spatial Clustering)

**DBSCAN** regroupe les points bas√©s sur la densit√©.

**Param√®tres cl√©s**:

- **eps**: distance maximale entre deux points pour √™tre consid√©r√©s voisins
- **min_samples**: nombre minimum de points pour former un cluster dense

```{python}
#| echo: true
#| eval: false

from sklearn.cluster import DBSCAN

# Clustering par densit√©
dbscan = DBSCAN(eps=1.5, min_samples=2)
labels = dbscan.fit_predict(X)

print("Labels DBSCAN:", labels)
# -1 = bruit (outliers)
```

**Avantages**:

- Pas besoin de sp√©cifier le nombre de clusters
- D√©tecte les clusters de forme arbitraire
- Robuste aux outliers

**Inconv√©nients**:

- Sensible aux param√®tres eps et min_samples
- Difficult√© avec des densit√©s vari√©es

### Autres m√©thodes

- **Agglomerative Clustering**: approche hi√©rarchique
- **Gaussian Mixture Models (GMM)**: mod√®le probabiliste
- **Mean Shift**: bas√© sur la densit√© de noyau

## Mesures de Qualit√©

Comment √©valuer la qualit√© d'un clustering sans labels vrais ?

### Silhouette Score

Mesure de coh√©rence intra-cluster et s√©paration inter-cluster.

**Valeurs**:

- Proche de 1: bonne s√©paration
- Proche de 0: clusters se chevauchent
- N√©gatif: mauvais clustering

```{python}
#| echo: true
#| eval: false

from sklearn.metrics import silhouette_score

score = silhouette_score(X, labels)
print(f"Silhouette Score: {score:.3f}")
```

### Inertie (Elbow Method)

Somme des distances carr√©es des points √† leur centro√Øde.

```{python}
#| echo: true
#| eval: false

import matplotlib.pyplot as plt

inertias = []
K = range(1, 10)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.plot(K, inertias, 'bo-')
plt.xlabel('Nombre de clusters (k)')
plt.ylabel('Inertie')
plt.title('M√©thode du coude (Elbow Method)')
plt.grid(True)
plt.show()
```

### Davies-Bouldin Index

Mesure de similarit√© moyenne entre clusters.

## Applications R√©elles

### Segmentation Client

```{python}
#| echo: true
#| eval: false

# Exemple fictif de segmentation client
import pandas as pd

data = {
    'age': [25, 30, 35, 40, 45, 50, 55, 60],
    'revenu_annuel_k': [40, 45, 50, 80, 90, 30, 35, 25],
    'score_depense': [8, 7, 6, 9, 8, 3, 4, 2]
}

df = pd.DataFrame(data)

from sklearn.preprocessing import StandardScaler

# Normalisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# Clustering
kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X_scaled)

print(df.groupby('cluster').mean())
```

### Regroupement de Documents

- Groupement d'articles par th√®me
- Organisation d'emails
- Cat√©gorisation de produits

### Analyse d'Images

- Segmentation d'image
- Regroupement de pixels similaires
- Compression d'image

## R√©duction de Dimension

### Pourquoi r√©duire la dimension ?

- Visualisation de donn√©es multidimensionnelles
- R√©duction du bruit
- Acc√©l√©ration des algorithmes
- √âviter le "fl√©au de la dimension"

### PCA (Principal Component Analysis)

**PCA** transforme les donn√©es en composantes orthogonales capturant la variance maximale.

```{python}
#| echo: true
#| eval: false

from sklearn.decomposition import PCA
import numpy as np

# Donn√©es de d√©monstration
np.random.seed(42)
X = np.random.randn(100, 5)  # 100 √©chantillons, 5 features

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"Variance expliqu√©e: {pca.explained_variance_ratio_}")
print(f"Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}")

# Visualisation
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel('Premi√®re composante principale')
plt.ylabel('Deuxi√®me composante principale')
plt.title('PCA - Visualisation 2D')
plt.show()
```

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

M√©thode non lin√©aire particuli√®rement efficace pour la visualisation.

```{python}
#| echo: true
#| eval: false

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

plt.scatter(X_tsne[:, 0], X_tsne[:, 1])
plt.xlabel('t-SNE 1')
plt.ylabel('t-SNE 2')
plt.title('t-SNE - Visualisation 2D')
plt.show()
```

## Exercices de R√©flexion

::: {.callout-warning icon=false}
## Question 1
Pour chacun des sc√©narios suivants, proposez une m√©thode de clustering adapt√©e et justifiez votre choix :

a) Segmentation de clients avec des variables d√©mographiques et comportementales
b) D√©tection de fraudes dans des transactions bancaires
c) Regroupement de documents textuels
d) Analyse de pixels d'une image satellite
:::

::: {.callout-note collapse="true"}
## Correction Question 1

**a) Segmentation de clients:**

- **M√©thode recommand√©e**: **k-means**
- **Justification**:
  - Variables d√©mographiques et comportementales ‚Üí donn√©es num√©riques
  - Nombre de segments g√©n√©ralement connu √† l'avance (ex: 3-5 segments)
  - Besoin d'interpr√©tabilit√© pour le marketing
  - Rapide et efficace sur grands volumes de clients

```{python}
#| echo: true
#| eval: false

# Exemple de segmentation client
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Pr√©paration
scaler = StandardScaler()
X_scaled = scaler.fit_transform(client_features)

# K-means avec 4 segments
kmeans = KMeans(n_clusters=4, random_state=42)
segments = kmeans.fit_predict(X_scaled)

# Profilage des segments
profiles = pd.DataFrame(X_scaled, columns=feature_names)
profiles['segment'] = segments
print(profiles.groupby('segment').mean())
```

**b) D√©tection de fraudes:**

- **M√©thode recommand√©e**: **DBSCAN** ou **Isolation Forest**
- **Justification**:

  - Fraudes = anomalies (outliers)
  - DBSCAN identifie les points de bruit (label -1)
  - Pas besoin de conna√Ætre le nombre de types de fraude
  - D√©tecte des patterns de fraude de formes vari√©es

```{python}
#| echo: true
#| eval: false

from sklearn.cluster import DBSCAN

# DBSCAN pour d√©tecter les anomalies
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(transactions_features)

# Points anormaux (potentielles fraudes)
anomalies = transactions_features[labels == -1]
print(f"Nombre de transactions suspectes: {len(anomalies)}")
```

**c) Regroupement de documents textuels:**

- **M√©thode recommand√©e**: **k-means** sur TF-IDF + **Hierarchical Clustering**
- **Justification**:

  - TF-IDF transforme texte en vecteurs num√©riques
  - K-means efficace en haute dimension (nombreux mots)
  - Hierarchical permet d'explorer la hi√©rarchie des th√®mes
  - Peut combiner avec topic modeling (LDA)

```{python}
#| echo: true
#| eval: false

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Vectorisation des textes
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X_tfidf = vectorizer.fit_transform(documents)

# Clustering
kmeans = KMeans(n_clusters=5, random_state=42)
doc_clusters = kmeans.fit_predict(X_tfidf)

# Top mots par cluster
terms = vectorizer.get_feature_names_out()
for i in range(5):
    center = kmeans.cluster_centers_[i]
    top_terms = [terms[j] for j in center.argsort()[-10:]]
    print(f"Cluster {i}: {', '.join(top_terms)}")
```

**d) Analyse de pixels d'image satellite:**

- **M√©thode recommand√©e**: **k-means** ou **Mean Shift**
- **Justification**:

  - Segmentation d'image = clustering de pixels (RGB ou multi-spectral)
  - K-means rapide pour millions de pixels
  - Mean Shift d√©tecte automatiquement le nombre de segments
  - Peut identifier zones (for√™t, eau, ville, etc.)

```{python}
#| echo: true
#| eval: false

import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Image satellite (exemple)
# image shape: (height, width, channels)
pixels = image.reshape(-1, image.shape[2])  # Reshape en (n_pixels, channels)

# K-means sur pixels
kmeans = KMeans(n_clusters=5, random_state=42)
labels = kmeans.fit_predict(pixels)

# Reconstruction de l'image segment√©e
segmented_image = labels.reshape(image.shape[:2])
plt.imshow(segmented_image, cmap='tab10')
plt.title('Segmentation de l\'image satellite')
plt.show()
```
:::

::: {.callout-warning icon=false}
## Question 2
Pour un dataset avec 10 000 √©chantillons et 50 features :

a) Expliquez comment d√©terminer le nombre optimal de clusters pour k-means
b) Proposez une approche pour visualiser la structure des clusters
c) Quel avantage PCA peut-il apporter avant le clustering ?
:::

::: {.callout-note collapse="true"}
## Correction Question 2

**a) D√©terminer le nombre optimal de clusters:**

**M√©thode 1: Elbow Method (M√©thode du coude)**

```{python}
#| echo: true
#| eval: false

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Tester diff√©rentes valeurs de k
inertias = []
silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X)
    
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X, kmeans.labels_))

# Visualisation
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Courbe du coude
ax1.plot(K_range, inertias, 'bo-')
ax1.set_xlabel('Nombre de clusters (k)')
ax1.set_ylabel('Inertie')
ax1.set_title('M√©thode du Coude')
ax1.grid(True)

# Silhouette score
ax2.plot(K_range, silhouette_scores, 'go-')
ax2.set_xlabel('Nombre de clusters (k)')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Score Silhouette')
ax2.grid(True)

plt.show()

# Le k optimal est au "coude" de la courbe d'inertie
# ET avec un bon silhouette score
```

**M√©thode 2: Gap Statistic**

```{python}
#| echo: true
#| eval: false

# Compare l'inertie observ√©e vs inertie sur donn√©es al√©atoires
def gap_statistic(X, k_max=10, n_refs=10):
    gaps = []
    for k in range(1, k_max + 1):
        # Inertie sur donn√©es r√©elles
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X)
        real_inertia = kmeans.inertia_
        
        # Inertie moyenne sur donn√©es de r√©f√©rence
        ref_inertias = []
        for _ in range(n_refs):
            X_ref = np.random.uniform(X.min(), X.max(), X.shape)
            kmeans_ref = KMeans(n_clusters=k, random_state=42)
            kmeans_ref.fit(X_ref)
            ref_inertias.append(kmeans_ref.inertia_)
        
        gap = np.log(np.mean(ref_inertias)) - np.log(real_inertia)
        gaps.append(gap)
    
    return gaps

# K optimal = premier k o√π gap commence √† d√©cro√Ætre
```

**M√©thode 3: Silhouette Analysis d√©taill√©e**

```{python}
#| echo: true
#| eval: false

from sklearn.metrics import silhouette_samples
import matplotlib.cm as cm

for k in [2, 3, 4, 5]:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    
    silhouette_vals = silhouette_samples(X, labels)
    
    plt.figure(figsize=(10, 6))
    y_lower = 10
    
    for i in range(k):
        cluster_silhouette_vals = silhouette_vals[labels == i]
        cluster_silhouette_vals.sort()
        
        size = cluster_silhouette_vals.shape[0]
        y_upper = y_lower + size
        
        plt.fill_betweenx(np.arange(y_lower, y_upper),
                         0, cluster_silhouette_vals,
                         alpha=0.7)
        y_lower = y_upper + 10
    
    plt.title(f'Silhouette Plot (k={k})')
    plt.xlabel('Coefficient Silhouette')
    plt.ylabel('Cluster')
    plt.axvline(x=silhouette_score(X, labels), color="red", linestyle="--")
    plt.show()
```

**b) Visualiser la structure des clusters:**

**Approche 1: PCA pour r√©duction 2D/3D**

```{python}
#| echo: true
#| eval: false

from sklearn.decomposition import PCA

# R√©duction √† 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualisation
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], 
                     c=labels, cmap='viridis', alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('Clusters visualis√©s avec PCA')
plt.colorbar(scatter, label='Cluster')
plt.show()

print(f"Variance expliqu√©e totale: {sum(pca.explained_variance_ratio_):.2%}")
```

**Approche 2: t-SNE pour visualisation non-lin√©aire**

```{python}
#| echo: true
#| eval: false

from sklearn.manifold import TSNE

# t-SNE (plus lent mais meilleure visualisation)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(10, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)
plt.title('Clusters visualis√©s avec t-SNE')
plt.colorbar(label='Cluster')
plt.show()
```

**Approche 3: Pairplot des features importantes**

```{python}
#| echo: true
#| eval: false

import seaborn as sns

# S√©lectionner top features par variance
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold(threshold=0.5)
X_selected = selector.fit_transform(X)

# Pairplot avec 4-5 features les plus variables
df_plot = pd.DataFrame(X_selected[:, :5], columns=[f'F{i}' for i in range(5)])
df_plot['cluster'] = labels

sns.pairplot(df_plot, hue='cluster', palette='tab10')
plt.show()
```

**c) Avantages de PCA avant le clustering:**

**1. R√©duction de dimension ‚Üí Efficacit√© computationnelle**

```{python}
#| echo: true
#| eval: false

# Sans PCA: 50 features
import time

start = time.time()
kmeans_full = KMeans(n_clusters=5, random_state=42)
kmeans_full.fit(X)  # X: (10000, 50)
time_full = time.time() - start

# Avec PCA: 10 features (gardant 95% de variance)
pca = PCA(n_components=0.95)  # Garde 95% de variance
X_pca = pca.fit_transform(X)  # X_pca: (10000, ~10)

start = time.time()
kmeans_pca = KMeans(n_clusters=5, random_state=42)
kmeans_pca.fit(X_pca)
time_pca = time.time() - start

print(f"Temps sans PCA: {time_full:.2f}s")
print(f"Temps avec PCA: {time_pca:.2f}s")
print(f"Acc√©l√©ration: {time_full/time_pca:.1f}x")
print(f"Dimensions r√©duites: {X.shape[1]} ‚Üí {X_pca.shape[1]}")
```

**2. R√©duction du bruit**

```{python}
#| echo: true
#| eval: false

# PCA √©limine les composantes de faible variance (souvent du bruit)
pca_full = PCA()
pca_full.fit(X)

# Afficher la variance par composante
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
         pca_full.explained_variance_ratio_, 'bo-')
plt.xlabel('Composante')
plt.ylabel('Variance expliqu√©e')
plt.title('Scree Plot')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
         np.cumsum(pca_full.explained_variance_ratio_), 'ro-')
plt.xlabel('Nombre de composantes')
plt.ylabel('Variance cumul√©e')
plt.axhline(y=0.95, color='g', linestyle='--', label='95%')
plt.title('Variance Cumul√©e')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Garder les composantes qui expliquent 95% de la variance
# ‚Üí √©limine le bruit des derni√®res composantes
```

**3. √âvite la mal√©diction de la dimensionnalit√©**

```{python}
#| echo: true
#| eval: false

# En haute dimension, les distances deviennent moins significatives
from scipy.spatial.distance import pdist, squareform

# Calcul des distances moyennes
distances_full = pdist(X[:100])  # Sur 100 √©chantillons pour rapidit√©
distances_pca = pdist(X_pca[:100])

print(f"Distance moyenne (50D): {np.mean(distances_full):.2f}")
print(f"Distance moyenne (10D): {np.mean(distances_pca):.2f}")
print(f"√âcart-type distances (50D): {np.std(distances_full):.2f}")
print(f"√âcart-type distances (10D): {np.std(distances_pca):.2f}")

# En dimension r√©duite, les distances sont plus discriminantes
```

**4. D√©corr√©lation des features**

```{python}
#| echo: true
#| eval: false

# PCA produit des composantes non-corr√©l√©es
# ‚Üí Am√©liore k-means qui suppose ind√©pendance

# Corr√©lation avant PCA
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.heatmap(np.corrcoef(X.T), cmap='coolwarm', center=0,
            cbar_kws={'label': 'Corr√©lation'})
plt.title('Corr√©lations avant PCA')

# Corr√©lation apr√®s PCA
plt.subplot(1, 2, 2)
sns.heatmap(np.corrcoef(X_pca.T), cmap='coolwarm', center=0,
            cbar_kws={'label': 'Corr√©lation'})
plt.title('Corr√©lations apr√®s PCA')

plt.tight_layout()
plt.show()

# Apr√®s PCA: corr√©lations nulles entre composantes
```

**R√©sum√© des avantages:**

| Avantage | Description | Impact |
|----------|-------------|--------|
| **Efficacit√©** | 50 ‚Üí 10 dimensions | 5-10x plus rapide |
| **D√©bruitage** | √âlimine variance faible | Clusters plus nets |
| **Distances** | Plus discriminantes en faible dim | Meilleur clustering |
| **D√©corr√©lation** | Features ind√©pendantes | K-means plus efficace |
| **Visualisation** | R√©duction √† 2-3D | Interpr√©tation facile |
:::

::: {.callout-warning icon=false}
## Question 3
Impl√©mentez un pipeline complet de clustering sur le dataset Iris :

1. Chargez les donn√©es (ignorer les labels pour l'apprentissage non supervis√©)
2. Appliquez PCA pour r√©duire √† 2 dimensions
3. Testez k-means avec k=2,3,4 et comparez les r√©sultats
4. Visualisez les clusters obtenus
5. Calculez le silhouette score pour chaque k
:::

::: {.callout-note collapse="true"}
## Correction Question 3

```{python}
#| echo: true
#| eval: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score

# 1. Chargement des donn√©es (SANS utiliser les labels pour clustering)
print("=" * 70)
print("PIPELINE COMPLET DE CLUSTERING - DATASET IRIS")
print("=" * 70)

iris = datasets.load_iris()
X = iris.data  # Features seulement (ignorer iris.target)
feature_names = iris.feature_names
true_labels = iris.target  # Gard√© seulement pour √©valuation finale

print(f"\n1. Chargement des donn√©es:")
print(f"   Dimensions: {X.shape}")
print(f"   Features: {feature_names}")

# Normalisation (important avant PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print(f"   ‚úì Donn√©es normalis√©es")

# 2. Application de PCA pour r√©duction √† 2D
print(f"\n2. R√©duction de dimension avec PCA:")

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(f"   Variance expliqu√©e par composante: {pca.explained_variance_ratio_}")
print(f"   Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}")
print(f"   Dimensions: {X.shape[1]}D ‚Üí {X_pca.shape[1]}D")

# Visualisation des donn√©es apr√®s PCA (sans clustering)
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], 
                     c=true_labels, cmap='viridis', 
                     alpha=0.6, edgecolors='w')
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
plt.title('Dataset Iris apr√®s PCA (color√© par vraies classes)')
plt.colorbar(scatter, label='Vraie classe')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 3. Test de k-means avec k=2,3,4
print(f"\n3. Clustering k-means avec diff√©rentes valeurs de k:")
print("-" * 70)

K_values = [2, 3, 4]
results = []

for k in K_values:
    # Clustering
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_pca)
    
    # M√©triques
    inertia = kmeans.inertia_
    silhouette = silhouette_score(X_pca, labels)
    
    # Comparaison avec vraies classes (juste pour curiosit√©)
    ari = adjusted_rand_score(true_labels, labels)
    
    results.append({
        'k': k,
        'Inertie': inertia,
        'Silhouette': silhouette,
        'ARI (vs vrai)': ari,
        'labels': labels,
        'centroids': kmeans.cluster_centers_
    })
    
    print(f"\nk = {k}:")
    print(f"   Inertie: {inertia:.2f}")
    print(f"   Silhouette Score: {silhouette:.3f}")
    print(f"   Taille des clusters: {np.bincount(labels)}")
    print(f"   ARI (comparaison avec vraies classes): {ari:.3f}")

# DataFrame des r√©sultats
df_results = pd.DataFrame([{k: v for k, v in r.items() if k not in ['labels', 'centroids']} 
                          for r in results])
print(f"\nüìä Tableau r√©capitulatif:")
print(df_results.to_string(index=False))

# Meilleur k selon silhouette
best_k = df_results.loc[df_results['Silhouette'].idxmax(), 'k']
print(f"\n‚≠ê Meilleur k selon Silhouette Score: {best_k}")

# 4. Visualisation des clusters pour chaque k
print(f"\n4. Visualisation des clusters:")

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, result in enumerate(results):
    k = result['k']
    labels = result['labels']
    centroids = result['centroids']
    ax = axes[idx]
    # Scatter plot des points
    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], 
                        c=labels, cmap='tab10', 
                        alpha=0.6, edgecolors='w', s=50)
    
    # Centro√Ødes
    ax.scatter(centroids[:, 0], centroids[:, 1], 
              c='red', marker='X', s=200, 
              edgecolors='black', linewidths=2,
              label='Centro√Ødes')
    
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    ax.set_title(f'k={k} (Silhouette={result["Silhouette"]:.3f})')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Colorbar
    plt.colorbar(scatter, ax=ax, label='Cluster')

plt.tight_layout()
plt.show()

# 5. Analyse approfondie du meilleur k
print(f"\n5. Analyse d√©taill√©e pour k={best_k}:")
print("-" * 70)

best_result = [r for r in results if r['k'] == best_k][0]
best_labels = best_result['labels']

# Profilage des clusters
print(f"\nProfilage des clusters (features originales):")

df_analysis = pd.DataFrame(X, columns=feature_names)
df_analysis['Cluster'] = best_labels

cluster_profiles = df_analysis.groupby('Cluster').agg(['mean', 'std'])
print(cluster_profiles)

# Heatmap des caract√©ristiques par cluster
cluster_means = df_analysis.groupby('Cluster').mean()

plt.figure(figsize=(10, 6))
sns.heatmap(cluster_means.T, annot=True, fmt='.2f', cmap='YlOrRd',
            cbar_kws={'label': 'Valeur moyenne'})
plt.xlabel('Cluster')
plt.ylabel('Feature')
plt.title(f'Profil des {best_k} clusters (valeurs moyennes)')
plt.tight_layout()
plt.show()

# Comparaison avec les vraies classes (curiosit√© acad√©mique)
print(f"\nüìà Comparaison avec les vraies esp√®ces d'Iris:")
print("(Note: Le clustering est NON SUPERVIS√â, cette comparaison est")
print(" juste pour comprendre ce que l'algorithme a trouv√©)")

confusion_unsupervised = pd.crosstab(
    pd.Series(true_labels, name='Vraie esp√®ce'),
    pd.Series(best_labels, name='Cluster trouv√©')
)
print(confusion_unsupervised)

# Conclusion
print(f"\n" + "=" * 70)
print("CONCLUSION")
print("=" * 70)
print(f"‚úì PCA a r√©duit les donn√©es de 4D √† 2D")
print(f"‚úì {sum(pca.explained_variance_ratio_):.1%} de variance pr√©serv√©e")
print(f"‚úì K optimal selon silhouette: {best_k}")
print(f"‚úì Silhouette score: {best_result['Silhouette']:.3f}")
print(f"‚úì Les clusters correspondent {'assez bien' if best_result['ARI (vs vrai)'] > 0.7 else 'partiellement'} aux vraies esp√®ces")
print(f"  (ARI = {best_result['ARI (vs vrai)']:.3f})")
```

**R√©sultat attendu:**

Le pipeline devrait r√©v√©ler que:

- **k=3** est optimal (correspond aux 3 esp√®ces d'Iris)
- Le silhouette score sera autour de 0.5-0.6
- PCA capture environ 95% de la variance en 2D
- Les clusters trouv√©s correspondent assez bien aux vraies esp√®ces
- Une esp√®ce (Setosa) sera bien s√©par√©e, les deux autres se chevaucheront un peu
:::

## R√©sum√© de la S√©ance

::: {.callout-important icon=false}
## Points cl√©s √† retenir

1. **Apprentissage non supervis√©** = d√©couvrir des patterns dans des donn√©es non √©tiquet√©es
2. **Clustering** = regrouper des donn√©es similaires (k-means, DBSCAN, hi√©rarchique)
3. **Mesures de qualit√©** : silhouette score, inertie, Davies-Bouldin
4. **R√©duction de dimension** : PCA (lin√©aire), t-SNE (non-lin√©aire)
5. **Applications** : segmentation client, analyse de documents, traitement d'image
6. **D√©fis** : choix du nombre de clusters, qualit√© sans v√©rit√© terrain
:::

## Lectures Compl√©mentaires

1. G√©ron, A. (2019) - Chapitre 9: Unsupervised Learning Techniques
2. [Scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html)
3. [Visualizing Data using t-SNE](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)</parameter>