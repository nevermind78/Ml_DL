# S√©ance 3: TP1 - Pipeline de Classification Binaire

::: {.callout-note icon=false}
## Informations de la s√©ance
- **Type**: Travaux Pratiques
- **Dur√©e**: 2h
- **Objectifs**: Obj6, Obj7
- **Dataset**: Titanic (pr√©diction de survie)
:::

## Objectifs du TP

√Ä la fin de ce TP, vous serez capable de:

1. Charger et explorer un dataset
2. Pr√©parer les donn√©es pour l'apprentissage
3. Cr√©er un pipeline de pr√©traitement avec Scikit-learn
4. Entra√Æner un mod√®le de classification binaire
5. √âvaluer les performances du mod√®le

üìé **Lien du TP :** [TP1 ‚Äî Pipeline Classification](https://nevermind78.github.io/AN_slides/TP1_Pipeline_Classification_pyodide.html#/title-slide)

## 1. Configuration de l'Environnement

```{python}
#| echo: true
#| eval: false

# Installation des biblioth√®ques (si n√©cessaire)
# !pip install scikit-learn pandas numpy matplotlib seaborn

# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline

# Configuration
plt.style.use('default')
sns.set_palette("husl")
np.random.seed(42)

print("‚úì Biblioth√®ques import√©es avec succ√®s")
```

## 2. Chargement et Exploration des Donn√©es

### 2.1 Chargement du Dataset Titanic

```{python}
#| echo: true
#| eval: false

# Chargement depuis seaborn
titanic = sns.load_dataset('titanic')

# Affichage des premi√®res lignes
print("Aper√ßu des donn√©es:")
print(titanic.head())

print(f"\nDimensions: {titanic.shape}")
print(f"Colonnes: {titanic.columns.tolist()}")
```

### 2.2 Exploration Initiale

```{python}
#| echo: true
#| eval: false

# Informations g√©n√©rales
print("Informations sur le dataset:")
print(titanic.info())

print("\nStatistiques descriptives:")
print(titanic.describe())

# V√©rification des valeurs manquantes
print("\nValeurs manquantes:")
print(titanic.isnull().sum())

# Distribution de la variable cible
print("\nDistribution de la survie:")
print(titanic['survived'].value_counts())
print(f"\nTaux de survie: {titanic['survived'].mean():.2%}")
```

### 2.3 Visualisations Exploratoires

```{python}
#| echo: true
#| eval: false

# Figure 1: Distribution de la survie
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Survie globale
axes[0, 0].pie(
    titanic['survived'].value_counts(), 
    labels=['D√©c√©d√©', 'Survivant'],
    autopct='%1.1f%%',
    startangle=90,
    colors=['#ff6b6b', '#51cf66']
)
axes[0, 0].set_title('Distribution de la Survie')

# Survie par sexe
survival_by_sex = titanic.groupby(['sex', 'survived']).size().unstack()
survival_by_sex.plot(kind='bar', ax=axes[0, 1], color=['#ff6b6b', '#51cf66'])
axes[0, 1].set_title('Survie par Sexe')
axes[0, 1].set_xlabel('Sexe')
axes[0, 1].set_ylabel('Nombre de passagers')
axes[0, 1].legend(['D√©c√©d√©', 'Survivant'])
axes[0, 1].tick_params(axis='x', rotation=0)

# Survie par classe
survival_by_class = titanic.groupby(['pclass', 'survived']).size().unstack()
survival_by_class.plot(kind='bar', ax=axes[1, 0], color=['#ff6b6b', '#51cf66'])
axes[1, 0].set_title('Survie par Classe')
axes[1, 0].set_xlabel('Classe')
axes[1, 0].set_ylabel('Nombre de passagers')
axes[1, 0].legend(['D√©c√©d√©', 'Survivant'])

# Distribution de l'√¢ge
axes[1, 1].hist(titanic[titanic['survived']==0]['age'].dropna(), 
                alpha=0.5, label='D√©c√©d√©', bins=30, color='#ff6b6b')
axes[1, 1].hist(titanic[titanic['survived']==1]['age'].dropna(), 
                alpha=0.5, label='Survivant', bins=30, color='#51cf66')
axes[1, 1].set_title('Distribution de l\'√¢ge par survie')
axes[1, 1].set_xlabel('√Çge')
axes[1, 1].set_ylabel('Fr√©quence')
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

## 3. Pr√©paration des Donn√©es

### 3.1 S√©lection des Features

```{python}
#| echo: true
#| eval: false

# S√©lection des colonnes pertinentes
features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']
target = 'survived'

# Cr√©ation du dataset de travail
df = titanic[features + [target]].copy()

print(f"Dataset de travail: {df.shape}")
print(f"\nValeurs manquantes:")
print(df.isnull().sum())
```

### 3.2 Traitement des Valeurs Manquantes

```{python}
#| echo: true
#| eval: false

# Strat√©gies de traitement
# 1. Age: remplir avec la m√©diane
df['age'].fillna(df['age'].median(), inplace=True)

# 2. Embarked: remplir avec le mode (valeur la plus fr√©quente)
df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)

# 3. Fare: remplir avec la m√©diane (si manquant)
df['fare'].fillna(df['fare'].median(), inplace=True)

# V√©rification
print("Apr√®s traitement:")
print(df.isnull().sum())
```

### 3.3 Encodage des Variables Cat√©gorielles

```{python}
#| echo: true
#| eval: false

# Encodage de 'sex'
df['sex'] = df['sex'].map({'male': 0, 'female': 1})

# Encodage de 'embarked' (One-Hot Encoding)
df = pd.get_dummies(df, columns=['embarked'], prefix='embarked', drop_first=True)

print("Dataset apr√®s encodage:")
print(df.head())
print(f"\nNouvelles dimensions: {df.shape}")
```

### 3.4 S√©paration Features / Target

```{python}
#| echo: true
#| eval: false

# S√©paration X (features) et y (target)
X = df.drop('survived', axis=1)
y = df['survived']

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")
print(f"\nFeatures utilis√©es:\n{X.columns.tolist()}")
```

## 4. Split Train/Validation/Test

### 4.1 Split Train/Test

```{python}
#| echo: true
#| eval: false

# Split 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% pour le test
    random_state=42,    # reproductibilit√©
    stratify=y          # pr√©server la distribution des classes
)

print(f"Train set: {X_train.shape}")
print(f"Test set:  {X_test.shape}")

# V√©rification de la distribution
print(f"\nDistribution train: {y_train.value_counts(normalize=True)}")
print(f"Distribution test:  {y_test.value_counts(normalize=True)}")
```

### 4.2 Split Train/Validation (optionnel)

```{python}
#| echo: true
#| eval: false

# Optionnel: cr√©er un ensemble de validation
X_train_full, X_val, y_train_full, y_val = train_test_split(
    X_train, y_train,
    test_size=0.2,  # 20% du train pour validation
    random_state=42,
    stratify=y_train
)

print(f"Train full: {X_train_full.shape}")
print(f"Validation: {X_val.shape}")
print(f"Test:       {X_test.shape}")
```

## 5. Pipeline de Pr√©traitement et Entra√Ænement

### 5.1 Cr√©ation du Pipeline

```{python}
#| echo: true
#| eval: false

# Pipeline: Standardisation + Mod√®le
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # √âtape 1: Standardisation
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))  # √âtape 2: Mod√®le
])

print("Pipeline cr√©√©:")
print(pipeline)
```

### 5.2 Entra√Ænement du Mod√®le

```{python}
#| echo: true
#| eval: false

# Entra√Ænement
print("Entra√Ænement en cours...")
pipeline.fit(X_train, y_train)
print("‚úì Entra√Ænement termin√©")

# Pr√©dictions
y_train_pred = pipeline.predict(X_train)
y_test_pred = pipeline.predict(X_test)

print("‚úì Pr√©dictions effectu√©es")
```

## 6. √âvaluation Initiale

### 6.1 Accuracy

```{python}
#| echo: true
#| eval: false

# Calcul de l'accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Accuracy Train: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"Accuracy Test:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# Analyse de l'overfitting
diff = train_accuracy - test_accuracy
print(f"\nDiff√©rence Train-Test: {diff:.4f}")
if diff < 0.05:
    print("‚Üí Bon √©quilibre biais-variance")
elif diff < 0.10:
    print("‚Üí L√©ger overfitting")
else:
    print("‚Üí Overfitting significatif")
```

### 6.2 Matrice de Confusion

```{python}
#| echo: true
#| eval: false

# Calcul de la matrice de confusion
cm = confusion_matrix(y_test, y_test_pred)

# Visualisation
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['D√©c√©d√©', 'Survivant'],
            yticklabels=['D√©c√©d√©', 'Survivant'])
plt.title('Matrice de Confusion - Test Set')
plt.ylabel('Vraie Classe')
plt.xlabel('Classe Pr√©dite')
plt.tight_layout()
plt.show()

# Interpr√©tation
tn, fp, fn, tp = cm.ravel()
print(f"\nVrais N√©gatifs (TN):  {tn}")
print(f"Faux Positifs (FP):   {fp}")
print(f"Faux N√©gatifs (FN):   {fn}")
print(f"Vrais Positifs (TP):  {tp}")
```

### 6.3 Rapport de Classification

```{python}
#| echo: true
#| eval: false

# Rapport d√©taill√©
print("\nRapport de Classification:")
print(classification_report(y_test, y_test_pred, 
                          target_names=['D√©c√©d√©', 'Survivant']))
```

## 7. Comparaison de Plusieurs Mod√®les

```{python}
#| echo: true
#| eval: false

# D√©finition des mod√®les
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
}

# Entra√Ænement et √©valuation
results = {}
for name, model in models.items():
    # Pipeline pour chaque mod√®le
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', model)
    ])
    
    # Entra√Ænement
    pipe.fit(X_train, y_train)
    
    # √âvaluation
    train_score = pipe.score(X_train, y_train)
    test_score = pipe.score(X_test, y_test)
    
    results[name] = {
        'train': train_score,
        'test': test_score,
        'diff': train_score - test_score
    }
    
    print(f"\n{name}:")
    print(f"  Train Accuracy: {train_score:.4f}")
    print(f"  Test Accuracy:  {test_score:.4f}")
    print(f"  Diff√©rence:     {train_score - test_score:.4f}")

# Visualisation comparative
df_results = pd.DataFrame(results).T
df_results[['train', 'test']].plot(kind='bar', figsize=(10, 6))
plt.title('Comparaison des Performances des Mod√®les')
plt.xlabel('Mod√®le')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'])
plt.xticks(rotation=45, ha='right')
plt.ylim([0, 1])
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()
```

## 8. Analyse des Pr√©dictions

### 8.1 Exemples de Pr√©dictions

```{python}
#| echo: true
#| eval: false

# Pr√©dictions avec probabilit√©s
y_proba = pipeline.predict_proba(X_test)

# Affichage de quelques exemples
n_samples = 5
indices = np.random.choice(len(X_test), n_samples, replace=False)

print("Exemples de pr√©dictions:\n")
for idx in indices:
    actual = y_test.iloc[idx]
    predicted = y_test_pred[idx]
    proba = y_proba[idx]
    
    print(f"Passager {idx}:")
    print(f"  Vraie classe:     {'Survivant' if actual == 1 else 'D√©c√©d√©'}")
    print(f"  Pr√©diction:       {'Survivant' if predicted == 1 else 'D√©c√©d√©'}")
    print(f"  Probabilit√©s:     D√©c√©d√©={proba[0]:.2%}, Survivant={proba[1]:.2%}")
    print(f"  Correct:          {'+' if actual == predicted else '+'}")
    print()
```

### 8.2 Analyse des Erreurs

```{python}
#| echo: true
#| eval: false

# Identification des erreurs
errors = X_test[y_test != y_test_pred].copy()
errors['actual'] = y_test[y_test != y_test_pred]
errors['predicted'] = y_test_pred[y_test != y_test_pred]

print(f"Nombre d'erreurs: {len(errors)}")
print(f"Taux d'erreur: {len(errors)/len(X_test):.2%}")

print("\nQuelques erreurs:")
print(errors.head())

# Analyse des caract√©ristiques des erreurs
print("\nCaract√©ristiques moyennes des erreurs vs correctes:")
correct = X_test[y_test == y_test_pred]

comparison = pd.DataFrame({
    'Erreurs': errors.drop(['actual', 'predicted'], axis=1).mean(),
    'Correctes': correct.mean()
})
print(comparison)
```

## Exercices Pratiques

::: {.callout-warning icon=false collapse="true"}
## Exercice 1: Feature Engineering

Cr√©ez une nouvelle feature `family_size` = `sibsp` + `parch` + 1, puis r√©-entra√Ænez le mod√®le. La performance s'am√©liore-t-elle ?

### Solution

```{python}
#| echo: true
#| eval: false
#| code-fold: true

# Cr√©ation de la nouvelle feature
df['family_size'] = df['sibsp'] + df['parch'] + 1

# Refaire le split et l'entra√Ænement
X_new = df.drop('survived', axis=1)
y_new = df['survived']

X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(
    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new
)

pipeline_new = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

pipeline_new.fit(X_train_new, y_train_new)
new_score = pipeline_new.score(X_test_new, y_test_new)

print(f"Accuracy avec family_size: {new_score:.4f}")
print(f"Accuracy sans family_size: {test_accuracy:.4f}")
print(f"Am√©lioration: {new_score - test_accuracy:.4f}")
```
:::

::: {.callout-warning icon=false collapse="true"}
## Exercice 2: Optimisation des Hyperparam√®tres

Testez diff√©rentes valeurs de `max_depth` pour le Decision Tree (3, 5, 7, 10, None). Quelle valeur donne les meilleures performances sur le test set ?

### Solution

```{python}
#| echo: true
#| eval: false
#| code-fold: true

depths = [3, 5, 7, 10, None]
results_depth = []

for depth in depths:
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', DecisionTreeClassifier(max_depth=depth, random_state=42))
    ])
    
    pipe.fit(X_train, y_train)
    train_score = pipe.score(X_train, y_train)
    test_score = pipe.score(X_test, y_test)
    
    results_depth.append({
        'max_depth': depth,
        'train': train_score,
        'test': test_score,
        'diff': train_score - test_score
    })
    
df_depth = pd.DataFrame(results_depth)
print(df_depth)

# Meilleure valeur
best_depth = df_depth.loc[df_depth['test'].idxmax(), 'max_depth']
print(f"\nMeilleur max_depth: {best_depth}")
```
:::

::: {.callout-warning icon=false collapse="true"}
## Exercice 3: Analyse d'Importance

Pour le Random Forest, affichez l'importance des features. Quelles sont les 3 features les plus importantes ?

### Solution

```{python}
#| echo: true
#| eval: false
#| code-fold: true

# Entra√Æner Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Importance des features
importances = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("Importance des features:")
print(importances)

# Visualisation
plt.figure(figsize=(10, 6))
plt.barh(importances['feature'], importances['importance'])
plt.xlabel('Importance')
plt.title('Importance des Features - Random Forest')
plt.tight_layout()
plt.show()

print(f"\nTop 3 features:")
print(importances.head(3))
```
:::

## R√©sum√© du TP

::: {.callout-important icon=false}
## Ce que vous avez appris

1. **Chargement et exploration** de donn√©es avec pandas
2. **Pr√©traitement** des donn√©es:
   - Traitement des valeurs manquantes
   - Encodage des variables cat√©gorielles
   - Standardisation
3. **Pipeline Scikit-learn** pour automatiser le workflow
4. **Split Train/Test** avec stratification
5. **Entra√Ænement et √©valuation** de mod√®les de classification
6. **Comparaison** de plusieurs algorithmes
7. **Analyse des r√©sultats** et des erreurs
:::

## Checklist de Validation

- [ ] Dataset charg√© et explor√©
- [ ] Valeurs manquantes trait√©es
- [ ] Variables cat√©gorielles encod√©es
- [ ] Pipeline cr√©√© avec StandardScaler
- [ ] Mod√®le entra√Æn√© avec succ√®s
- [ ] Accuracy calcul√©e (train et test)
- [ ] Matrice de confusion g√©n√©r√©e
- [ ] Comparaison de plusieurs mod√®les effectu√©e
- [ ] Analyse des erreurs r√©alis√©e

## Pour Aller Plus Loin

1. Testez d'autres features (titre extrait du nom, cabine, etc.)
2. Exp√©rimentez avec le seuil de d√©cision (au lieu de 0.5)
3. Utilisez la validation crois√©e (voir TP2)
4. Essayez d'autres algorithmes (SVM, Gradient Boosting)