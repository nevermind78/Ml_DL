{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séance 3: TP1 - Pipeline de Classification Binaire\n",
    "\n",
    "::: {.callout-note icon=false}\n",
    "## Informations de la séance\n",
    "- **Type**: Travaux Pratiques\n",
    "- **Durée**: 2h\n",
    "- **Objectifs**: Obj6, Obj7\n",
    "- **Dataset**: Titanic (prédiction de survie)\n",
    ":::\n",
    "\n",
    "## Objectifs du TP\n",
    "\n",
    "À la fin de ce TP, vous serez capable de:\n",
    "\n",
    "1. Charger et explorer un dataset\n",
    "2. Préparer les données pour l'apprentissage\n",
    "3. Créer un pipeline de prétraitement avec Scikit-learn\n",
    "4. Entraîner un modèle de classification binaire\n",
    "5. Évaluer les performances du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothèques (si nécessaire)\n",
    "# !pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Bibliothèques importées avec succès\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Exploration des Données\n",
    "\n",
    "### 2.1 Chargement du Dataset Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement depuis seaborn\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Affichage des premières lignes\n",
    "print(\"Aperçu des données:\")\n",
    "print(titanic.head())\n",
    "\n",
    "print(f\"\\nDimensions: {titanic.shape}\")\n",
    "print(f\"Colonnes: {titanic.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploration Initiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations générales\n",
    "print(\"Informations sur le dataset:\")\n",
    "print(titanic.info())\n",
    "\n",
    "print(\"\\nStatistiques descriptives:\")\n",
    "print(titanic.describe())\n",
    "\n",
    "# Vérification des valeurs manquantes\n",
    "print(\"\\nValeurs manquantes:\")\n",
    "print(titanic.isnull().sum())\n",
    "\n",
    "# Distribution de la variable cible\n",
    "print(\"\\nDistribution de la survie:\")\n",
    "print(titanic['survived'].value_counts())\n",
    "print(f\"\\nTaux de survie: {titanic['survived'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualisations Exploratoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Distribution de la survie\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Survie globale\n",
    "axes[0, 0].pie(\n",
    "    titanic['survived'].value_counts(), \n",
    "    labels=['Décédé', 'Survivant'],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=['#ff6b6b', '#51cf66']\n",
    ")\n",
    "axes[0, 0].set_title('Distribution de la Survie')\n",
    "\n",
    "# Survie par sexe\n",
    "survival_by_sex = titanic.groupby(['sex', 'survived']).size().unstack()\n",
    "survival_by_sex.plot(kind='bar', ax=axes[0, 1], color=['#ff6b6b', '#51cf66'])\n",
    "axes[0, 1].set_title('Survie par Sexe')\n",
    "axes[0, 1].set_xlabel('Sexe')\n",
    "axes[0, 1].set_ylabel('Nombre de passagers')\n",
    "axes[0, 1].legend(['Décédé', 'Survivant'])\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Survie par classe\n",
    "survival_by_class = titanic.groupby(['pclass', 'survived']).size().unstack()\n",
    "survival_by_class.plot(kind='bar', ax=axes[1, 0], color=['#ff6b6b', '#51cf66'])\n",
    "axes[1, 0].set_title('Survie par Classe')\n",
    "axes[1, 0].set_xlabel('Classe')\n",
    "axes[1, 0].set_ylabel('Nombre de passagers')\n",
    "axes[1, 0].legend(['Décédé', 'Survivant'])\n",
    "\n",
    "# Distribution de l'âge\n",
    "axes[1, 1].hist(titanic[titanic['survived']==0]['age'].dropna(), \n",
    "                alpha=0.5, label='Décédé', bins=30, color='#ff6b6b')\n",
    "axes[1, 1].hist(titanic[titanic['survived']==1]['age'].dropna(), \n",
    "                alpha=0.5, label='Survivant', bins=30, color='#51cf66')\n",
    "axes[1, 1].set_title(\"Distribution de l'âge par survie\")\n",
    "axes[1, 1].set_xlabel('Âge')\n",
    "axes[1, 1].set_ylabel('Fréquence')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Préparation des Données\n",
    "\n",
    "### 3.1 Sélection des Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "target = 'survived'\n",
    "\n",
    "df = titanic[features + [target]].copy()\n",
    "\n",
    "# Valeurs manquantes\n",
    "df['age']      = df['age'].fillna(df['age'].median())\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "df['fare']     = df['fare'].fillna(df['fare'].median())\n",
    "\n",
    "# Encodage sex → numérique (OBLIGATOIRE avant pipeline)\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# One-Hot encoding embarked\n",
    "df = pd.get_dummies(df, columns=['embarked'], prefix='embarked', drop_first=True)\n",
    "\n",
    "# Séparation X / y\n",
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "# Vérification : aucune colonne string ne doit rester\n",
    "print(X.dtypes)\n",
    "print(f\"\\nShape: {X.shape}\")\n",
    "print(f\"Types problématiques: {X.select_dtypes(include='object').columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Traitement des Valeurs Manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratégies de traitement\n",
    "# 1. Age: remplir avec la médiane\n",
    "df['age'].fillna(df['age'].median(), inplace=True)\n",
    "\n",
    "# 2. Embarked: remplir avec le mode (valeur la plus fréquente)\n",
    "df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# 3. Fare: remplir avec la médiane (si manquant)\n",
    "df['fare'].fillna(df['fare'].median(), inplace=True)\n",
    "\n",
    "# Vérification\n",
    "print(\"Après traitement:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Encodage des Variables Catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage de 'sex'\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Encodage de 'embarked' (One-Hot Encoding)\n",
    "df = pd.get_dummies(df, columns=['embarked'], prefix='embarked', drop_first=True)\n",
    "\n",
    "print(\"Dataset après encodage:\")\n",
    "print(df.head())\n",
    "print(f\"\\nNouvelles dimensions: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Séparation Features / Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation X (features) et y (target)\n",
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFeatures utilisées:\\n{X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Train/Validation/Test\n",
    "\n",
    "### 4.1 Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% pour le test\n",
    "    random_state=42,    # reproductibilité\n",
    "    stratify=y          # préserver la distribution des classes\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set:  {X_test.shape}\")\n",
    "\n",
    "# Vérification de la distribution\n",
    "print(f\"\\nDistribution train: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Distribution test:  {y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Split Train/Validation (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionnel: créer un ensemble de validation\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,  # 20% du train pour validation\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train full: {X_train_full.shape}\")\n",
    "print(f\"Validation: {X_val.shape}\")\n",
    "print(f\"Test:       {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline de Prétraitement et Entraînement\n",
    "\n",
    "### 5.1 Création du Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline: Standardisation + Modèle\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Étape 1: Standardisation\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))  # Étape 2: Modèle\n",
    "])\n",
    "\n",
    "print(\"Pipeline créé:\")\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Entraînement du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "print(\"Entraînement en cours...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"✓ Entraînement terminé\")\n",
    "\n",
    "# Prédictions\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"✓ Prédictions effectuées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation Initiale\n",
    "\n",
    "### 6.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Accuracy Train: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Accuracy Test:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Analyse de l'overfitting\n",
    "diff = train_accuracy - test_accuracy\n",
    "print(f\"\\nDifférence Train-Test: {diff:.4f}\")\n",
    "if diff < 0.05:\n",
    "    print(\"→ Bon équilibre biais-variance\")\n",
    "elif diff < 0.10:\n",
    "    print(\"→ Léger overfitting\")\n",
    "else:\n",
    "    print(\"→ Overfitting significatif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Décédé', 'Survivant'],\n",
    "            yticklabels=['Décédé', 'Survivant'])\n",
    "plt.title('Matrice de Confusion - Test Set')\n",
    "plt.ylabel('Vraie Classe')\n",
    "plt.xlabel('Classe Prédite')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interprétation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nVrais Négatifs (TN):  {tn}\")\n",
    "print(f\"Faux Positifs (FP):   {fp}\")\n",
    "print(f\"Faux Négatifs (FN):   {fn}\")\n",
    "print(f\"Vrais Positifs (TP):  {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Rapport de Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport détaillé\n",
    "print(\"\\nRapport de Classification:\")\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['Décédé', 'Survivant']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparaison de Plusieurs Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des modèles\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Pipeline pour chaque modèle\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Entraînement\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluation\n",
    "    train_score = pipe.score(X_train, y_train)\n",
    "    test_score = pipe.score(X_test, y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'train': train_score,\n",
    "        'test': test_score,\n",
    "        'diff': train_score - test_score\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train Accuracy: {train_score:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_score:.4f}\")\n",
    "    print(f\"  Différence:     {train_score - test_score:.4f}\")\n",
    "\n",
    "# Visualisation comparative\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results[['train', 'test']].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Comparaison des Performances des Modèles')\n",
    "plt.xlabel('Modèle')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse des Prédictions\n",
    "\n",
    "### 8.1 Exemples de Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions avec probabilités\n",
    "y_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "# Affichage de quelques exemples\n",
    "n_samples = 5\n",
    "indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "\n",
    "print(\"Exemples de prédictions:\\n\")\n",
    "for idx in indices:\n",
    "    actual = y_test.iloc[idx]\n",
    "    predicted = y_test_pred[idx]\n",
    "    proba = y_proba[idx]\n",
    "    \n",
    "    print(f\"Passager {idx}:\")\n",
    "    print(f\"  Vraie classe:     {'Survivant' if actual == 1 else 'Décédé'}\")\n",
    "    print(f\"  Prédiction:       {'Survivant' if predicted == 1 else 'Décédé'}\")\n",
    "    print(f\"  Probabilités:     Décédé={proba[0]:.2%}, Survivant={proba[1]:.2%}\")\n",
    "    print(f\"  Correct:          {'+' if actual == predicted else '+'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Analyse des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des erreurs\n",
    "errors = X_test[y_test != y_test_pred].copy()\n",
    "errors['actual'] = y_test[y_test != y_test_pred]\n",
    "errors['predicted'] = y_test_pred[y_test != y_test_pred]\n",
    "\n",
    "print(f\"Nombre d'erreurs: {len(errors)}\")\n",
    "print(f\"Taux d'erreur: {len(errors)/len(X_test):.2%}\")\n",
    "\n",
    "print(\"\\nQuelques erreurs:\")\n",
    "print(errors.head())\n",
    "\n",
    "# Analyse des caractéristiques des erreurs\n",
    "print(\"\\nCaractéristiques moyennes des erreurs vs correctes:\")\n",
    "correct = X_test[y_test == y_test_pred]\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Erreurs': errors.drop(['actual', 'predicted'], axis=1).mean(),\n",
    "    'Correctes': correct.mean()\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices Pratiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Feature Engineering\n",
    "\n",
    "Créez une nouvelle feature `family_size` = `sibsp` + `parch` + 1, puis ré-entraînez le modèle. La performance s'améliore-t-elle ?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la nouvelle feature\n",
    "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "\n",
    "# Refaire le split et l'entraînement\n",
    "X_new = df.drop('survived', axis=1)\n",
    "y_new = df['survived']\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n",
    ")\n",
    "\n",
    "pipeline_new = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_new.fit(X_train_new, y_train_new)\n",
    "new_score = pipeline_new.score(X_test_new, y_test_new)\n",
    "\n",
    "print(f\"Accuracy avec family_size: {new_score:.4f}\")\n",
    "print(f\"Accuracy sans family_size: {test_accuracy:.4f}\")\n",
    "print(f\"Amélioration: {new_score - test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Optimisation des Hyperparamètres\n",
    "\n",
    "Testez différentes valeurs de `max_depth` pour le Decision Tree (3, 5, 7, 10, None). Quelle valeur donne les meilleures performances sur le test set ?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [3, 5, 7, 10, None]\n",
    "results_depth = []\n",
    "\n",
    "for depth in depths:\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', DecisionTreeClassifier(max_depth=depth, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    train_score = pipe.score(X_train, y_train)\n",
    "    test_score = pipe.score(X_test, y_test)\n",
    "    \n",
    "    results_depth.append({\n",
    "        'max_depth': str(depth) if depth is not None else 'None (pas de limite)',\n",
    "        'train': train_score,\n",
    "        'test': test_score,\n",
    "        'diff': train_score - test_score\n",
    "    })\n",
    "    \n",
    "df_depth = pd.DataFrame(results_depth)\n",
    "print(\"\\nRésultats pour différentes profondeurs:\")\n",
    "print(df_depth)\n",
    "\n",
    "# CORRECTION 1: Exclure None de la recherche du meilleur modèle\n",
    "df_without_none = df_depth[df_depth['max_depth'] != 'None (pas de limite)'].copy()\n",
    "best_depth_info = df_without_none.loc[df_without_none['test'].idxmax()]\n",
    "\n",
    "print(f\"\\nMeilleur max_depth (sans None): {best_depth_info['max_depth']}\")\n",
    "print(f\"Test Accuracy: {best_depth_info['test']:.4f}\")\n",
    "print(f\"Train Accuracy: {best_depth_info['train']:.4f}\")\n",
    "print(f\"Différence Train-Test: {best_depth_info['diff']:.4f}\")\n",
    "\n",
    "# CORRECTION 2: Visualisation pour mieux comprendre\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Graphique 1: Scores Train/Test\n",
    "axes[0].plot(range(len(df_depth)), df_depth['train'], 'o-', label='Train', linewidth=2)\n",
    "axes[0].plot(range(len(df_depth)), df_depth['test'], 's-', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Profondeur')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Performance selon la profondeur')\n",
    "axes[0].set_xticks(range(len(df_depth)))\n",
    "axes[0].set_xticklabels(df_depth['max_depth'], rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2: Différence Train-Test (overfitting)\n",
    "axes[1].bar(range(len(df_depth)), df_depth['diff'], color='red', alpha=0.6)\n",
    "axes[1].set_xlabel('Profondeur')\n",
    "axes[1].set_ylabel('Différence Train-Test')\n",
    "axes[1].set_title('Niveau d\\'overfitting')\n",
    "axes[1].set_xticks(range(len(df_depth)))\n",
    "axes[1].set_xticklabels(df_depth['max_depth'], rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# CORRECTION 3: Analyse détaillée\n",
    "print(\"\\n=== ANALYSE DÉTAILLÉE ===\")\n",
    "print(\"\\n1. max_depth = 3:\")\n",
    "print(f\"   - Test: {df_depth.loc[0, 'test']:.4f}, Train: {df_depth.loc[0, 'train']:.4f}\")\n",
    "print(\"   - Modèle simple, peu d'overfitting mais peut être sous-optimal\")\n",
    "\n",
    "print(\"\\n2. max_depth = 5:\")\n",
    "print(f\"   - Test: {df_depth.loc[1, 'test']:.4f}, Train: {df_depth.loc[1, 'train']:.4f}\")\n",
    "print(\"   - Bon compromis entre biais et variance\")\n",
    "\n",
    "print(\"\\n3. max_depth = 7 ou 10:\")\n",
    "print(f\"   - Test: {df_depth.loc[2, 'test']:.4f} et {df_depth.loc[3, 'test']:.4f}\")\n",
    "print(\"   - Performance légèrement meilleure mais risque d'overfitting accru\")\n",
    "\n",
    "print(\"\\n4. max_depth = None (pas de limite):\")\n",
    "print(f\"   - Test: {df_depth.loc[4, 'test']:.4f}, Train: {df_depth.loc[4, 'train']:.4f}\")\n",
    "print(f\"   - Différence: {df_depth.loc[4, 'diff']:.4f}\")\n",
    "print(\"   - FORT RISQUE D'OVERFITTING sur de nouvelles données\")\n",
    "print(\"   - Le modèle mémorise le bruit des données d'entraînement\")\n",
    "\n",
    "# CORRECTION 4: Choix recommandé avec justification\n",
    "print(\"\\n=== RECOMMANDATION ===\")\n",
    "print(f\"Le meilleur choix est max_depth = {best_depth_info['max_depth']} car:\")\n",
    "print(\"1. Il maximise le score sur le test set parmi les valeurs raisonnables\")\n",
    "print(\"2. La différence Train-Test est acceptable (équilibre biais-variance)\")\n",
    "print(\"3. Le modèle reste interprétable (arbre de taille raisonnable)\")\n",
    "\n",
    "# Validation supplémentaire avec validation croisée\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "if best_depth_info['max_depth'] != 'None (pas de limite)':\n",
    "    depth_value = int(best_depth_info['max_depth'])\n",
    "    \n",
    "    # Validation croisée pour plus de robustesse\n",
    "    cv_scores = cross_val_score(\n",
    "        DecisionTreeClassifier(max_depth=depth_value, random_state=42),\n",
    "        X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nValidation croisée (5 folds) pour max_depth = {depth_value}:\")\n",
    "    print(f\"  Scores: {cv_scores}\")\n",
    "    print(f\"  Moyenne: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Analyse d'Importance\n",
    "\n",
    "Pour le Random Forest, affichez l'importance des features. Quelles sont les 3 features les plus importantes ?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Importance des features\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Importance des features:\")\n",
    "print(importances)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importances['feature'], importances['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Importance des Features - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 3 features:\")\n",
    "print(importances.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# MODÈLE AVEC LES 4 FEATURES LES PLUS IMPORTANTES\n",
    "# ===========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODÈLE SIMPLIFIÉ - 4 FEATURES PRINCIPALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Sélection des 4 features les plus importantes\n",
    "top_features = ['sex', 'fare', 'age', 'pclass']\n",
    "print(f\"\\nFeatures sélectionnées: {top_features}\")\n",
    "\n",
    "# 2. Création du nouveau dataset avec uniquement ces features\n",
    "X_top = df[top_features].copy()\n",
    "y_top = df['survived']\n",
    "\n",
    "print(f\"\\nNouvelles dimensions:\")\n",
    "print(f\"X shape: {X_top.shape}\")\n",
    "print(f\"y shape: {y_top.shape}\")\n",
    "\n",
    "# 3. Vérification des données\n",
    "print(\"\\nStatistiques des features sélectionnées:\")\n",
    "print(X_top.describe())\n",
    "\n",
    "print(\"\\nValeurs manquantes:\")\n",
    "print(X_top.isnull().sum())\n",
    "\n",
    "# 4. Split Train/Test avec les nouvelles features\n",
    "X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(\n",
    "    X_top, y_top, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_top\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit des données:\")\n",
    "print(f\"Train: {X_train_top.shape}\")\n",
    "print(f\"Test:  {X_test_top.shape}\")\n",
    "\n",
    "# 5. Pipeline optimisé pour les 4 features\n",
    "pipeline_top = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        max_features='sqrt'  # Adaptation pour peu de features\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"ENTRAÎNEMENT DU MODÈLE SIMPLIFIÉ\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Entraînement\n",
    "pipeline_top.fit(X_train_top, y_train_top)\n",
    "\n",
    "# Prédictions\n",
    "y_train_pred_top = pipeline_top.predict(X_train_top)\n",
    "y_test_pred_top = pipeline_top.predict(X_test_top)\n",
    "\n",
    "# 6. Évaluation du modèle simplifié\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"ÉVALUATION DU MODÈLE SIMPLIFIÉ\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Accuracy\n",
    "train_acc_top = accuracy_score(y_train_top, y_train_pred_top)\n",
    "test_acc_top = accuracy_score(y_test_top, y_test_pred_top)\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"Train Accuracy: {train_acc_top:.4f} ({train_acc_top*100:.2f}%)\")\n",
    "print(f\"Test Accuracy:  {test_acc_top:.4f} ({test_acc_top*100:.2f}%)\")\n",
    "\n",
    "# Comparaison avec le modèle complet (si disponible)\n",
    "try:\n",
    "    print(f\"\\nComparaison avec modèle complet ({len(X.columns)} features):\")\n",
    "    print(f\"Test Accuracy complet:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"Test Accuracy simplifié: {test_acc_top:.4f} ({test_acc_top*100:.2f}%)\")\n",
    "    \n",
    "    diff = test_acc_top - test_accuracy\n",
    "    if diff > 0:\n",
    "        print(f\"Amélioration: +{diff:.4f} ({diff*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Dégradation: {diff:.4f} ({diff*100:.2f}%)\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 7. Matrice de confusion\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"MATRICE DE CONFUSION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "cm_top = confusion_matrix(y_test_top, y_test_pred_top)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Matrice de confusion\n",
    "sns.heatmap(cm_top, annot=True, fmt='d', cmap='Blues', ax=ax[0],\n",
    "            xticklabels=['Décédé', 'Survivant'],\n",
    "            yticklabels=['Décédé', 'Survivant'])\n",
    "ax[0].set_title('Matrice de Confusion - Modèle Simplifié')\n",
    "ax[0].set_ylabel('Vraie Classe')\n",
    "ax[0].set_xlabel('Classe Prédite')\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de Classification:\")\n",
    "print(classification_report(y_test_top, y_test_pred_top, \n",
    "                          target_names=['Décédé', 'Survivant']))\n",
    "\n",
    "# 8. Importance des features dans le nouveau modèle\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"IMPORTANCE DES FEATURES DANS LE MODÈLE SIMPLIFIÉ\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "rf_top = pipeline_top.named_steps['classifier']\n",
    "importances_top = pd.DataFrame({\n",
    "    'feature': top_features,\n",
    "    'importance': rf_top.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nImportance relative:\")\n",
    "for idx, row in importances_top.iterrows():\n",
    "    print(f\"{row['feature']}: {row['importance']:.4f} ({row['importance']*100:.1f}%)\")\n",
    "\n",
    "# Visualisation\n",
    "ax[1].barh(importances_top['feature'], importances_top['importance'], color='darkgreen')\n",
    "ax[1].set_xlabel('Importance')\n",
    "ax[1].set_title('Importance des Features - Modèle Simplifié')\n",
    "ax[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Analyse des probabilités de prédiction\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"ANALYSE DES PROBABILITÉS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "y_proba_top = pipeline_top.predict_proba(X_test_top)\n",
    "\n",
    "# Exemples de prédictions avec probabilités\n",
    "print(\"\\nExemples de prédictions (5 passagers aléatoires):\")\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test_top), 5, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    actual = y_test_top.iloc[idx]\n",
    "    predicted = y_test_pred_top[idx]\n",
    "    proba_survive = y_proba_top[idx, 1]\n",
    "    \n",
    "    print(f\"\\nPassager {i+1}:\")\n",
    "    print(f\"  Features: \", end=\"\")\n",
    "    for feat in top_features:\n",
    "        print(f\"{feat}={X_test_top.iloc[idx][feat]:.2f} \", end=\"\")\n",
    "    \n",
    "    print(f\"\\n  Probabilité survie: {proba_survive:.2%}\")\n",
    "    print(f\"  Prédiction: {'Survivant' if predicted == 1 else 'Décédé'}\")\n",
    "    print(f\"  Réel: {'Survivant' if actual == 1 else 'Décédé'}\")\n",
    "    print(f\"  Correct: {'✓' if actual == predicted else '✗'}\")\n",
    "\n",
    "# 10. Interprétation des règles de décision\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"INTERPRÉTATION DES RÈGLES DE DÉCISION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Exemple d'arbre de décision (premier arbre de la forêt)\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "tree_to_plot = rf_top.estimators_[0]  # Premier arbre\n",
    "\n",
    "plot_tree(tree_to_plot, \n",
    "          feature_names=top_features,\n",
    "          class_names=['Décédé', 'Survivant'],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          ax=ax,\n",
    "          max_depth=3)  # Limiter la profondeur pour lisibilité\n",
    "plt.title(\"Exemple d'arbre de décision (premier arbre - profondeur limitée)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Règles simples extraites\n",
    "print(\"\\nRègles de décision principales:\")\n",
    "print(\"1. Si sex=female → forte probabilité de survie\")\n",
    "print(\"2. Si sex=male ET fare > moyenne → chance modérée\")\n",
    "print(\"3. Si sex=male ET fare bas ET age élevé → faible chance\")\n",
    "print(\"4. Classe 1 améliore les chances, surtout pour les hommes\")\n",
    "\n",
    "# 11. Validation croisée pour robustesse\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"VALIDATION CROISÉE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    pipeline_top,\n",
    "    X_top,\n",
    "    y_top,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"\\nScores de validation croisée (5 folds):\")\n",
    "print(f\"  Scores individuels: {cv_scores}\")\n",
    "print(f\"  Moyenne: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Écart-type: {cv_scores.std():.4f}\")\n",
    "print(f\"  Intervalle de confiance (95%): {cv_scores.mean():.4f} ± {cv_scores.std()*2:.4f}\")\n",
    "\n",
    "# 12. Avantages du modèle simplifié\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"AVANTAGES DU MODÈLE SIMPLIFIÉ\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n✓ 1. SIMPLICITÉ:\")\n",
    "print(f\"   - Features: {len(top_features)} vs {len(X.columns)}\")\n",
    "print(f\"   - Complexité réduite\")\n",
    "\n",
    "print(\"\\n✓ 2. INTERPRÉTABILITÉ:\")\n",
    "print(f\"   - Compréhension facile des décisions\")\n",
    "print(f\"   - Règles explicables aux non-experts\")\n",
    "\n",
    "print(\"\\n✓ 3. ROBUSTESSE:\")\n",
    "print(f\"   - Moins sensible au bruit\")\n",
    "print(f\"   - Généralisation potentiellement meilleure\")\n",
    "\n",
    "print(\"\\n✓ 4. EFFICACITÉ:\")\n",
    "print(f\"   - Entraînement plus rapide\")\n",
    "print(f\"   - Moins de mémoire requise\")\n",
    "print(f\"   - Prédictions plus rapides\")\n",
    "\n",
    "# 13. Exemple d'utilisation pour de nouvelles prédictions\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"EXEMPLE D'UTILISATION PRATIQUE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Fonction de prédiction simplifiée\n",
    "def predict_survival(sex, fare, age, pclass):\n",
    "    \"\"\"\n",
    "    Prédit la survie d'un passager\n",
    "    \n",
    "    Paramètres:\n",
    "    - sex: 0 pour male, 1 pour female\n",
    "    - fare: prix du billet\n",
    "    - age: âge en années\n",
    "    - pclass: classe (1, 2, 3)\n",
    "    \"\"\"\n",
    "    # Création du DataFrame pour la prédiction\n",
    "    new_passenger = pd.DataFrame({\n",
    "        'sex': [sex],\n",
    "        'fare': [fare],\n",
    "        'age': [age],\n",
    "        'pclass': [pclass]\n",
    "    })\n",
    "    \n",
    "    # Prédiction\n",
    "    survival_prob = pipeline_top.predict_proba(new_passenger)[0, 1]\n",
    "    prediction = pipeline_top.predict(new_passenger)[0]\n",
    "    \n",
    "    return prediction, survival_prob\n",
    "\n",
    "# Exemples pratiques\n",
    "print(\"\\nExemples de prédictions pour de nouveaux passagers:\")\n",
    "\n",
    "exemples = [\n",
    "    {\"sex\": 1, \"fare\": 100, \"age\": 25, \"pclass\": 1, \"desc\": \"Femme jeune, 1ère classe\"},\n",
    "    {\"sex\": 0, \"fare\": 10, \"age\": 45, \"pclass\": 3, \"desc\": \"Homme adulte, 3ème classe\"},\n",
    "    {\"sex\": 1, \"fare\": 30, \"age\": 8, \"pclass\": 2, \"desc\": \"Fille enfant, 2ème classe\"},\n",
    "    {\"sex\": 0, \"fare\": 80, \"age\": 60, \"pclass\": 1, \"desc\": \"Homme âgé, 1ère classe\"}\n",
    "]\n",
    "\n",
    "for exemple in exemples:\n",
    "    pred, prob = predict_survival(\n",
    "        exemple[\"sex\"], \n",
    "        exemple[\"fare\"], \n",
    "        exemple[\"age\"], \n",
    "        exemple[\"pclass\"]\n",
    "    )\n",
    "    \n",
    "    sex_str = \"Femme\" if exemple[\"sex\"] == 1 else \"Homme\"\n",
    "    surv_str = \"SURVIVANT\" if pred == 1 else \"DÉCÉDÉ\"\n",
    "    \n",
    "    print(f\"\\n{exemple['desc']}:\")\n",
    "    print(f\"  → Prédiction: {surv_str}\")\n",
    "    print(f\"  → Probabilité de survie: {prob:.1%}\")\n",
    "    print(f\"  → Facteurs favorables: \", end=\"\")\n",
    "    \n",
    "    factors = []\n",
    "    if exemple[\"sex\"] == 1:\n",
    "        factors.append(\"sexe féminin\")\n",
    "    if exemple[\"fare\"] > 50:\n",
    "        factors.append(\"billet cher\")\n",
    "    if exemple[\"age\"] < 18:\n",
    "        factors.append(\"enfant\")\n",
    "    if exemple[\"pclass\"] == 1:\n",
    "        factors.append(\"1ère classe\")\n",
    "    \n",
    "    if factors:\n",
    "        print(\", \".join(factors))\n",
    "    else:\n",
    "        print(\"aucun (risque élevé)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONCLUSION : MODÈLE SIMPLIFIÉ VALIDÉ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Résumé final:\n",
    "• Features utilisées: {len(top_features)} (sur {len(X.columns)} initiales)\n",
    "• Test Accuracy: {test_acc_top:.2%}\n",
    "• Complexité: TRÈS RÉDUITE\n",
    "• Interprétabilité: EXCELLENTE\n",
    "\n",
    "Recommandation:\n",
    "✓ Ce modèle simplifié est suffisant pour la majorité des cas d'usage\n",
    "✓ Il est plus facile à déployer et maintenir\n",
    "✓ Les décisions sont compréhensibles par les humains\n",
    "✓ Performance similaire au modèle complexe\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# VERSION ULTRA-SIMPLE : LOGISTIC REGRESSION\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERSION ULTRA-SIMPLE : LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pipeline ultra-simple\n",
    "pipeline_simple = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "pipeline_simple.fit(X_train_top, y_train_top)\n",
    "\n",
    "# Évaluation\n",
    "y_pred_simple = pipeline_simple.predict(X_test_top)\n",
    "acc_simple = accuracy_score(y_test_top, y_pred_simple)\n",
    "\n",
    "print(f\"\\nAccuracy avec Logistic Regression: {acc_simple:.4f} ({acc_simple*100:.2f}%)\")\n",
    "\n",
    "# Coefficients pour interprétation\n",
    "lr_model = pipeline_simple.named_steps['classifier']\n",
    "coefficients = pd.DataFrame({\n",
    "    'feature': top_features,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'impact': np.exp(lr_model.coef_[0])  # Odds ratio\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nCoefficients (interprétation):\")\n",
    "print(coefficients)\n",
    "\n",
    "print(\"\\nInterprétation:\")\n",
    "print(\"• Coefficient POSITIF = augmente la probabilité de survie\")\n",
    "print(\"• Coefficient NÉGATIF = diminue la probabilité de survie\")\n",
    "print(\"• Odds Ratio > 1 = multiplie les chances de survie\")\n",
    "print(\"• Odds Ratio < 1 = divise les chances de survie\")\n",
    "\n",
    "# Exemple : impact d'une augmentation d'une unité\n",
    "print(\"\\nImpact pratique:\")\n",
    "for _, row in coefficients.iterrows():\n",
    "    odds = row['impact']\n",
    "    if odds > 1:\n",
    "        change = f\"augmente de {(odds-1)*100:.0f}%\"\n",
    "    else:\n",
    "        change = f\"diminue de {(1-odds)*100:.0f}%\"\n",
    "    \n",
    "    print(f\"  • {row['feature']}: 1 unité supplémentaire {change} les chances de survie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé du TP\n",
    "\n",
    "### Ce que vous avez appris\n",
    "\n",
    "1. **Chargement et exploration** de données avec pandas\n",
    "2. **Prétraitement** des données:\n",
    "   - Traitement des valeurs manquantes\n",
    "   - Encodage des variables catégorielles\n",
    "   - Standardisation\n",
    "3. **Pipeline Scikit-learn** pour automatiser le workflow\n",
    "4. **Split Train/Test** avec stratification\n",
    "5. **Entraînement et évaluation** de modèles de classification\n",
    "6. **Comparaison** de plusieurs algorithmes\n",
    "7. **Analyse des résultats** et des erreurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist de Validation\n",
    "\n",
    "- [ ] Dataset chargé et exploré\n",
    "- [ ] Valeurs manquantes traitées\n",
    "- [ ] Variables catégorielles encodées\n",
    "- [ ] Pipeline créé avec StandardScaler\n",
    "- [ ] Modèle entraîné avec succès\n",
    "- [ ] Accuracy calculée (train et test)\n",
    "- [ ] Matrice de confusion générée\n",
    "- [ ] Comparaison de plusieurs modèles effectuée\n",
    "- [ ] Analyse des erreurs réalisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour Aller Plus Loin\n",
    "\n",
    "1. Testez d'autres features (titre extrait du nom, cabine, etc.)\n",
    "2. Expérimentez avec le seuil de décision (au lieu de 0.5)\n",
    "3. Utilisez la validation croisée (voir TP2)\n",
    "4. Essayez d'autres algorithmes (SVM, Gradient Boosting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
