{
  "hash": "21fcd37de480773729e314bc701481a2",
  "result": {
    "engine": "jupyter",
    "markdown": "# S√©ance 1: Introduction IA et Machine Learning\n\n::: {.callout-note icon=false}\n## Informations de la s√©ance\n- **Type**: Cours\n- **Dur√©e**: 2h\n- **Objectifs**: Obj1, Obj2, Obj3\n:::\n\n## 1. D√©finitions et Concepts de Base\n\n### 1.1 Intelligence Artificielle (IA)\n\nL'**Intelligence Artificielle** est un domaine de l'informatique qui vise √† cr√©er des syst√®mes capables d'effectuer des t√¢ches n√©cessitant normalement l'intelligence humaine.\n\n::: {.callout-tip}\n## Exemples d'IA au quotidien\n- Assistants vocaux (Siri, Alexa, Google Assistant)\n- Recommandations Netflix/Spotify\n- Filtres anti-spam des emails\n- Reconnaissance faciale sur smartphones\n- Traduction automatique\n:::\n\n### 1.2 Machine Learning (Apprentissage Automatique)\n\nLe **Machine Learning** est une sous-discipline de l'IA qui permet aux ordinateurs d'apprendre √† partir de donn√©es sans √™tre explicitement programm√©s.\n\n**Diff√©rence cl√©**:\n\n- **Programmation traditionnelle**: Humain √©crit les r√®gles ‚Üí Ordinateur applique\n- **Machine Learning**: Ordinateur apprend les r√®gles √† partir des donn√©es\n\n```python\n# Approche traditionnelle\ndef classifier_email(email):\n    if \"viagra\" in email or \"lottery\" in email:\n        return \"spam\"\n    else:\n        return \"not spam\"\n\n# Approche Machine Learning\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)  # Apprend des exemples\nprediction = model.predict(new_email)\n```\n\n### 1.3 Deep Learning\n\nLe **Deep Learning** est une sous-cat√©gorie du ML utilisant des r√©seaux de neurones artificiels profonds (plusieurs couches).\n\n**Diagramme mermaid:**\n```{mermaid}\ngraph TD\n    A[Intelligence Artificielle] --> B[Machine Learning]\n    B --> C[Deep Learning]\n    A --> D[Syst√®mes experts]\n    A --> E[Robotique]\n    B --> F[Apprentissage supervis√©]\n    B --> G[Apprentissage non supervis√©]\n    B --> H[Apprentissage par renforcement]\n```\n\n## 2. Applications et Cas d'Utilisation\n\n### 2.1 Vision par Ordinateur\n- D√©tection d'objets\n- Reconnaissance faciale\n- Diagnostic m√©dical (imagerie)\n- Voitures autonomes\n\n### 2.2 Traitement du Langage Naturel (NLP)\n- Chatbots et assistants virtuels\n- Traduction automatique\n- Analyse de sentiments\n- R√©sum√© automatique de textes\n\n### 2.3 Syst√®mes de Recommandation\n- E-commerce (Amazon, Alibaba)\n- Streaming (Netflix, YouTube)\n- R√©seaux sociaux (Facebook, Instagram)\n\n### 2.4 Finance\n- D√©tection de fraude\n- Trading algorithmique\n- √âvaluation de risque de cr√©dit\n\n### 2.5 Sant√©\n- Diagnostic de maladies\n- D√©couverte de m√©dicaments\n- Analyse d'imagerie m√©dicale\n\n## 3. Types d'Apprentissage\n\n### 3.1 Apprentissage Supervis√©\n\nLe mod√®le apprend √† partir de **donn√©es √©tiquet√©es** (avec r√©ponses connues).\n\n::: {.callout-note}\n## Exemple\n**Donn√©es d'entra√Ænement**: emails avec labels \"spam\" ou \"non spam\"\n**Objectif**: Pr√©dire si un nouveau email est spam\n:::\n\n**T√¢ches principales**:\n- **Classification**: pr√©dire une cat√©gorie (spam/non spam, chat/chien)\n- **R√©gression**: pr√©dire une valeur continue (prix maison, temp√©rature)\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Classification\nclf = LogisticRegression()\nclf.fit(X_train, y_train)  # y_train contient les cat√©gories\npred_class = clf.predict(X_test)\n\n# R√©gression\nreg = RandomForestRegressor()\nreg.fit(X_train, y_train)  # y_train contient les valeurs continues\npred_value = reg.predict(X_test)\n```\n\n### 3.2 Apprentissage Non Supervis√©\n\nLe mod√®le apprend √† partir de **donn√©es non √©tiquet√©es** (sans r√©ponses).\n\n::: {.callout-note}\n## Exemple\n**Donn√©es**: comportements d'achat de clients\n**Objectif**: Identifier des groupes de clients similaires (segmentation)\n:::\n\n**T√¢ches principales**:\n- **Clustering**: regrouper des donn√©es similaires\n- **R√©duction de dimension**: simplifier les donn√©es\n- **D√©tection d'anomalies**: identifier des points inhabituels\n\n```python\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Clustering\nkmeans = KMeans(n_clusters=3)\nclusters = kmeans.fit_predict(X)\n\n# R√©duction de dimension\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n```\n\n### 3.3 Apprentissage Semi-Supervis√©\n\nCombine donn√©es √©tiquet√©es (peu) et non √©tiquet√©es (beaucoup).\n\n**Cas d'usage**: Lorsque l'√©tiquetage est co√ªteux (imagerie m√©dicale, reconnaissance vocale)\n\n### 3.4 Apprentissage par Renforcement\n\nL'agent apprend par **essai-erreur** en interagissant avec un environnement.\n\n::: {.callout-note}\n## Exemple\n- Jeux vid√©o (AlphaGo, Chess AI)\n- Robotique\n- Contr√¥le de syst√®mes complexes\n:::\n\n**Composants**:\n- **Agent**: celui qui apprend\n- **Environnement**: le monde dans lequel l'agent √©volue\n- **Actions**: ce que l'agent peut faire\n- **R√©compenses**: feedback positif/n√©gatif\n\n## 4. √âtapes de Conception d'un Mod√®le IA\n\n### 4.1 Pipeline ML Standard\n\n**Diagramme mermaid:**\n```{mermaid}\ngraph TB\n    A[1 D√©finir le probl√®me] --> B[2 Collecter les donn√©es]\n    B --> C[3 Explorer les donn√©es]\n    C --> D[4 Pr√©parer les donn√©es]\n    D --> E[5 Choisir un mod√®le]\n    E --> F[6 Entra√Æner le mod√®le]\n    F --> G[7 √âvaluer le mod√®le]\n    G --> H{Performance OK?}\n    H -->|Non| E\n    H -->|Oui| I[8 D√©ployer]\n    I --> J[9 Monitorer]\n```\n\n### 4.2 D√©tails des √âtapes\n\n#### √âtape 1: D√©finir le Probl√®me\n- Quel type de probl√®me? (classification, r√©gression, clustering)\n- Quelles sont les m√©triques de succ√®s?\n- Quelles sont les contraintes?\n\n#### √âtape 2: Collecter les Donn√©es\n- Sources de donn√©es\n- Quantit√© n√©cessaire\n- Qualit√© des donn√©es\n\n#### √âtape 3: Explorer les Donn√©es (EDA)\n- Statistiques descriptives\n- Visualisations\n- Identifier les patterns, outliers, donn√©es manquantes\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Exemple EDA simple\ndf = pd.read_csv('data.csv')\nprint(df.info())\nprint(df.describe())\n\n# Visualisation\nsns.pairplot(df)\nplt.show()\n```\n\n#### √âtape 4: Pr√©parer les Donn√©es\n- Nettoyage (valeurs manquantes, doublons)\n- Transformation (normalisation, encodage)\n- Feature engineering\n- Split train/test\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Split des donn√©es\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Normalisation\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Gestion des valeurs manquantes\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n```\n\n#### √âtape 5: Choisir un Mod√®le\n- Bas√© sur le type de probl√®me\n- Complexit√© vs interpr√©tabilit√©\n- Ressources disponibles\n\n#### √âtape 6: Entra√Æner le Mod√®le\n- Ajuster les param√®tres\n- Optimisation\n\n#### √âtape 7: √âvaluer le Mod√®le\n- M√©triques appropri√©es\n- Validation crois√©e\n- Analyse des erreurs\n\n#### √âtape 8: D√©ployer\n- Mise en production\n- API, application web, etc.\n\n#### √âtape 9: Monitorer\n- Performances en production\n- D√©rive des donn√©es (data drift)\n- Mise √† jour du mod√®le\n\n## 5. Concepts Cl√©s\n\n### 5.1 Overfitting vs Underfitting\n\n::: {.panel-tabset}\n\n## Underfitting\n- Mod√®le **trop simple**\n- Ne capture pas les patterns dans les donn√©es\n- **Biais √©lev√©**, variance faible\n- Mauvaise performance train ET test\n\n## Overfitting\n- Mod√®le **trop complexe**\n- M√©morise les donn√©es d'entra√Ænement (bruit inclus)\n- Biais faible, **variance √©lev√©e**\n- Bonne performance train, **mauvaise** performance test\n\n## Juste bien (Good fit)\n- Mod√®le √©quilibr√©\n- Capture les vrais patterns\n- Biais et variance faibles\n- Bonne g√©n√©ralisation\n\n:::\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# G√©n√©ration de donn√©es\nnp.random.seed(42)\nX = np.linspace(0, 10, 50)\ny = 2*X + 1 + np.random.randn(50)*2\n\n# Sous-ajustement (linear)\nunderfit_model = LinearRegression()\nunderfit_model.fit(X.reshape(-1, 1), y)\ny_underfit = underfit_model.predict(X.reshape(-1, 1))\n\n# Bon ajustement (polynomial degree 2)\ngoodfit_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=2)),\n    ('linear', LinearRegression())\n])\ngoodfit_model.fit(X.reshape(-1, 1), y)\ny_goodfit = goodfit_model.predict(X.reshape(-1, 1))\n\n# Surajustement (polynomial degree 15)\noverfit_model = Pipeline([\n    ('poly', PolynomialFeatures(degree=15)),\n    ('linear', LinearRegression())\n])\noverfit_model.fit(X.reshape(-1, 1), y)\ny_overfit = overfit_model.predict(X.reshape(-1, 1))\n\n# Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].scatter(X, y, alpha=0.5)\naxes[0].plot(X, y_underfit, 'r-', linewidth=2)\naxes[0].set_title('Underfitting (lin√©aire)')\n\naxes[1].scatter(X, y, alpha=0.5)\naxes[1].plot(X, y_goodfit, 'g-', linewidth=2)\naxes[1].set_title('Good Fit (polynomial deg 2)')\n\naxes[2].scatter(X, y, alpha=0.5)\naxes[2].plot(X, y_overfit, 'b-', linewidth=2)\naxes[2].set_title('Overfitting (polynomial deg 15)')\n\nplt.tight_layout()\nplt.show()\n```\n\n### 5.2 Compromis Biais-Variance\n\n\n## **L'√©quilibre fondamental du Machine Learning**\n\nLe **compromis biais-variance** est un concept essentiel qui explique pourquoi certains mod√®les ne g√©n√©ralisent pas bien. Imaginez apprendre pour un examen :\n- **Biais √©lev√©** = Vous survolez trop le cours (sous-apprentissage)\n- **Variance √©lev√©e** = Vous m√©morisez par c≈ìur sans comprendre (sur-apprentissage)\n\n## **Formules Math√©matiques Cl√©s**\n\n**Erreur totale du mod√®le :**\n$$E_{\\text{total}} = \\underbrace{\\text{Biais}^2}_{\\text{simplicit√©}} + \\underbrace{\\text{Variance}}_{\\text{complexit√©}} + \\epsilon$$\n\n**O√π :**\n- $\\text{Biais} = E[\\hat{f}(x)] - f(x)$ (diff√©rence entre pr√©diction moyenne et v√©rit√©)\n- $\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$ (variabilit√© des pr√©dictions)\n- $\\epsilon$ = Bruit irr√©ductible des donn√©es\n\n## **Exemple Illustratif avec Python**\n\n::: {#ee135e86 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](seance1_files/figure-html/cell-2-output-1.png){width=1430 height=757}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nüîç ANALYSE DU COMPROMIS BIAS-VARIANCE\n========================================\nDegr√© 1 (Underfitting) :\n  ‚Üí Biais¬≤ √©lev√© (2.93) - Mod√®le trop simple\n  ‚Üí Variance faible - Stable mais impr√©cis\n\nDegr√© 3 (Good Fit) :\n  ‚Üí Biais¬≤ mod√©r√© (2.33) - Capture bien les patterns\n  ‚Üí Variance mod√©r√©e - G√©n√©ralise correctement\n\nDegr√© 9 (Overfitting) :\n  ‚Üí Biais¬≤ faible (0.62) - Pr√©cision sur l'entra√Ænement\n  ‚Üí Variance √©lev√©e - M√©morise le bruit\n  ‚Üí √âcart Train/Test: 0.24\n```\n:::\n:::\n\n\n## **Diagramme du Compromis**\n\n```{mermaid}\ngraph TD\n    A[Erreur Totale] --> B[Biais¬≤<br>Erreur due √† la simplicit√©]\n    A --> C[Variance<br>Erreur due √† la complexit√©]\n    A --> D[Bruit irr√©ductible<br>Non contr√¥lable]\n    \n    E[Complexit√© ‚Üë] --> F{Biais}\n    E --> G{Variance}\n    \n    F -->|‚Üì| H[üìâ Baisse du Biais]\n    F -->|‚Üë| I[üìà Augmentation du Biais]\n    \n    G -->|‚Üë| J[üìà Hausse de la Variance]\n    G -->|‚Üì| K[üìâ Baisse de la Variance]\n    \n    L[üîç Recherche de l'√©quilibre] --> M[Zone optimale<br>Biais¬≤ ‚âà Variance]\n    M --> N[‚úÖ Mod√®le g√©n√©ralise bien]\n```\n\n## **Tableau Synth√®se Visuel**\n\n| Zone | Biais | Variance | Sympt√¥mes | Solution |\n|------|-------|----------|-----------|----------|\n| **Underfitting** | üìà Haut | üìâ Bas | Erreurs train/test √©lev√©es | ‚Üë Complexit√© du mod√®le |\n| **Good Fit** | ‚Üï Mod√©r√© | ‚Üï Mod√©r√© | G√©n√©ralisation bonne | ‚úÖ Maintenir |\n| **Overfitting** | üìâ Bas | üìà Haut | Train parfait, test m√©diocre | ‚Üì Complexit√©, r√©gularisation |\n\n## **Comment Trouver l'√âquilibre ?**\n\n1. **Commencez simple** (r√©gression lin√©aire comme baseline)\n2. **Augmentez progressivement** la complexit√©\n3. **Surveillez l'√©cart** entre performance d'entra√Ænement et de test\n4. **Arr√™tez quand** l'erreur de test commence √† augmenter\n\n**Formule √† retenir** :\n$$E_{\\text{test}} = \\text{Biais}^2 + \\text{Variance} + \\epsilon$$\n\n**Le succ√®s** = trouver le point o√π cette somme est minimale !\n\n> **R√®gle d'or** : Visez l'√©quilibre o√π votre mod√®le est assez complexe pour apprendre les patterns importants, mais assez simple pour ignorer le bruit al√©atoire.\n\nCette compr√©hension est cruciale pour choisir et ajuster vos mod√®les. L'objectif n'est pas d'√©liminer le biais ou la variance, mais de trouver l'√©quilibre optimal pour votre probl√®me sp√©cifique !\n\n#### Exercice Pratique : Diagnostic et Correction\n\n```python\nfrom sklearn.datasets import make_moons\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\n# Donn√©es non-lin√©aires\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Test de diff√©rents mod√®les\nmodels = {\n    'Arbre Profond (Variance)': DecisionTreeClassifier(max_depth=20),\n    'Arbre Simple (Biais)': DecisionTreeClassifier(max_depth=2),\n    'Arbre Optimis√©': DecisionTreeClassifier(max_depth=5, min_samples_split=10),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5),\n    'SVM Lin√©aire (Biais)': SVC(kernel='linear', C=1),\n    'SVM RBF (Variance)': SVC(kernel='rbf', C=10, gamma=10)\n}\n\nprint(\"üß™ TEST DU COMPROMIS BIAS-VARIANCE\")\nprint(\"=\" * 50)\n\nfor name, model in models.items():\n    diagnose_bias_variance(model, X_train, X_test, y_train, y_test)\n    print(\"-\" * 40)\n```\n\n#### Conclusion et Bonnes Pratiques\n\n**Checklist de Validation**\n- [ ] **Biais √©lev√© suspect√©** ‚Üí Essayer mod√®les plus complexes\n- [ ] **Variance √©lev√©e suspect√©e** ‚Üí Ajouter r√©gularisation\n- [ ] **Donn√©es limit√©es** ‚Üí Privil√©gier mod√®les simples\n- [ ] **Donn√©es abondantes** ‚Üí Mod√®les complexes possibles\n- [ ] **Toujours** utiliser validation crois√©e\n\n**R√®gles Empiriques**\n1. **Commencez simple** : Lin√©aire/logistique comme baseline\n2. **Augmentez progressivement** la complexit√©\n3. **Surveillez l'√©cart** entre train et validation\n4. **Utilisez l'ensemble de test** UNE SEULE FOIS √† la fin\n5. **Documentez** vos choix d'hyperparam√®tres\n\n**Formule √† Retenir**\n> **Mod√®le Id√©al = Biais¬≤ + Variance + Bruit**  \n> ‚Üí Minimiser la somme, pas individuellement\n\nLe compromis biais-variance n'est pas un probl√®me √† √©liminer mais un √©quilibre √† ma√Ætriser. La cl√© r√©side dans la compr√©hension des besoins de votre probl√®me sp√©cifique et l'ajustement continu de votre approche.\n\n## 6. Exercices de R√©flexion\n\n::: {.callout-warning icon=false}\n## Question 1\nPour chacun des probl√®mes suivants, identifiez le type d'apprentissage appropri√© (supervis√©, non supervis√©, renforcement):\n\na) Pr√©dire si un patient a une maladie cardiaque\nb) Regrouper des articles de presse par th√®me\nc) Apprendre √† un robot √† marcher\nd) Pr√©dire le prix d'une maison\ne) D√©tecter des transactions frauduleuses inhabituelles\n:::\n::: {.callout-note collapse=\"true\"}\n## R√©ponse 1\na) **Apprentissage supervis√©** (Classification) : On pr√©dit une √©tiquette binaire (malade ou non).\nb) **Apprentissage non supervis√©** (Clustering) : On regroupe des donn√©es sans √©tiquettes pr√©alables.\nc) **Apprentissage par renforcement** : Le robot apprend par essais et erreurs avec un syst√®me de r√©compenses.\nd) **Apprentissage supervis√©** (R√©gression) : On pr√©dit une valeur num√©rique continue.\ne) **Apprentissage non supervis√©** (D√©tection d'anomalies) : On cherche des comportements qui s'√©cartent de la norme.\n:::\n\n::: {.callout-warning icon=false}\n## Question 2\nExpliquez pourquoi un mod√®le avec 100% de pr√©cision sur les donn√©es d'entra√Ænement peut √™tre probl√©matique.\n:::\n::: {.callout-note collapse=\"true\"}\n## R√©ponse 2\nUne pr√©cision de 100 % sur les donn√©es d'entra√Ænement est souvent le signe d'un **surapprentissage (overfitting)**. Le mod√®le a \"m√©moris√©\" le bruit et les particularit√©s des donn√©es d'entra√Ænement au lieu d'apprendre les tendances g√©n√©rales. Par cons√©quent, il risque d'avoir de tr√®s mauvaises performances sur de nouvelles donn√©es (faible capacit√© de g√©n√©ralisation).\n:::\n\n::: {.callout-warning icon=false}\n## Question 3\nDonnez 3 exemples d'applications ML dans votre domaine d'int√©r√™t et identifiez le type de probl√®me (classification, r√©gression, clustering).\n:::\n::: {.callout-note collapse=\"true\"}\n## R√©ponse 3\n*Exemples dans le domaine du commerce √©lectronique :*\n\n1. **Syst√®me de recommandation de produits** : Identifier des groupes de clients aux comportements similaires (**Clustering**).\n2. **Pr√©vision de la demande (stocks)** : Pr√©dire le nombre d'unit√©s qui seront vendues le mois prochain (**R√©gression**).\n3. **Filtrage de commentaires abusifs** : Identifier si un avis client est conforme ou non aux r√®gles de la plateforme (**Classification**).\n:::\n\n## R√©sum√© de la S√©ance\n\n::: {.callout-important icon=false}\n## Points cl√©s √† retenir\n\n1. **ML** = apprentissage √† partir de donn√©es sans programmation explicite\n2. **Trois types principaux**: supervis√©, non supervis√©, renforcement\n3. **Pipeline ML**: Probl√®me ‚Üí Donn√©es ‚Üí Exploration ‚Üí Pr√©paration ‚Üí Mod√®le ‚Üí √âvaluation ‚Üí D√©ploiement\n4. **Overfitting vs Underfitting**: √©quilibre crucial pour la g√©n√©ralisation\n5. **Applications diverses**: vision, NLP, recommandations, finance, sant√©\n\n:::\n\n## Lectures Compl√©mentaires\n\n1. G√©ron, A. (2019) - Chapitre 1: The Machine Learning Landscape\n2. [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)\n3. [Andrew Ng - What is Machine Learning?](https://www.coursera.org/learn/machine-learning)\n\n",
    "supporting": [
      "seance1_files"
    ],
    "filters": [],
    "includes": {}
  }
}