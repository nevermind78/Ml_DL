{
  "hash": "88b006214e933662ae2acdf7327c6bd7",
  "result": {
    "engine": "jupyter",
    "markdown": "# S√©ance 9: Apprentissage Non Supervis√©\n\n::: {.callout-note icon=false}\n## Informations de la s√©ance\n- **Type**: Cours\n- **Dur√©e**: 2h\n- **Objectifs**: Obj8, Obj9\n:::\n\n## D√©finitions et Principes\n\nL'**apprentissage non supervis√©** est un type d'apprentissage o√π le mod√®le apprend √† partir de **donn√©es non √©tiquet√©es**, sans r√©ponses connues.\n\n**Objectif principal**: D√©couvrir des structures, des patterns ou des regroupements naturels dans les donn√©es.\n\n::: {.callout-tip}\n## Pourquoi l'apprentissage non supervis√© ?\n- Les donn√©es √©tiquet√©es sont rares ou co√ªteuses √† obtenir\n- Exploration de donn√©es inconnues\n- R√©duction de dimension pour visualisation\n- D√©tection d'anomalies\n:::\n\n## Clustering (Regroupement)\n\nLe **clustering** consiste √† regrouper des donn√©es similaires dans des clusters (groupes).\n\n### k-means\n\nL'algorithme **k-means** est l'une des m√©thodes de clustering les plus populaires.\n\n**Principe**:\n\n1. Choisir k points initiaux (centro√Ødes)\n2. Assigner chaque point au centro√Øde le plus proche\n3. Recalculer les centro√Ødes (moyenne des points du cluster)\n4. R√©p√©ter jusqu'√† convergence\n\n::: {#e4f6d0c8 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Donn√©es non √©tiquet√©es\nX = np.array([[1, 2], [1, 4], [1, 0],\n              [10, 2], [10, 4], [10, 0]])\n\n# Clustering avec k=2\nkmeans = KMeans(n_clusters=2, random_state=42)\nlabels = kmeans.fit_predict(X)\n\nprint(\"Labels des clusters:\", labels)\nprint(\"Centro√Ødes:\", kmeans.cluster_centers_)\n```\n:::\n\n\n**Avantages**:\n\n- Simple et rapide\n- √âvolutif pour grands datasets\n- R√©sultats faciles √† interpr√©ter\n\n**Inconv√©nients**:\n\n- N√©cessite de sp√©cifier k\n- Sensible aux valeurs aberrantes\n- Suppose des clusters sph√©riques et de taille similaire\n\n### DBSCAN (Density-Based Spatial Clustering)\n\n**DBSCAN** regroupe les points bas√©s sur la densit√©.\n\n**Param√®tres cl√©s**:\n\n- **eps**: distance maximale entre deux points pour √™tre consid√©r√©s voisins\n- **min_samples**: nombre minimum de points pour former un cluster dense\n\n::: {#018f3b65 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.cluster import DBSCAN\n\n# Clustering par densit√©\ndbscan = DBSCAN(eps=1.5, min_samples=2)\nlabels = dbscan.fit_predict(X)\n\nprint(\"Labels DBSCAN:\", labels)\n# -1 = bruit (outliers)\n```\n:::\n\n\n**Avantages**:\n\n- Pas besoin de sp√©cifier le nombre de clusters\n- D√©tecte les clusters de forme arbitraire\n- Robuste aux outliers\n\n**Inconv√©nients**:\n\n- Sensible aux param√®tres eps et min_samples\n- Difficult√© avec des densit√©s vari√©es\n\n### Autres m√©thodes\n\n- **Agglomerative Clustering**: approche hi√©rarchique\n- **Gaussian Mixture Models (GMM)**: mod√®le probabiliste\n- **Mean Shift**: bas√© sur la densit√© de noyau\n\n## Mesures de Qualit√©\n\nComment √©valuer la qualit√© d'un clustering sans labels vrais ?\n\n### Silhouette Score\n\nMesure de coh√©rence intra-cluster et s√©paration inter-cluster.\n\n**Valeurs**:\n\n- Proche de 1: bonne s√©paration\n- Proche de 0: clusters se chevauchent\n- N√©gatif: mauvais clustering\n\n::: {#08ab1926 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score\n\nscore = silhouette_score(X, labels)\nprint(f\"Silhouette Score: {score:.3f}\")\n```\n:::\n\n\n### Inertie (Elbow Method)\n\nSomme des distances carr√©es des points √† leur centro√Øde.\n\n::: {#81bc18f9 .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\ninertias = []\nK = range(1, 10)\n\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n\nplt.plot(K, inertias, 'bo-')\nplt.xlabel('Nombre de clusters (k)')\nplt.ylabel('Inertie')\nplt.title('M√©thode du coude (Elbow Method)')\nplt.grid(True)\nplt.show()\n```\n:::\n\n\n### Davies-Bouldin Index\n\nMesure de similarit√© moyenne entre clusters.\n\n## Applications R√©elles\n\n### Segmentation Client\n\n::: {#f351a707 .cell execution_count=5}\n``` {.python .cell-code}\n# Exemple fictif de segmentation client\nimport pandas as pd\n\ndata = {\n    'age': [25, 30, 35, 40, 45, 50, 55, 60],\n    'revenu_annuel_k': [40, 45, 50, 80, 90, 30, 35, 25],\n    'score_depense': [8, 7, 6, 9, 8, 3, 4, 2]\n}\n\ndf = pd.DataFrame(data)\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Normalisation\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df)\n\n# Clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ndf['cluster'] = kmeans.fit_predict(X_scaled)\n\nprint(df.groupby('cluster').mean())\n```\n:::\n\n\n### Regroupement de Documents\n\n- Groupement d'articles par th√®me\n- Organisation d'emails\n- Cat√©gorisation de produits\n\n### Analyse d'Images\n\n- Segmentation d'image\n- Regroupement de pixels similaires\n- Compression d'image\n\n## R√©duction de Dimension\n\n### Pourquoi r√©duire la dimension ?\n\n- Visualisation de donn√©es multidimensionnelles\n- R√©duction du bruit\n- Acc√©l√©ration des algorithmes\n- √âviter le \"fl√©au de la dimension\"\n\n### PCA (Principal Component Analysis)\n\n**PCA** transforme les donn√©es en composantes orthogonales capturant la variance maximale.\n\n::: {#5b9117b4 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\n# Donn√©es de d√©monstration\nnp.random.seed(42)\nX = np.random.randn(100, 5)  # 100 √©chantillons, 5 features\n\n# PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nprint(f\"Variance expliqu√©e: {pca.explained_variance_ratio_}\")\nprint(f\"Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\n\n# Visualisation\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('Premi√®re composante principale')\nplt.ylabel('Deuxi√®me composante principale')\nplt.title('PCA - Visualisation 2D')\nplt.show()\n```\n:::\n\n\n### t-SNE (t-Distributed Stochastic Neighbor Embedding)\n\nM√©thode non lin√©aire particuli√®rement efficace pour la visualisation.\n\n::: {#630680c9 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)\n\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nplt.xlabel('t-SNE 1')\nplt.ylabel('t-SNE 2')\nplt.title('t-SNE - Visualisation 2D')\nplt.show()\n```\n:::\n\n\n## Exercices de R√©flexion\n\n::: {.callout-warning icon=false}\n## Question 1\nPour chacun des sc√©narios suivants, proposez une m√©thode de clustering adapt√©e et justifiez votre choix :\n\na) Segmentation de clients avec des variables d√©mographiques et comportementales\nb) D√©tection de fraudes dans des transactions bancaires\nc) Regroupement de documents textuels\nd) Analyse de pixels d'une image satellite\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Correction Question 1\n\n**a) Segmentation de clients:**\n\n- **M√©thode recommand√©e**: **k-means**\n- **Justification**:\n  - Variables d√©mographiques et comportementales ‚Üí donn√©es num√©riques\n  - Nombre de segments g√©n√©ralement connu √† l'avance (ex: 3-5 segments)\n  - Besoin d'interpr√©tabilit√© pour le marketing\n  - Rapide et efficace sur grands volumes de clients\n\n::: {#8789c65a .cell execution_count=8}\n``` {.python .cell-code}\n# Exemple de segmentation client\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Pr√©paration\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(client_features)\n\n# K-means avec 4 segments\nkmeans = KMeans(n_clusters=4, random_state=42)\nsegments = kmeans.fit_predict(X_scaled)\n\n# Profilage des segments\nprofiles = pd.DataFrame(X_scaled, columns=feature_names)\nprofiles['segment'] = segments\nprint(profiles.groupby('segment').mean())\n```\n:::\n\n\n**b) D√©tection de fraudes:**\n\n- **M√©thode recommand√©e**: **DBSCAN** ou **Isolation Forest**\n- **Justification**:\n\n  - Fraudes = anomalies (outliers)\n  - DBSCAN identifie les points de bruit (label -1)\n  - Pas besoin de conna√Ætre le nombre de types de fraude\n  - D√©tecte des patterns de fraude de formes vari√©es\n\n::: {#08aff0f6 .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN pour d√©tecter les anomalies\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(transactions_features)\n\n# Points anormaux (potentielles fraudes)\nanomalies = transactions_features[labels == -1]\nprint(f\"Nombre de transactions suspectes: {len(anomalies)}\")\n```\n:::\n\n\n**c) Regroupement de documents textuels:**\n\n- **M√©thode recommand√©e**: **k-means** sur TF-IDF + **Hierarchical Clustering**\n- **Justification**:\n\n  - TF-IDF transforme texte en vecteurs num√©riques\n  - K-means efficace en haute dimension (nombreux mots)\n  - Hierarchical permet d'explorer la hi√©rarchie des th√®mes\n  - Peut combiner avec topic modeling (LDA)\n\n::: {#c8589599 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n# Vectorisation des textes\nvectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\nX_tfidf = vectorizer.fit_transform(documents)\n\n# Clustering\nkmeans = KMeans(n_clusters=5, random_state=42)\ndoc_clusters = kmeans.fit_predict(X_tfidf)\n\n# Top mots par cluster\nterms = vectorizer.get_feature_names_out()\nfor i in range(5):\n    center = kmeans.cluster_centers_[i]\n    top_terms = [terms[j] for j in center.argsort()[-10:]]\n    print(f\"Cluster {i}: {', '.join(top_terms)}\")\n```\n:::\n\n\n**d) Analyse de pixels d'image satellite:**\n\n- **M√©thode recommand√©e**: **k-means** ou **Mean Shift**\n- **Justification**:\n\n  - Segmentation d'image = clustering de pixels (RGB ou multi-spectral)\n  - K-means rapide pour millions de pixels\n  - Mean Shift d√©tecte automatiquement le nombre de segments\n  - Peut identifier zones (for√™t, eau, ville, etc.)\n\n::: {#b8f760d2 .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Image satellite (exemple)\n# image shape: (height, width, channels)\npixels = image.reshape(-1, image.shape[2])  # Reshape en (n_pixels, channels)\n\n# K-means sur pixels\nkmeans = KMeans(n_clusters=5, random_state=42)\nlabels = kmeans.fit_predict(pixels)\n\n# Reconstruction de l'image segment√©e\nsegmented_image = labels.reshape(image.shape[:2])\nplt.imshow(segmented_image, cmap='tab10')\nplt.title('Segmentation de l\\'image satellite')\nplt.show()\n```\n:::\n\n\n:::\n\n::: {.callout-warning icon=false}\n## Question 2\nPour un dataset avec 10 000 √©chantillons et 50 features :\n\na) Expliquez comment d√©terminer le nombre optimal de clusters pour k-means\nb) Proposez une approche pour visualiser la structure des clusters\nc) Quel avantage PCA peut-il apporter avant le clustering ?\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Correction Question 2\n\n**a) D√©terminer le nombre optimal de clusters:**\n\n**M√©thode 1: Elbow Method (M√©thode du coude)**\n\n::: {#72e1b3f9 .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Tester diff√©rentes valeurs de k\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    \n    inertias.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n\n# Visualisation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Courbe du coude\nax1.plot(K_range, inertias, 'bo-')\nax1.set_xlabel('Nombre de clusters (k)')\nax1.set_ylabel('Inertie')\nax1.set_title('M√©thode du Coude')\nax1.grid(True)\n\n# Silhouette score\nax2.plot(K_range, silhouette_scores, 'go-')\nax2.set_xlabel('Nombre de clusters (k)')\nax2.set_ylabel('Silhouette Score')\nax2.set_title('Score Silhouette')\nax2.grid(True)\n\nplt.show()\n\n# Le k optimal est au \"coude\" de la courbe d'inertie\n# ET avec un bon silhouette score\n```\n:::\n\n\n**M√©thode 2: Gap Statistic**\n\n::: {#4151745c .cell execution_count=13}\n``` {.python .cell-code}\n# Compare l'inertie observ√©e vs inertie sur donn√©es al√©atoires\ndef gap_statistic(X, k_max=10, n_refs=10):\n    gaps = []\n    for k in range(1, k_max + 1):\n        # Inertie sur donn√©es r√©elles\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(X)\n        real_inertia = kmeans.inertia_\n        \n        # Inertie moyenne sur donn√©es de r√©f√©rence\n        ref_inertias = []\n        for _ in range(n_refs):\n            X_ref = np.random.uniform(X.min(), X.max(), X.shape)\n            kmeans_ref = KMeans(n_clusters=k, random_state=42)\n            kmeans_ref.fit(X_ref)\n            ref_inertias.append(kmeans_ref.inertia_)\n        \n        gap = np.log(np.mean(ref_inertias)) - np.log(real_inertia)\n        gaps.append(gap)\n    \n    return gaps\n\n# K optimal = premier k o√π gap commence √† d√©cro√Ætre\n```\n:::\n\n\n**M√©thode 3: Silhouette Analysis d√©taill√©e**\n\n::: {#0aed15f6 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_samples\nimport matplotlib.cm as cm\n\nfor k in [2, 3, 4, 5]:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X)\n    \n    silhouette_vals = silhouette_samples(X, labels)\n    \n    plt.figure(figsize=(10, 6))\n    y_lower = 10\n    \n    for i in range(k):\n        cluster_silhouette_vals = silhouette_vals[labels == i]\n        cluster_silhouette_vals.sort()\n        \n        size = cluster_silhouette_vals.shape[0]\n        y_upper = y_lower + size\n        \n        plt.fill_betweenx(np.arange(y_lower, y_upper),\n                         0, cluster_silhouette_vals,\n                         alpha=0.7)\n        y_lower = y_upper + 10\n    \n    plt.title(f'Silhouette Plot (k={k})')\n    plt.xlabel('Coefficient Silhouette')\n    plt.ylabel('Cluster')\n    plt.axvline(x=silhouette_score(X, labels), color=\"red\", linestyle=\"--\")\n    plt.show()\n```\n:::\n\n\n**b) Visualiser la structure des clusters:**\n\n**Approche 1: PCA pour r√©duction 2D/3D**\n\n::: {#a5843d06 .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\n# R√©duction √† 2D\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                     c=labels, cmap='viridis', alpha=0.6)\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\nplt.title('Clusters visualis√©s avec PCA')\nplt.colorbar(scatter, label='Cluster')\nplt.show()\n\nprint(f\"Variance expliqu√©e totale: {sum(pca.explained_variance_ratio_):.2%}\")\n```\n:::\n\n\n**Approche 2: t-SNE pour visualisation non-lin√©aire**\n\n::: {#d47a438e .cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\n\n# t-SNE (plus lent mais meilleure visualisation)\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_tsne = tsne.fit_transform(X)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)\nplt.title('Clusters visualis√©s avec t-SNE')\nplt.colorbar(label='Cluster')\nplt.show()\n```\n:::\n\n\n**Approche 3: Pairplot des features importantes**\n\n::: {#2d2440e1 .cell execution_count=17}\n``` {.python .cell-code}\nimport seaborn as sns\n\n# S√©lectionner top features par variance\nfrom sklearn.feature_selection import VarianceThreshold\n\nselector = VarianceThreshold(threshold=0.5)\nX_selected = selector.fit_transform(X)\n\n# Pairplot avec 4-5 features les plus variables\ndf_plot = pd.DataFrame(X_selected[:, :5], columns=[f'F{i}' for i in range(5)])\ndf_plot['cluster'] = labels\n\nsns.pairplot(df_plot, hue='cluster', palette='tab10')\nplt.show()\n```\n:::\n\n\n**c) Avantages de PCA avant le clustering:**\n\n**1. R√©duction de dimension ‚Üí Efficacit√© computationnelle**\n\n::: {#74f40095 .cell execution_count=18}\n``` {.python .cell-code}\n# Sans PCA: 50 features\nimport time\n\nstart = time.time()\nkmeans_full = KMeans(n_clusters=5, random_state=42)\nkmeans_full.fit(X)  # X: (10000, 50)\ntime_full = time.time() - start\n\n# Avec PCA: 10 features (gardant 95% de variance)\npca = PCA(n_components=0.95)  # Garde 95% de variance\nX_pca = pca.fit_transform(X)  # X_pca: (10000, ~10)\n\nstart = time.time()\nkmeans_pca = KMeans(n_clusters=5, random_state=42)\nkmeans_pca.fit(X_pca)\ntime_pca = time.time() - start\n\nprint(f\"Temps sans PCA: {time_full:.2f}s\")\nprint(f\"Temps avec PCA: {time_pca:.2f}s\")\nprint(f\"Acc√©l√©ration: {time_full/time_pca:.1f}x\")\nprint(f\"Dimensions r√©duites: {X.shape[1]} ‚Üí {X_pca.shape[1]}\")\n```\n:::\n\n\n**2. R√©duction du bruit**\n\n::: {#0dd9e0ec .cell execution_count=19}\n``` {.python .cell-code}\n# PCA √©limine les composantes de faible variance (souvent du bruit)\npca_full = PCA()\npca_full.fit(X)\n\n# Afficher la variance par composante\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         pca_full.explained_variance_ratio_, 'bo-')\nplt.xlabel('Composante')\nplt.ylabel('Variance expliqu√©e')\nplt.title('Scree Plot')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         np.cumsum(pca_full.explained_variance_ratio_), 'ro-')\nplt.xlabel('Nombre de composantes')\nplt.ylabel('Variance cumul√©e')\nplt.axhline(y=0.95, color='g', linestyle='--', label='95%')\nplt.title('Variance Cumul√©e')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Garder les composantes qui expliquent 95% de la variance\n# ‚Üí √©limine le bruit des derni√®res composantes\n```\n:::\n\n\n**3. √âvite la mal√©diction de la dimensionnalit√©**\n\n::: {#6c41d671 .cell execution_count=20}\n``` {.python .cell-code}\n# En haute dimension, les distances deviennent moins significatives\nfrom scipy.spatial.distance import pdist, squareform\n\n# Calcul des distances moyennes\ndistances_full = pdist(X[:100])  # Sur 100 √©chantillons pour rapidit√©\ndistances_pca = pdist(X_pca[:100])\n\nprint(f\"Distance moyenne (50D): {np.mean(distances_full):.2f}\")\nprint(f\"Distance moyenne (10D): {np.mean(distances_pca):.2f}\")\nprint(f\"√âcart-type distances (50D): {np.std(distances_full):.2f}\")\nprint(f\"√âcart-type distances (10D): {np.std(distances_pca):.2f}\")\n\n# En dimension r√©duite, les distances sont plus discriminantes\n```\n:::\n\n\n**4. D√©corr√©lation des features**\n\n::: {#1a925b36 .cell execution_count=21}\n``` {.python .cell-code}\n# PCA produit des composantes non-corr√©l√©es\n# ‚Üí Am√©liore k-means qui suppose ind√©pendance\n\n# Corr√©lation avant PCA\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.heatmap(np.corrcoef(X.T), cmap='coolwarm', center=0,\n            cbar_kws={'label': 'Corr√©lation'})\nplt.title('Corr√©lations avant PCA')\n\n# Corr√©lation apr√®s PCA\nplt.subplot(1, 2, 2)\nsns.heatmap(np.corrcoef(X_pca.T), cmap='coolwarm', center=0,\n            cbar_kws={'label': 'Corr√©lation'})\nplt.title('Corr√©lations apr√®s PCA')\n\nplt.tight_layout()\nplt.show()\n\n# Apr√®s PCA: corr√©lations nulles entre composantes\n```\n:::\n\n\n**R√©sum√© des avantages:**\n\n| Avantage | Description | Impact |\n|----------|-------------|--------|\n| **Efficacit√©** | 50 ‚Üí 10 dimensions | 5-10x plus rapide |\n| **D√©bruitage** | √âlimine variance faible | Clusters plus nets |\n| **Distances** | Plus discriminantes en faible dim | Meilleur clustering |\n| **D√©corr√©lation** | Features ind√©pendantes | K-means plus efficace |\n| **Visualisation** | R√©duction √† 2-3D | Interpr√©tation facile |\n:::\n\n::: {.callout-warning icon=false}\n## Question 3\nImpl√©mentez un pipeline complet de clustering sur le dataset Iris :\n\n1. Chargez les donn√©es (ignorer les labels pour l'apprentissage non supervis√©)\n2. Appliquez PCA pour r√©duire √† 2 dimensions\n3. Testez k-means avec k=2,3,4 et comparez les r√©sultats\n4. Visualisez les clusters obtenus\n5. Calculez le silhouette score pour chaque k\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Correction Question 3\n\n::: {#fbc20fa4 .cell execution_count=22}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, adjusted_rand_score\n\n# 1. Chargement des donn√©es (SANS utiliser les labels pour clustering)\nprint(\"=\" * 70)\nprint(\"PIPELINE COMPLET DE CLUSTERING - DATASET IRIS\")\nprint(\"=\" * 70)\n\niris = datasets.load_iris()\nX = iris.data  # Features seulement (ignorer iris.target)\nfeature_names = iris.feature_names\ntrue_labels = iris.target  # Gard√© seulement pour √©valuation finale\n\nprint(f\"\\n1. Chargement des donn√©es:\")\nprint(f\"   Dimensions: {X.shape}\")\nprint(f\"   Features: {feature_names}\")\n\n# Normalisation (important avant PCA)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nprint(f\"   ‚úì Donn√©es normalis√©es\")\n\n# 2. Application de PCA pour r√©duction √† 2D\nprint(f\"\\n2. R√©duction de dimension avec PCA:\")\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"   Variance expliqu√©e par composante: {pca.explained_variance_ratio_}\")\nprint(f\"   Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\nprint(f\"   Dimensions: {X.shape[1]}D ‚Üí {X_pca.shape[1]}D\")\n\n# Visualisation des donn√©es apr√®s PCA (sans clustering)\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                     c=true_labels, cmap='viridis', \n                     alpha=0.6, edgecolors='w')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\nplt.title('Dataset Iris apr√®s PCA (color√© par vraies classes)')\nplt.colorbar(scatter, label='Vraie classe')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 3. Test de k-means avec k=2,3,4\nprint(f\"\\n3. Clustering k-means avec diff√©rentes valeurs de k:\")\nprint(\"-\" * 70)\n\nK_values = [2, 3, 4]\nresults = []\n\nfor k in K_values:\n    # Clustering\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_pca)\n    \n    # M√©triques\n    inertia = kmeans.inertia_\n    silhouette = silhouette_score(X_pca, labels)\n    \n    # Comparaison avec vraies classes (juste pour curiosit√©)\n    ari = adjusted_rand_score(true_labels, labels)\n    \n    results.append({\n        'k': k,\n        'Inertie': inertia,\n        'Silhouette': silhouette,\n        'ARI (vs vrai)': ari,\n        'labels': labels,\n        'centroids': kmeans.cluster_centers_\n    })\n    \n    print(f\"\\nk = {k}:\")\n    print(f\"   Inertie: {inertia:.2f}\")\n    print(f\"   Silhouette Score: {silhouette:.3f}\")\n    print(f\"   Taille des clusters: {np.bincount(labels)}\")\n    print(f\"   ARI (comparaison avec vraies classes): {ari:.3f}\")\n\n# DataFrame des r√©sultats\ndf_results = pd.DataFrame([{k: v for k, v in r.items() if k not in ['labels', 'centroids']} \n                          for r in results])\nprint(f\"\\nüìä Tableau r√©capitulatif:\")\nprint(df_results.to_string(index=False))\n\n# Meilleur k selon silhouette\nbest_k = df_results.loc[df_results['Silhouette'].idxmax(), 'k']\nprint(f\"\\n‚≠ê Meilleur k selon Silhouette Score: {best_k}\")\n\n# 4. Visualisation des clusters pour chaque k\nprint(f\"\\n4. Visualisation des clusters:\")\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, result in enumerate(results):\n    k = result['k']\n    labels = result['labels']\n    centroids = result['centroids']\n    ax = axes[idx]\n    # Scatter plot des points\n    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], \n                        c=labels, cmap='tab10', \n                        alpha=0.6, edgecolors='w', s=50)\n    \n    # Centro√Ødes\n    ax.scatter(centroids[:, 0], centroids[:, 1], \n              c='red', marker='X', s=200, \n              edgecolors='black', linewidths=2,\n              label='Centro√Ødes')\n    \n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n    ax.set_title(f'k={k} (Silhouette={result[\"Silhouette\"]:.3f})')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # Colorbar\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n\nplt.tight_layout()\nplt.show()\n\n# 5. Analyse approfondie du meilleur k\nprint(f\"\\n5. Analyse d√©taill√©e pour k={best_k}:\")\nprint(\"-\" * 70)\n\nbest_result = [r for r in results if r['k'] == best_k][0]\nbest_labels = best_result['labels']\n\n# Profilage des clusters\nprint(f\"\\nProfilage des clusters (features originales):\")\n\ndf_analysis = pd.DataFrame(X, columns=feature_names)\ndf_analysis['Cluster'] = best_labels\n\ncluster_profiles = df_analysis.groupby('Cluster').agg(['mean', 'std'])\nprint(cluster_profiles)\n\n# Heatmap des caract√©ristiques par cluster\ncluster_means = df_analysis.groupby('Cluster').mean()\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(cluster_means.T, annot=True, fmt='.2f', cmap='YlOrRd',\n            cbar_kws={'label': 'Valeur moyenne'})\nplt.xlabel('Cluster')\nplt.ylabel('Feature')\nplt.title(f'Profil des {best_k} clusters (valeurs moyennes)')\nplt.tight_layout()\nplt.show()\n\n# Comparaison avec les vraies classes (curiosit√© acad√©mique)\nprint(f\"\\nüìà Comparaison avec les vraies esp√®ces d'Iris:\")\nprint(\"(Note: Le clustering est NON SUPERVIS√â, cette comparaison est\")\nprint(\" juste pour comprendre ce que l'algorithme a trouv√©)\")\n\nconfusion_unsupervised = pd.crosstab(\n    pd.Series(true_labels, name='Vraie esp√®ce'),\n    pd.Series(best_labels, name='Cluster trouv√©')\n)\nprint(confusion_unsupervised)\n\n# Conclusion\nprint(f\"\\n\" + \"=\" * 70)\nprint(\"CONCLUSION\")\nprint(\"=\" * 70)\nprint(f\"‚úì PCA a r√©duit les donn√©es de 4D √† 2D\")\nprint(f\"‚úì {sum(pca.explained_variance_ratio_):.1%} de variance pr√©serv√©e\")\nprint(f\"‚úì K optimal selon silhouette: {best_k}\")\nprint(f\"‚úì Silhouette score: {best_result['Silhouette']:.3f}\")\nprint(f\"‚úì Les clusters correspondent {'assez bien' if best_result['ARI (vs vrai)'] > 0.7 else 'partiellement'} aux vraies esp√®ces\")\nprint(f\"  (ARI = {best_result['ARI (vs vrai)']:.3f})\")\n```\n:::\n\n\n**R√©sultat attendu:**\n\nLe pipeline devrait r√©v√©ler que:\n\n- **k=3** est optimal (correspond aux 3 esp√®ces d'Iris)\n- Le silhouette score sera autour de 0.5-0.6\n- PCA capture environ 95% de la variance en 2D\n- Les clusters trouv√©s correspondent assez bien aux vraies esp√®ces\n- Une esp√®ce (Setosa) sera bien s√©par√©e, les deux autres se chevaucheront un peu\n:::\n\n## R√©sum√© de la S√©ance\n\n::: {.callout-important icon=false}\n## Points cl√©s √† retenir\n\n1. **Apprentissage non supervis√©** = d√©couvrir des patterns dans des donn√©es non √©tiquet√©es\n2. **Clustering** = regrouper des donn√©es similaires (k-means, DBSCAN, hi√©rarchique)\n3. **Mesures de qualit√©** : silhouette score, inertie, Davies-Bouldin\n4. **R√©duction de dimension** : PCA (lin√©aire), t-SNE (non-lin√©aire)\n5. **Applications** : segmentation client, analyse de documents, traitement d'image\n6. **D√©fis** : choix du nombre de clusters, qualit√© sans v√©rit√© terrain\n:::\n\n## Lectures Compl√©mentaires\n\n1. G√©ron, A. (2019) - Chapitre 9: Unsupervised Learning Techniques\n2. [Scikit-learn Clustering Documentation](https://scikit-learn.org/stable/modules/clustering.html)\n3. [Visualizing Data using t-SNE](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)</parameter>\n\n",
    "supporting": [
      "seance9_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}