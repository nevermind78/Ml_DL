{
  "hash": "46873410e17e6601f69a566937d887d9",
  "result": {
    "engine": "jupyter",
    "markdown": "# S√©ance 10: TP4 - Clustering & R√©duction de Dimension\n\n::: {.callout-note icon=false}\n## Informations de la s√©ance\n- **Type**: TP\n- **Dur√©e**: 2h\n- **Objectifs**: Obj9, Obj10\n:::\n\n## 1. Objectifs du TP\n\nDans ce TP, vous allez :\n\n1. Appliquer diff√©rentes m√©thodes de clustering sur des datasets r√©els\n2. Utiliser des techniques pour d√©terminer le nombre optimal de clusters\n3. Visualiser et interpr√©ter les r√©sultats de clustering\n4. Utiliser PCA pour am√©liorer l'analyse et la visualisation\n5. Interpr√©ter les r√©sultats dans un contexte m√©tier\n\n## 2. Pr√©paration de l'Environnement\n\n::: {#5cbab407 .cell execution_count=1}\n``` {.python .cell-code}\n# Importations n√©cessaires\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\nfrom sklearn.manifold import TSNE\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Configuration des graphiques\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"‚úÖ Environnement pr√™t!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ Environnement pr√™t!\n```\n:::\n:::\n\n\n## 3. Dataset 1: Mall Customers Segmentation\n\n### 3.1 Chargement et Exploration\n\n::: {#70635dde .cell execution_count=2}\n``` {.python .cell-code}\n# Dataset: Caract√©ristiques de clients d'un centre commercial\n# Source: https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python\n\n# Cr√©ation d'un dataset synth√©tique pour l'exemple\nnp.random.seed(42)\nn_samples = 300\n\n# G√©n√©ration de donn√©es\nage = np.random.normal(35, 10, n_samples).clip(18, 70)\nannual_income = np.random.normal(60, 20, n_samples).clip(15, 140)\nspending_score = np.random.normal(50, 25, n_samples).clip(1, 100)\n\n# Cr√©ation de clusters artificiels\n# Cluster 1: Jeunes d√©pensiers\nmask1 = (age < 30) & (spending_score > 60)\nannual_income[mask1] = np.random.normal(40, 5, mask1.sum()).clip(30, 50)\n\n# Cluster 2: Seniors √©conomes\nmask2 = (age > 50) & (spending_score < 40)\nannual_income[mask2] = np.random.normal(70, 10, mask2.sum()).clip(60, 90)\n\n# DataFrame\nmall_data = pd.DataFrame({\n    'Age': age,\n    'Annual_Income_k': annual_income,\n    'Spending_Score': spending_score\n})\n\n# Affichage des premi√®res lignes\nprint(\"üìä Dataset Mall Customers:\")\nprint(f\"Dimensions: {mall_data.shape}\")\nprint(\"\\nPremi√®res lignes:\")\nprint(mall_data.head())\nprint(\"\\nStatistiques descriptives:\")\nprint(mall_data.describe())\n\n# Visualisation 3D\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(mall_data['Age'], \n                     mall_data['Annual_Income_k'], \n                     mall_data['Spending_Score'],\n                     c='blue', alpha=0.6, edgecolors='w', s=50)\nax.set_xlabel('Age')\nax.set_ylabel('Revenu Annuel (k$)')\nax.set_zlabel('Score de D√©pense')\nax.set_title('Distribution des Clients - 3D')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüìä Dataset Mall Customers:\nDimensions: (300, 3)\n\nPremi√®res lignes:\n         Age  Annual_Income_k  Spending_Score\n0  39.967142        43.420100       68.924715\n1  33.617357        48.796379       26.945867\n2  41.476885        74.945872       71.740148\n3  50.230299        72.207405       83.890946\n4  32.658466        59.581968       60.335873\n\nStatistiques descriptives:\n              Age  Annual_Income_k  Spending_Score\ncount  300.000000       300.000000      300.000000\nmean    35.075182        57.599007       51.901542\nstd      9.482022        19.021360       23.928198\nmin     18.000000        15.000000        1.000000\n25%     28.167541        43.164043       35.713119\n50%     35.592195        57.575426       51.068513\n75%     41.266577        70.819138       67.786740\nmax     70.000000       121.577616      100.000000\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-3-output-2.png){}\n:::\n:::\n\n\n### 3.2 Pr√©traitement des Donn√©es\n\n::: {#ed58269d .cell execution_count=3}\n``` {.python .cell-code}\n# Normalisation des donn√©es\nscaler = StandardScaler()\nmall_scaled = scaler.fit_transform(mall_data)\n\nprint(\"‚úÖ Donn√©es normalis√©es (moyenne=0, √©cart-type=1)\")\nprint(f\"Moyennes apr√®s normalisation: {mall_scaled.mean(axis=0).round(2)}\")\nprint(f\"√âcarts-types apr√®s normalisation: {mall_scaled.std(axis=0).round(2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚úÖ Donn√©es normalis√©es (moyenne=0, √©cart-type=1)\nMoyennes apr√®s normalisation: [ 0. -0. -0.]\n√âcarts-types apr√®s normalisation: [1. 1. 1.]\n```\n:::\n:::\n\n\n### 3.3 D√©termination du Nombre Optimal de Clusters\n\n::: {#998c5028 .cell execution_count=4}\n``` {.python .cell-code}\n# M√©thode du coude (Elbow Method)\ninertias = []\nsilhouette_scores = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(mall_scaled)\n    inertias.append(kmeans.inertia_)\n    \n    if k > 1:  # silhouette_score n√©cessite au moins 2 clusters\n        score = silhouette_score(mall_scaled, kmeans.labels_)\n        silhouette_scores.append(score)\n\n# Graphique Elbow Method\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Courbe d'inertie\nax1.plot(K_range, inertias, 'bo-')\nax1.set_xlabel('Nombre de clusters (k)')\nax1.set_ylabel('Inertie')\nax1.set_title('M√©thode du Coude')\nax1.grid(True)\n\n# Score silhouette\nax2.plot(range(2, 11), silhouette_scores, 'go-')\nax2.set_xlabel('Nombre de clusters (k)')\nax2.set_ylabel('Score Silhouette')\nax2.set_title('Score Silhouette par k')\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìà Analyse:\")\nprint(f\"Inertie pour k=3: {inertias[1]:.2f}\")\nprint(f\"Inertie pour k=4: {inertias[2]:.2f}\")\nprint(f\"Silhouette pour k=3: {silhouette_scores[1]:.3f}\")\nprint(f\"Silhouette pour k=4: {silhouette_scores[2]:.3f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-5-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nüìà Analyse:\nInertie pour k=3: 523.08\nInertie pour k=4: 422.27\nSilhouette pour k=3: 0.238\nSilhouette pour k=4: 0.256\n```\n:::\n:::\n\n\n### 3.4 Application de k-means\n\n::: {#5f05a834 .cell execution_count=5}\n``` {.python .cell-code}\n# Clustering avec k=3 (choisi d'apr√®s l'analyse)\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nmall_data['Cluster_kmeans'] = kmeans.fit_predict(mall_scaled)\n\n# Affichage des r√©sultats\nprint(\"üéØ R√©sultats du clustering k-means (k=3):\")\nprint(f\"Taille des clusters: {np.bincount(mall_data['Cluster_kmeans'])}\")\n\n# Caract√©ristiques par cluster\ncluster_stats = mall_data.groupby('Cluster_kmeans').agg({\n    'Age': ['mean', 'std', 'count'],\n    'Annual_Income_k': ['mean', 'std'],\n    'Spending_Score': ['mean', 'std']\n}).round(2)\n\nprint(\"\\nüìä Statistiques par cluster:\")\nprint(cluster_stats)\n\n# Interpr√©tation m√©tier\nprint(\"\\nüí° Interpr√©tation m√©tier:\")\nprint(\"Cluster 0: Clients moyens (√¢ge et revenu moyens, d√©penses moyennes)\")\nprint(\"Cluster 1: Jeunes d√©pensiers (√¢ge jeune, revenu mod√©r√©, d√©penses √©lev√©es)\")\nprint(\"Cluster 2: Seniors √©conomes (√¢ge √©lev√©, revenu √©lev√©, d√©penses faibles)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüéØ R√©sultats du clustering k-means (k=3):\nTaille des clusters: [100 101  99]\n\nüìä Statistiques par cluster:\n                  Age             Annual_Income_k        Spending_Score       \n                 mean   std count            mean    std           mean    std\nCluster_kmeans                                                                \n0               42.03  8.39   100           74.40  14.19          47.83  19.62\n1               29.68  7.12   101           54.36  16.21          33.41  14.86\n2               33.55  8.35    99           43.93  12.03          74.88  15.18\n\nüí° Interpr√©tation m√©tier:\nCluster 0: Clients moyens (√¢ge et revenu moyens, d√©penses moyennes)\nCluster 1: Jeunes d√©pensiers (√¢ge jeune, revenu mod√©r√©, d√©penses √©lev√©es)\nCluster 2: Seniors √©conomes (√¢ge √©lev√©, revenu √©lev√©, d√©penses faibles)\n```\n:::\n:::\n\n\n### 3.5 Visualisation avec PCA\n\n::: {#fc974d17 .cell execution_count=6}\n``` {.python .cell-code}\n# R√©duction √† 2D avec PCA pour visualisation\npca = PCA(n_components=2)\nmall_pca = pca.fit_transform(mall_scaled)\n\n# Ajout des composantes principales au DataFrame\nmall_data['PCA1'] = mall_pca[:, 0]\nmall_data['PCA2'] = mall_pca[:, 1]\n\nprint(f\"üìâ Variance expliqu√©e par PCA: {pca.explained_variance_ratio_.round(3)}\")\nprint(f\"üìä Variance totale expliqu√©e: {sum(pca.explained_variance_ratio_):.2%}\")\n\n# Visualisation des clusters avec PCA\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nscatter = plt.scatter(mall_data['PCA1'], mall_data['PCA2'], \n                     c=mall_data['Cluster_kmeans'], cmap='viridis', \n                     alpha=0.7, s=50)\nplt.xlabel(f'Premi√®re Composante Principale ({pca.explained_variance_ratio_[0]:.1%})')\nplt.ylabel(f'Deuxi√®me Composante Principale ({pca.explained_variance_ratio_[1]:.1%})')\nplt.title('Clusters k-means visualis√©s avec PCA')\nplt.colorbar(scatter, label='Cluster')\nplt.grid(True, alpha=0.3)\n\n# Visualisation t-SNE (comparaison)\nplt.subplot(1, 2, 2)\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nmall_tsne = tsne.fit_transform(mall_scaled)\n\nplt.scatter(mall_tsne[:, 0], mall_tsne[:, 1], \n           c=mall_data['Cluster_kmeans'], cmap='viridis', \n           alpha=0.7, s=50)\nplt.xlabel('t-SNE 1')\nplt.ylabel('t-SNE 2')\nplt.title('Clusters k-means visualis√©s avec t-SNE')\nplt.colorbar(scatter, label='Cluster')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüìâ Variance expliqu√©e par PCA: [0.416 0.323]\nüìä Variance totale expliqu√©e: 73.93%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\n## 4. Dataset 2: Clustering Hi√©rarchique sur Donn√©es de Fleurs\n\n### 4.1 Chargement et Exploration\n\n::: {#cb889f9d .cell execution_count=7}\n``` {.python .cell-code}\n# Chargement du dataset Iris (sans utiliser les labels pour l'apprentissage non supervis√©)\niris = datasets.load_iris()\niris_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n\nprint(\"üå∏ Dataset Iris (sans les labels):\")\nprint(f\"Dimensions: {iris_data.shape}\")\nprint(\"\\nDescription:\")\nprint(iris_data.describe())\n\n# Matrice de corr√©lation\nplt.figure(figsize=(8, 6))\nsns.heatmap(iris_data.corr(), annot=True, cmap='coolwarm', center=0)\nplt.title('Matrice de Corr√©lation - Dataset Iris')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüå∏ Dataset Iris (sans les labels):\nDimensions: (150, 4)\n\nDescription:\n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount         150.000000        150.000000         150.000000   \nmean            5.843333          3.057333           3.758000   \nstd             0.828066          0.435866           1.765298   \nmin             4.300000          2.000000           1.000000   \n25%             5.100000          2.800000           1.600000   \n50%             5.800000          3.000000           4.350000   \n75%             6.400000          3.300000           5.100000   \nmax             7.900000          4.400000           6.900000   \n\n       petal width (cm)  \ncount        150.000000  \nmean           1.199333  \nstd            0.762238  \nmin            0.100000  \n25%            0.300000  \n50%            1.300000  \n75%            1.800000  \nmax            2.500000  \n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-8-output-2.png){}\n:::\n:::\n\n\n### 4.2 Clustering Hi√©rarchique\n\n::: {#8ee65c87 .cell execution_count=8}\n``` {.python .cell-code}\n# Normalisation\niris_scaled = StandardScaler().fit_transform(iris_data)\n\n# Clustering hi√©rarchique agglom√©ratif\nagg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\niris_labels = agg_clustering.fit_predict(iris_scaled)\n\n# Dendrogramme\nplt.figure(figsize=(12, 5))\n\n# Sous-√©chantillon pour le dendrogramme (pour lisibilit√©)\nlinkage_matrix = linkage(iris_scaled[:50], method='ward')\n\nplt.subplot(1, 2, 1)\ndendrogram(linkage_matrix, truncate_mode='level', p=5)\nplt.xlabel('Indices des √©chantillons')\nplt.ylabel('Distance')\nplt.title('Dendrogramme - Clustering Hi√©rarchique')\nplt.grid(True, alpha=0.3)\n\n# Visualisation avec PCA\nplt.subplot(1, 2, 2)\niris_pca = PCA(n_components=2).fit_transform(iris_scaled)\nplt.scatter(iris_pca[:, 0], iris_pca[:, 1], c=iris_labels, cmap='tab20c', s=50)\nplt.xlabel('Premi√®re Composante Principale')\nplt.ylabel('Deuxi√®me Composante Principale')\nplt.title('Clusters Hi√©rarchiques - Visualisation PCA')\nplt.colorbar(label='Cluster')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# √âvaluation\nprint(\"üìä √âvaluation du clustering hi√©rarchique:\")\nprint(f\"Silhouette Score: {silhouette_score(iris_scaled, iris_labels):.3f}\")\nprint(f\"Calinski-Harabasz Score: {calinski_harabasz_score(iris_scaled, iris_labels):.2f}\")\nprint(f\"Davies-Bouldin Score: {davies_bouldin_score(iris_scaled, iris_labels):.3f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-9-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nüìä √âvaluation du clustering hi√©rarchique:\nSilhouette Score: 0.447\nCalinski-Harabasz Score: 222.72\nDavies-Bouldin Score: 0.803\n```\n:::\n:::\n\n\n## 5. Comparaison des M√©thodes de Clustering\n\n::: {#667d570f .cell execution_count=9}\n``` {.python .cell-code}\n# Test de diff√©rentes m√©thodes sur le dataset Iris\nmethods = {\n    'K-means (k=3)': KMeans(n_clusters=3, random_state=42, n_init=10),\n    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n    'Agglomerative (Ward)': AgglomerativeClustering(n_clusters=3, linkage='ward'),\n    'Agglomerative (Average)': AgglomerativeClustering(n_clusters=3, linkage='average')\n}\n\nresults = []\n\nfor name, model in methods.items():\n    labels = model.fit_predict(iris_scaled)\n    \n    if len(set(labels)) > 1:  # Au moins 2 clusters\n        silhouette = silhouette_score(iris_scaled, labels)\n        n_clusters = len(set(labels))\n    else:\n        silhouette = np.nan\n        n_clusters = len(set(labels))\n    \n    results.append({\n        'M√©thode': name,\n        'Nombre de clusters': n_clusters,\n        'Silhouette Score': silhouette\n    })\n\nresults_df = pd.DataFrame(results)\nprint(\"üìã Comparaison des m√©thodes de clustering:\")\nprint(results_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüìã Comparaison des m√©thodes de clustering:\n                M√©thode  Nombre de clusters  Silhouette Score\n          K-means (k=3)                   3          0.459948\n                 DBSCAN                   3          0.356516\n   Agglomerative (Ward)                   3          0.446689\nAgglomerative (Average)                   3          0.480267\n```\n:::\n:::\n\n\n## 6. Exercice Pratique Guid√©\n\n::: {.callout-warning icon=false}\n## Exercice 1: Dataset Wine\n1. Chargez le dataset Wine de scikit-learn (`datasets.load_wine()`)\n2. Normalisez les donn√©es\n3. D√©terminez le nombre optimal de clusters avec la m√©thode du coude et le silhouette score\n4. Appliquez k-means avec le k optimal\n5. Visualisez les clusters avec PCA\n6. Interpr√©tez les r√©sultats en termes de caract√©ristiques des vins\n:::\n::: {.callout-note collapse=\"true\"}\n### Solution Exercice 1\n\n::: {#9eece259 .cell execution_count=10}\n``` {.python .cell-code}\nprint(\"üç∑ Dataset Wine - Analyse compl√®te\")\nprint(\"=\"*60)\n\n# 1. Chargement\nwine = datasets.load_wine()\nwine_data = pd.DataFrame(wine.data, columns=wine.feature_names)\n\nprint(f\"\\nüìä Dimensions: {wine_data.shape}\")\nprint(f\"Nombre de classes originales: {len(np.unique(wine.target))}\")\nprint(f\"\\nCaract√©ristiques: {wine.feature_names}\")\n\n# Exploration initiale\nprint(\"\\nüìà Statistiques descriptives:\")\nprint(wine_data.describe())\n\n# 2. Normalisation\nwine_scaled = StandardScaler().fit_transform(wine_data)\nprint(\"\\n‚úÖ Donn√©es normalis√©es\")\n\n# 3. D√©termination du nombre optimal de clusters\nprint(\"\\nüîç D√©termination du nombre optimal de clusters...\")\n\ninertias_wine = []\nsilhouette_scores_wine = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans_wine = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans_wine.fit(wine_scaled)\n    inertias_wine.append(kmeans_wine.inertia_)\n    \n    score = silhouette_score(wine_scaled, kmeans_wine.labels_)\n    silhouette_scores_wine.append(score)\n\n# Visualisation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Courbe d'inertie\nax1.plot(K_range, inertias_wine, 'bo-', linewidth=2, markersize=8)\nax1.set_xlabel('Nombre de clusters (k)', fontsize=12)\nax1.set_ylabel('Inertie', fontsize=12)\nax1.set_title('M√©thode du Coude - Dataset Wine', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\nax1.axvline(x=3, color='r', linestyle='--', alpha=0.5, label='k optimal sugg√©r√©')\nax1.legend()\n\n# Score silhouette\nax2.plot(K_range, silhouette_scores_wine, 'go-', linewidth=2, markersize=8)\nax2.set_xlabel('Nombre de clusters (k)', fontsize=12)\nax2.set_ylabel('Score Silhouette', fontsize=12)\nax2.set_title('Score Silhouette - Dataset Wine', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\nax2.axvline(x=3, color='r', linestyle='--', alpha=0.5, label='k optimal sugg√©r√©')\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Analyse des scores\nprint(\"\\nüìä Analyse des m√©triques:\")\nfor i, k in enumerate(K_range):\n    print(f\"k={k}: Inertie={inertias_wine[i]:.2f}, Silhouette={silhouette_scores_wine[i]:.3f}\")\n\n# Identification du k optimal\noptimal_k = K_range[np.argmax(silhouette_scores_wine)]\nprint(f\"\\nüéØ Nombre optimal de clusters sugg√©r√©: k={optimal_k}\")\n\n# 4. Application de k-means avec k optimal\nkmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nwine_clusters = kmeans_final.fit_predict(wine_scaled)\n\nprint(f\"\\n‚úÖ Clustering effectu√© avec k={optimal_k}\")\nprint(f\"Taille des clusters: {np.bincount(wine_clusters)}\")\n\n# Ajout des clusters au DataFrame\nwine_data['Cluster'] = wine_clusters\n\n# Statistiques par cluster\nprint(\"\\nüìä Caract√©ristiques moyennes par cluster:\")\ncluster_profiles = wine_data.groupby('Cluster').mean()\nprint(cluster_profiles.round(2))\n\n# 5. Visualisation avec PCA\npca_wine = PCA(n_components=2)\nwine_pca = pca_wine.fit_transform(wine_scaled)\n\nprint(f\"\\nüìâ Variance expliqu√©e par PCA:\")\nprint(f\"PC1: {pca_wine.explained_variance_ratio_[0]:.2%}\")\nprint(f\"PC2: {pca_wine.explained_variance_ratio_[1]:.2%}\")\nprint(f\"Total: {sum(pca_wine.explained_variance_ratio_):.2%}\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Clusters identifi√©s\nscatter1 = axes[0].scatter(wine_pca[:, 0], wine_pca[:, 1], \n                          c=wine_clusters, cmap='viridis', \n                          s=100, alpha=0.7, edgecolors='black', linewidth=0.5)\naxes[0].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})', fontsize=12)\naxes[0].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})', fontsize=12)\naxes[0].set_title('Clusters identifi√©s - k-means', fontsize=14, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# Vraies classes (pour comparaison)\nscatter2 = axes[1].scatter(wine_pca[:, 0], wine_pca[:, 1], \n                          c=wine.target, cmap='plasma', \n                          s=100, alpha=0.7, edgecolors='black', linewidth=0.5)\naxes[1].set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})', fontsize=12)\naxes[1].set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})', fontsize=12)\naxes[1].set_title('Vraies classes (r√©f√©rence)', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\nplt.colorbar(scatter2, ax=axes[1], label='Classe r√©elle')\n\nplt.tight_layout()\nplt.show()\n\n# 6. Interpr√©tation en termes de caract√©ristiques des vins\nprint(\"\\nüçá Interpr√©tation m√©tier - Profils des clusters de vins:\")\nprint(\"=\"*60)\n\nfor cluster_id in range(optimal_k):\n    cluster_mask = wine_data['Cluster'] == cluster_id\n    cluster_subset = wine_data[cluster_mask]\n    \n    print(f\"\\nüç∑ CLUSTER {cluster_id} ({cluster_mask.sum()} vins):\")\n    print(\"-\" * 60)\n    \n    # Caract√©ristiques principales\n    top_features = cluster_profiles.loc[cluster_id].nlargest(5)\n    print(f\"Caract√©ristiques dominantes:\")\n    for feat, val in top_features.items():\n        if feat != 'Cluster':\n            print(f\"  ‚Ä¢ {feat}: {val:.2f}\")\n    \n    # Interpr√©tation qualitative\n    alcohol = cluster_profiles.loc[cluster_id, 'alcohol']\n    color_intensity = cluster_profiles.loc[cluster_id, 'color_intensity']\n    flavanoids = cluster_profiles.loc[cluster_id, 'flavanoids']\n    \n    print(f\"\\nProfil g√©n√©ral:\")\n    if alcohol > 13:\n        print(f\"  ‚Ä¢ Teneur en alcool √©lev√©e ({alcohol:.1f}%)\")\n    elif alcohol < 12:\n        print(f\"  ‚Ä¢ Teneur en alcool mod√©r√©e ({alcohol:.1f}%)\")\n    else:\n        print(f\"  ‚Ä¢ Teneur en alcool moyenne ({alcohol:.1f}%)\")\n    \n    if color_intensity > 5:\n        print(f\"  ‚Ä¢ Couleur intense ({color_intensity:.1f})\")\n    else:\n        print(f\"  ‚Ä¢ Couleur l√©g√®re ({color_intensity:.1f})\")\n    \n    if flavanoids > 2.5:\n        print(f\"  ‚Ä¢ Riche en flavono√Ødes ({flavanoids:.1f})\")\n    else:\n        print(f\"  ‚Ä¢ Pauvre en flavono√Ødes ({flavanoids:.1f})\")\n\n# M√©triques de qualit√© du clustering\nprint(\"\\nüìä √âvaluation de la qualit√© du clustering:\")\nprint(f\"Silhouette Score: {silhouette_score(wine_scaled, wine_clusters):.3f}\")\nprint(f\"Calinski-Harabasz Score: {calinski_harabasz_score(wine_scaled, wine_clusters):.2f}\")\nprint(f\"Davies-Bouldin Score: {davies_bouldin_score(wine_scaled, wine_clusters):.3f}\")\nprint(\"\\nüí° Note: Un bon silhouette score > 0.5, plus il est proche de 1, mieux c'est\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüç∑ Dataset Wine - Analyse compl√®te\n============================================================\n\nüìä Dimensions: (178, 13)\nNombre de classes originales: 3\n\nCaract√©ristiques: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n\nüìà Statistiques descriptives:\n          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\ncount  178.000000  178.000000  178.000000         178.000000  178.000000   \nmean    13.000618    2.336348    2.366517          19.494944   99.741573   \nstd      0.811827    1.117146    0.274344           3.339564   14.282484   \nmin     11.030000    0.740000    1.360000          10.600000   70.000000   \n25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n75%     13.677500    3.082500    2.557500          21.500000  107.000000   \nmax     14.830000    5.800000    3.230000          30.000000  162.000000   \n\n       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\ncount     178.000000  178.000000            178.000000       178.000000   \nmean        2.295112    2.029270              0.361854         1.590899   \nstd         0.625851    0.998859              0.124453         0.572359   \nmin         0.980000    0.340000              0.130000         0.410000   \n25%         1.742500    1.205000              0.270000         1.250000   \n50%         2.355000    2.135000              0.340000         1.555000   \n75%         2.800000    2.875000              0.437500         1.950000   \nmax         3.880000    5.080000              0.660000         3.580000   \n\n       color_intensity         hue  od280/od315_of_diluted_wines      proline  \ncount       178.000000  178.000000                    178.000000   178.000000  \nmean          5.058090    0.957449                      2.611685   746.893258  \nstd           2.318286    0.228572                      0.709990   314.907474  \nmin           1.280000    0.480000                      1.270000   278.000000  \n25%           3.220000    0.782500                      1.937500   500.500000  \n50%           4.690000    0.965000                      2.780000   673.500000  \n75%           6.200000    1.120000                      3.170000   985.000000  \nmax          13.000000    1.710000                      4.000000  1680.000000  \n\n‚úÖ Donn√©es normalis√©es\n\nüîç D√©termination du nombre optimal de clusters...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-11-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nüìä Analyse des m√©triques:\nk=2: Inertie=1658.76, Silhouette=0.259\nk=3: Inertie=1277.93, Silhouette=0.285\nk=4: Inertie=1175.43, Silhouette=0.260\nk=5: Inertie=1109.51, Silhouette=0.202\nk=6: Inertie=1046.00, Silhouette=0.237\nk=7: Inertie=981.60, Silhouette=0.204\nk=8: Inertie=935.20, Silhouette=0.157\nk=9: Inertie=889.89, Silhouette=0.150\nk=10: Inertie=845.90, Silhouette=0.144\n\nüéØ Nombre optimal de clusters sugg√©r√©: k=3\n\n‚úÖ Clustering effectu√© avec k=3\nTaille des clusters: [65 51 62]\n\nüìä Caract√©ristiques moyennes par cluster:\n         alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  \\\nCluster                                                            \n0          12.25        1.90  2.23              20.06      92.74   \n1          13.13        3.31  2.42              21.24      98.67   \n2          13.68        2.00  2.47              17.46     107.97   \n\n         total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\nCluster                                                                     \n0                 2.25        2.05                  0.36             1.62   \n1                 1.68        0.82                  0.45             1.15   \n2                 2.85        3.00                  0.29             1.92   \n\n         color_intensity   hue  od280/od315_of_diluted_wines  proline  \nCluster                                                                \n0                   2.97  1.06                          2.80   510.17  \n1                   7.23  0.69                          1.70   619.06  \n2                   5.45  1.07                          3.16  1100.23  \n\nüìâ Variance expliqu√©e par PCA:\nPC1: 36.20%\nPC2: 19.21%\nTotal: 55.41%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-11-output-4.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nüçá Interpr√©tation m√©tier - Profils des clusters de vins:\n============================================================\n\nüç∑ CLUSTER 0 (65 vins):\n------------------------------------------------------------\nCaract√©ristiques dominantes:\n  ‚Ä¢ proline: 510.17\n  ‚Ä¢ magnesium: 92.74\n  ‚Ä¢ alcalinity_of_ash: 20.06\n  ‚Ä¢ alcohol: 12.25\n  ‚Ä¢ color_intensity: 2.97\n\nProfil g√©n√©ral:\n  ‚Ä¢ Teneur en alcool moyenne (12.3%)\n  ‚Ä¢ Couleur l√©g√®re (3.0)\n  ‚Ä¢ Pauvre en flavono√Ødes (2.0)\n\nüç∑ CLUSTER 1 (51 vins):\n------------------------------------------------------------\nCaract√©ristiques dominantes:\n  ‚Ä¢ proline: 619.06\n  ‚Ä¢ magnesium: 98.67\n  ‚Ä¢ alcalinity_of_ash: 21.24\n  ‚Ä¢ alcohol: 13.13\n  ‚Ä¢ color_intensity: 7.23\n\nProfil g√©n√©ral:\n  ‚Ä¢ Teneur en alcool √©lev√©e (13.1%)\n  ‚Ä¢ Couleur intense (7.2)\n  ‚Ä¢ Pauvre en flavono√Ødes (0.8)\n\nüç∑ CLUSTER 2 (62 vins):\n------------------------------------------------------------\nCaract√©ristiques dominantes:\n  ‚Ä¢ proline: 1100.23\n  ‚Ä¢ magnesium: 107.97\n  ‚Ä¢ alcalinity_of_ash: 17.46\n  ‚Ä¢ alcohol: 13.68\n  ‚Ä¢ color_intensity: 5.45\n\nProfil g√©n√©ral:\n  ‚Ä¢ Teneur en alcool √©lev√©e (13.7%)\n  ‚Ä¢ Couleur intense (5.5)\n  ‚Ä¢ Riche en flavono√Ødes (3.0)\n\nüìä √âvaluation de la qualit√© du clustering:\nSilhouette Score: 0.285\nCalinski-Harabasz Score: 70.94\nDavies-Bouldin Score: 1.389\n\nüí° Note: Un bon silhouette score > 0.5, plus il est proche de 1, mieux c'est\n```\n:::\n:::\n\n\n:::\n\n::: {.callout-warning icon=true}\n## Exercice 2: Clustering DBSCAN\n1. Sur le dataset Mall Customers, testez DBSCAN avec diff√©rents param√®tres\n2. Comparez les r√©sultats avec k-means\n3. Visualisez les clusters obtenus\n4. Identifiez les points consid√©r√©s comme bruit (-1)\n5. Analysez les avantages/inconv√©nients de DBSCAN pour ce dataset\n:::\n\n::: {.callout-note collapse=\"true\"}\n### Solution Exercice 2\n\n::: {#017de7d2 .cell execution_count=11}\n``` {.python .cell-code}\nprint(\"üõçÔ∏è Clustering DBSCAN sur Mall Customers\")\nprint(\"=\"*60)\n\n# Test de diff√©rents param√®tres DBSCAN\neps_values = [0.3, 0.5, 0.7, 1.0]\nmin_samples_values = [3, 5, 10]\n\nprint(\"\\nüîç Test de diff√©rentes configurations DBSCAN:\")\nprint(\"-\" * 60)\n\nbest_config = {'eps': None, 'min_samples': None, 'score': -1, 'n_clusters': 0}\ndbscan_results = []\n\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(mall_scaled)\n        \n        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n        n_noise = list(labels).count(-1)\n        \n        # Calcul du silhouette score (si au moins 2 clusters)\n        if n_clusters >= 2:\n            # Exclure les points de bruit pour le calcul du score\n            mask = labels != -1\n            if mask.sum() > 0:\n                score = silhouette_score(mall_scaled[mask], labels[mask])\n            else:\n                score = -1\n        else:\n            score = -1\n        \n        dbscan_results.append({\n            'eps': eps,\n            'min_samples': min_samples,\n            'n_clusters': n_clusters,\n            'n_noise': n_noise,\n            'silhouette': score\n        })\n        \n        print(f\"eps={eps}, min_samples={min_samples}: \"\n              f\"{n_clusters} clusters, {n_noise} points de bruit, \"\n              f\"silhouette={score:.3f}\")\n        \n        if score > best_config['score'] and n_clusters > 0:\n            best_config = {\n                'eps': eps,\n                'min_samples': min_samples,\n                'score': score,\n                'n_clusters': n_clusters,\n                'labels': labels\n            }\n\nprint(f\"\\nüéØ Meilleure configuration: eps={best_config['eps']}, \"\n      f\"min_samples={best_config['min_samples']}\")\n\n# Application de DBSCAN avec les meilleurs param√®tres\ndbscan_best = DBSCAN(eps=best_config['eps'], min_samples=best_config['min_samples'])\nmall_data['Cluster_DBSCAN'] = dbscan_best.fit_predict(mall_scaled)\n\n# Analyse des r√©sultats\nn_clusters_dbscan = len(set(mall_data['Cluster_DBSCAN'])) - (1 if -1 in mall_data['Cluster_DBSCAN'].values else 0)\nn_noise_dbscan = (mall_data['Cluster_DBSCAN'] == -1).sum()\n\nprint(f\"\\nüìä R√©sultats DBSCAN:\")\nprint(f\"Nombre de clusters: {n_clusters_dbscan}\")\nprint(f\"Nombre de points de bruit: {n_noise_dbscan} ({n_noise_dbscan/len(mall_data)*100:.1f}%)\")\n\n# Statistiques par cluster (sans le bruit)\nprint(\"\\nüìä Taille des clusters (hors bruit):\")\ncluster_counts = mall_data[mall_data['Cluster_DBSCAN'] != -1]['Cluster_DBSCAN'].value_counts().sort_index()\nfor cluster_id, count in cluster_counts.items():\n    print(f\"Cluster {cluster_id}: {count} clients\")\n\n# Comparaison avec k-means\nprint(\"\\n‚öñÔ∏è Comparaison K-means vs DBSCAN:\")\nprint(\"-\" * 60)\nprint(f\"K-means:\")\nprint(f\"  ‚Ä¢ Nombre de clusters: 3 (pr√©d√©fini)\")\nprint(f\"  ‚Ä¢ Silhouette score: {silhouette_score(mall_scaled, mall_data['Cluster_kmeans']):.3f}\")\nprint(f\"  ‚Ä¢ Tous les points assign√©s\")\n\nif n_clusters_dbscan >= 2:\n    mask_dbscan = mall_data['Cluster_DBSCAN'] != -1\n    score_dbscan = silhouette_score(mall_scaled[mask_dbscan], \n                                    mall_data.loc[mask_dbscan, 'Cluster_DBSCAN'])\n    print(f\"\\nDBSCAN:\")\n    print(f\"  ‚Ä¢ Nombre de clusters: {n_clusters_dbscan} (automatique)\")\n    print(f\"  ‚Ä¢ Silhouette score: {score_dbscan:.3f}\")\n    print(f\"  ‚Ä¢ Points de bruit: {n_noise_dbscan}\")\n\n# Visualisation comparative\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# K-means - 2D (Age vs Income)\nax1 = axes[0, 0]\nscatter1 = ax1.scatter(mall_data['Age'], mall_data['Annual_Income_k'],\n                      c=mall_data['Cluster_kmeans'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax1.set_xlabel('Age', fontsize=12)\nax1.set_ylabel('Revenu Annuel (k$)', fontsize=12)\nax1.set_title('K-means: Age vs Revenu', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\nplt.colorbar(scatter1, ax=ax1, label='Cluster')\n\n# DBSCAN - 2D (Age vs Income)\nax2 = axes[0, 1]\nscatter2 = ax2.scatter(mall_data['Age'], mall_data['Annual_Income_k'],\n                      c=mall_data['Cluster_DBSCAN'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax2.set_xlabel('Age', fontsize=12)\nax2.set_ylabel('Revenu Annuel (k$)', fontsize=12)\nax2.set_title('DBSCAN: Age vs Revenu (points noirs = bruit)', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\nplt.colorbar(scatter2, ax=ax2, label='Cluster')\n\n# K-means - PCA\nax3 = axes[1, 0]\nscatter3 = ax3.scatter(mall_data['PCA1'], mall_data['PCA2'],\n                      c=mall_data['Cluster_kmeans'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax3.set_xlabel('PCA1', fontsize=12)\nax3.set_ylabel('PCA2', fontsize=12)\nax3.set_title('K-means: Visualisation PCA', fontsize=14, fontweight='bold')\nax3.grid(True, alpha=0.3)\nplt.colorbar(scatter3, ax=ax3, label='Cluster')\n\n# DBSCAN - PCA\nax4 = axes[1, 1]\nscatter4 = ax4.scatter(mall_data['PCA1'], mall_data['PCA2'],\n                      c=mall_data['Cluster_DBSCAN'], cmap='viridis',\n                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)\nax4.set_xlabel('PCA1', fontsize=12)\nax4.set_ylabel('PCA2', fontsize=12)\nax4.set_title('DBSCAN: Visualisation PCA (points noirs = bruit)', fontsize=14, fontweight='bold')\nax4.grid(True, alpha=0.3)\nplt.colorbar(scatter4, ax=ax4, label='Cluster')\n\nplt.tight_layout()\nplt.show()\n\n# Analyse des points de bruit\nif n_noise_dbscan > 0:\n    noise_points = mall_data[mall_data['Cluster_DBSCAN'] == -1]\n    print(\"\\nüîç Analyse des points de bruit (outliers):\")\n    print(\"-\" * 60)\n    print(f\"Nombre: {len(noise_points)}\")\n    print(\"\\nCaract√©ristiques moyennes des outliers:\")\n    print(noise_points[['Age', 'Annual_Income_k', 'Spending_Score']].describe())\n    \n    print(\"\\nüí° Interpr√©tation:\")\n    print(\"Les points de bruit repr√©sentent des clients atypiques qui ne correspondent\")\n    print(\"√† aucun segment majeur - ils peuvent √™tre des cas particuliers √† traiter\")\n    print(\"individuellement en marketing.\")\n\n# Profils des clusters DBSCAN\nprint(\"\\nüìä Profils des clusters DBSCAN:\")\nprint(\"-\" * 60)\nfor cluster_id in sorted(mall_data['Cluster_DBSCAN'].unique()):\n    if cluster_id != -1:\n        cluster_data = mall_data[mall_data['Cluster_DBSCAN'] == cluster_id]\n        print(f\"\\nCluster {cluster_id} ({len(cluster_data)} clients):\")\n        print(f\"  ‚Ä¢ Age moyen: {cluster_data['Age'].mean():.1f} ans\")\n        print(f\"  ‚Ä¢ Revenu moyen: {cluster_data['Annual_Income_k'].mean():.1f}k$\")\n        print(f\"  ‚Ä¢ Score de d√©pense moyen: {cluster_data['Spending_Score'].mean():.1f}\")\n\n# Avantages et inconv√©nients\nprint(\"\\n‚úÖ Avantages de DBSCAN pour ce dataset:\")\nprint(\"-\" * 60)\nprint(\"1. D√©tection automatique du nombre de clusters\")\nprint(\"2. Identification des outliers (points atypiques)\")\nprint(\"3. Capacit√© √† d√©tecter des clusters de formes non-sph√©riques\")\nprint(\"4. Robustesse face au bruit dans les donn√©es\")\n\nprint(\"\\n‚ùå Inconv√©nients de DBSCAN pour ce dataset:\")\nprint(\"-\" * 60)\nprint(\"1. Sensibilit√© aux param√®tres eps et min_samples\")\nprint(\"2. Difficult√© avec des clusters de densit√©s variables\")\nprint(\"3. Certains clients sont exclus (marqu√©s comme bruit)\")\nprint(\"4. Moins intuitif pour la segmentation marketing traditionnelle\")\n\nprint(\"\\nüéØ Recommandation:\")\nprint(\"Pour ce dataset de segmentation client:\")\nprint(\"‚Ä¢ K-means est pr√©f√©rable si on veut assigner TOUS les clients √† un segment\")\nprint(\"‚Ä¢ DBSCAN est utile si on veut identifier les clients atypiques s√©par√©ment\")\nprint(\"‚Ä¢ Une approche hybride pourrait combiner les deux m√©thodes\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nüõçÔ∏è Clustering DBSCAN sur Mall Customers\n============================================================\n\nüîç Test de diff√©rentes configurations DBSCAN:\n------------------------------------------------------------\neps=0.3, min_samples=3: 17 clusters, 230 points de bruit, silhouette=0.548\neps=0.3, min_samples=5: 1 clusters, 290 points de bruit, silhouette=-1.000\neps=0.3, min_samples=10: 0 clusters, 300 points de bruit, silhouette=-1.000\neps=0.5, min_samples=3: 9 clusters, 64 points de bruit, silhouette=-0.019\neps=0.5, min_samples=5: 7 clusters, 96 points de bruit, silhouette=0.024\neps=0.5, min_samples=10: 3 clusters, 237 points de bruit, silhouette=0.528\neps=0.7, min_samples=3: 3 clusters, 14 points de bruit, silhouette=0.126\neps=0.7, min_samples=5: 1 clusters, 33 points de bruit, silhouette=-1.000\neps=0.7, min_samples=10: 1 clusters, 69 points de bruit, silhouette=-1.000\neps=1.0, min_samples=3: 1 clusters, 3 points de bruit, silhouette=-1.000\neps=1.0, min_samples=5: 1 clusters, 6 points de bruit, silhouette=-1.000\neps=1.0, min_samples=10: 1 clusters, 11 points de bruit, silhouette=-1.000\n\nüéØ Meilleure configuration: eps=0.3, min_samples=3\n\nüìä R√©sultats DBSCAN:\nNombre de clusters: 17\nNombre de points de bruit: 230 (76.7%)\n\nüìä Taille des clusters (hors bruit):\nCluster 0: 3 clients\nCluster 1: 11 clients\nCluster 2: 4 clients\nCluster 3: 4 clients\nCluster 4: 4 clients\nCluster 5: 4 clients\nCluster 6: 4 clients\nCluster 7: 3 clients\nCluster 8: 4 clients\nCluster 9: 4 clients\nCluster 10: 3 clients\nCluster 11: 3 clients\nCluster 12: 3 clients\nCluster 13: 3 clients\nCluster 14: 4 clients\nCluster 15: 5 clients\nCluster 16: 4 clients\n\n‚öñÔ∏è Comparaison K-means vs DBSCAN:\n------------------------------------------------------------\nK-means:\n  ‚Ä¢ Nombre de clusters: 3 (pr√©d√©fini)\n  ‚Ä¢ Silhouette score: 0.238\n  ‚Ä¢ Tous les points assign√©s\n\nDBSCAN:\n  ‚Ä¢ Nombre de clusters: 17 (automatique)\n  ‚Ä¢ Silhouette score: 0.548\n  ‚Ä¢ Points de bruit: 230\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance10_files/figure-html/cell-12-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nüîç Analyse des points de bruit (outliers):\n------------------------------------------------------------\nNombre: 230\n\nCaract√©ristiques moyennes des outliers:\n              Age  Annual_Income_k  Spending_Score\ncount  230.000000       230.000000      230.000000\nmean    35.789658        58.892871       50.383212\nstd      9.702947        20.438536       25.027695\nmin     18.000000        15.000000        1.000000\n25%     29.421887        43.889952       32.231994\n50%     35.944192        58.870828       49.493150\n75%     42.070654        72.720614       67.044845\nmax     70.000000       121.577616      100.000000\n\nüí° Interpr√©tation:\nLes points de bruit repr√©sentent des clients atypiques qui ne correspondent\n√† aucun segment majeur - ils peuvent √™tre des cas particuliers √† traiter\nindividuellement en marketing.\n\nüìä Profils des clusters DBSCAN:\n------------------------------------------------------------\n\nCluster 0 (3 clients):\n  ‚Ä¢ Age moyen: 51.0 ans\n  ‚Ä¢ Revenu moyen: 63.8k$\n  ‚Ä¢ Score de d√©pense moyen: 31.6\n\nCluster 1 (11 clients):\n  ‚Ä¢ Age moyen: 29.3 ans\n  ‚Ä¢ Revenu moyen: 42.0k$\n  ‚Ä¢ Score de d√©pense moyen: 65.0\n\nCluster 2 (4 clients):\n  ‚Ä¢ Age moyen: 29.4 ans\n  ‚Ä¢ Revenu moyen: 73.3k$\n  ‚Ä¢ Score de d√©pense moyen: 55.7\n\nCluster 3 (4 clients):\n  ‚Ä¢ Age moyen: 34.0 ans\n  ‚Ä¢ Revenu moyen: 64.4k$\n  ‚Ä¢ Score de d√©pense moyen: 45.6\n\nCluster 4 (4 clients):\n  ‚Ä¢ Age moyen: 41.3 ans\n  ‚Ä¢ Revenu moyen: 42.8k$\n  ‚Ä¢ Score de d√©pense moyen: 40.9\n\nCluster 5 (4 clients):\n  ‚Ä¢ Age moyen: 21.1 ans\n  ‚Ä¢ Revenu moyen: 50.0k$\n  ‚Ä¢ Score de d√©pense moyen: 41.4\n\nCluster 6 (4 clients):\n  ‚Ä¢ Age moyen: 27.3 ans\n  ‚Ä¢ Revenu moyen: 63.4k$\n  ‚Ä¢ Score de d√©pense moyen: 45.2\n\nCluster 7 (3 clients):\n  ‚Ä¢ Age moyen: 37.0 ans\n  ‚Ä¢ Revenu moyen: 66.0k$\n  ‚Ä¢ Score de d√©pense moyen: 96.0\n\nCluster 8 (4 clients):\n  ‚Ä¢ Age moyen: 27.9 ans\n  ‚Ä¢ Revenu moyen: 41.1k$\n  ‚Ä¢ Score de d√©pense moyen: 76.3\n\nCluster 9 (4 clients):\n  ‚Ä¢ Age moyen: 37.6 ans\n  ‚Ä¢ Revenu moyen: 49.6k$\n  ‚Ä¢ Score de d√©pense moyen: 22.7\n\nCluster 10 (3 clients):\n  ‚Ä¢ Age moyen: 44.5 ans\n  ‚Ä¢ Revenu moyen: 67.2k$\n  ‚Ä¢ Score de d√©pense moyen: 79.3\n\nCluster 11 (3 clients):\n  ‚Ä¢ Age moyen: 30.6 ans\n  ‚Ä¢ Revenu moyen: 48.1k$\n  ‚Ä¢ Score de d√©pense moyen: 76.2\n\nCluster 12 (3 clients):\n  ‚Ä¢ Age moyen: 40.3 ans\n  ‚Ä¢ Revenu moyen: 49.8k$\n  ‚Ä¢ Score de d√©pense moyen: 36.1\n\nCluster 13 (3 clients):\n  ‚Ä¢ Age moyen: 39.8 ans\n  ‚Ä¢ Revenu moyen: 40.5k$\n  ‚Ä¢ Score de d√©pense moyen: 54.4\n\nCluster 14 (4 clients):\n  ‚Ä¢ Age moyen: 42.0 ans\n  ‚Ä¢ Revenu moyen: 60.1k$\n  ‚Ä¢ Score de d√©pense moyen: 53.0\n\nCluster 15 (5 clients):\n  ‚Ä¢ Age moyen: 18.4 ans\n  ‚Ä¢ Revenu moyen: 36.7k$\n  ‚Ä¢ Score de d√©pense moyen: 86.0\n\nCluster 16 (4 clients):\n  ‚Ä¢ Age moyen: 26.3 ans\n  ‚Ä¢ Revenu moyen: 76.0k$\n  ‚Ä¢ Score de d√©pense moyen: 48.5\n\n‚úÖ Avantages de DBSCAN pour ce dataset:\n------------------------------------------------------------\n1. D√©tection automatique du nombre de clusters\n2. Identification des outliers (points atypiques)\n3. Capacit√© √† d√©tecter des clusters de formes non-sph√©riques\n4. Robustesse face au bruit dans les donn√©es\n\n‚ùå Inconv√©nients de DBSCAN pour ce dataset:\n------------------------------------------------------------\n1. Sensibilit√© aux param√®tres eps et min_samples\n2. Difficult√© avec des clusters de densit√©s variables\n3. Certains clients sont exclus (marqu√©s comme bruit)\n4. Moins intuitif pour la segmentation marketing traditionnelle\n\nüéØ Recommandation:\nPour ce dataset de segmentation client:\n‚Ä¢ K-means est pr√©f√©rable si on veut assigner TOUS les clients √† un segment\n‚Ä¢ DBSCAN est utile si on veut identifier les clients atypiques s√©par√©ment\n‚Ä¢ Une approche hybride pourrait combiner les deux m√©thodes\n```\n:::\n:::\n\n\n:::\n\n## 7. Questions de R√©flexion\n\n::: {.callout-note icon=false}\n## Question 1\nDans l'analyse des clients du centre commercial, quelles actions marketing pourriez-vous recommander pour chaque segment identifi√© ?\n:::\n\n::: {.callout-note collapse=\"true\"}\n## R√©ponse Question 1\n\n**Analyse des segments et recommandations marketing:**\n\n**Cluster 0 - Clients Moyens (Segment Mainstream)**\n\n- **Profil**: √Çge moyen (30-45 ans), revenu moyen (50-70k$), d√©penses mod√©r√©es\n- **Taille estim√©e**: ~40% de la client√®le\n- **Actions recommand√©es**:\n  - Programmes de fid√©lit√© avec r√©compenses progressives\n  - Promotions r√©guli√®res sur des produits de consommation courante\n  - Communication √©quilibr√©e entre qualit√© et prix\n  - Campagnes saisonni√®res cibl√©es\n  - Cross-selling sur produits compl√©mentaires\n\n**Cluster 1 - Jeunes D√©pensiers (Segment Premium Jeune)**\n\n- **Profil**: Jeunes (18-30 ans), revenu mod√©r√© (30-50k$), score de d√©pense √©lev√© (>60)\n- **Taille estim√©e**: ~25% de la client√®le\n- **Actions recommand√©es**:\n  - Marketing digital et r√©seaux sociaux intensif\n  - Lancements de nouveaux produits tendance\n  - √âv√©nements exclusifs et exp√©riences immersives\n  - Programmes de parrainage avec r√©compenses imm√©diates\n  - Offres \"acheter maintenant, payer plus tard\"\n  - Collaboration avec influenceurs\n  - Collections capsules et √©ditions limit√©es\n\n**Cluster 2 - Seniors √âconomes (Segment Conservateur Ais√©)**\n\n- **Profil**: √Çge √©lev√© (>50 ans), revenu √©lev√© (70-90k$), d√©penses faibles (<40)\n- **Taille estim√©e**: ~35% de la client√®le\n- **Actions recommand√©es**:\n  - Mise en avant du rapport qualit√©-prix\n  - Service client premium et personnalis√©\n  - Programmes de points avec avantages √† long terme\n  - Communication par email et courrier traditionnel\n  - Offres exclusives sur des produits durables et de qualit√©\n  - Conseils personnalis√©s et service apr√®s-vente renforc√©\n  - √âv√©nements VIP en petit comit√©\n\n**Strat√©gie globale recommand√©e**:\n\n- Personnalisation des campagnes par segment\n- A/B testing des messages marketing par cluster\n- Optimisation de l'assortiment produit par profil\n- Formation du personnel √† la reconnaissance des profils\n- Mesure du ROI par segment pour allocation budg√©taire optimale\n:::\n\n::: {.callout-note icon=false}\n## Question 2\nQuand choisiriez-vous PCA vs t-SNE pour la visualisation des clusters ? Justifiez avec des exemples concrets.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## R√©ponse Question 2\n\n**Comparaison PCA vs t-SNE pour la visualisation de clusters:**\n\n**Choisir PCA quand:**\n\n1. **Interpr√©tabilit√© requise**\n\n   - Exemple: Rapport pour la direction n√©cessitant de comprendre quelles variables contribuent aux axes\n   - Les composantes principales sont des combinaisons lin√©aires interpr√©tables\n   - On peut expliquer \"PC1 repr√©sente 45% de la variance et combine principalement le revenu et l'√©ducation\"\n\n2. **Analyse de la variance**\n\n   - Exemple: D√©terminer combien de dimensions conserver\n   - Permet de quantifier l'information perdue: \"2 composantes capturent 78% de la variance\"\n   - Utile pour la r√©duction de dimension avant clustering\n\n3. **Datasets de taille moyenne √† grande**\n\n   - Exemple: 10,000+ observations\n   - PCA est beaucoup plus rapide (complexit√© lin√©aire vs quadratique)\n   - Scalabilit√© pour les applications en production\n\n4. **Stabilit√© et reproductibilit√©**\n\n   - Exemple: Dashboards actualis√©s quotidiennement\n   - PCA donne toujours le m√™me r√©sultat (d√©terministe)\n   - t-SNE peut varier √† chaque ex√©cution\n\n5. **Relations lin√©aires √† pr√©server**\n\n   - Exemple: Variables √©conomiques corr√©l√©es lin√©airement\n   - PCA pr√©serve les distances globales\n   - Meilleur pour comprendre la structure g√©n√©rale\n\n**Choisir t-SNE quand:**\n\n1. **Visualisation pure pour exploration**\n   - Exemple: Premi√®re exploration d'un dataset complexe\n   - R√©v√®le des structures non-lin√©aires cach√©es\n   - Meilleur pour \"voir\" les groupements naturels\n\n2. **Structures non-lin√©aires complexes**\n\n   - Exemple: Donn√©es d'images, de textes, ou g√©nomiques\n   - t-SNE peut \"d√©rouler\" des manifolds non-lin√©aires\n   - Cas o√π PCA montre un nuage de points uniforme\n\n3. **Pr√©servation des voisinages locaux**\n\n   - Exemple: Analyse de sous-populations fines\n   - t-SNE garde ensemble les points similaires\n   - Meilleur pour identifier des micro-clusters\n\n4. **Datasets de petite √† moyenne taille**\n\n   - Exemple: <5,000 observations\n   - Le co√ªt computationnel reste acceptable\n   - Permet d'optimiser les hyperparam√®tres (perplexity)\n\n5. **Pr√©sentation/communication visuelle**\n\n   - Exemple: Publications scientifiques, pr√©sentations\n   - Souvent plus \"impressionnant\" visuellement\n   - Clusters plus clairement s√©par√©s\n\n**Approche recommand√©e - Utiliser les DEUX:**\n\n```python\n# Strat√©gie optimale pour un projet r√©el\n# 1. PCA d'abord pour comprendre\npca = PCA(n_components=0.95)  # 95% de variance\ndata_pca = pca.fit_transform(data_scaled)\nprint(f\"Dimensions r√©duites de {data.shape[1]} √† {data_pca.shape[1]}\")\n\n# 2. PCA pour le clustering\nkmeans = KMeans(n_clusters=k)\nclusters = kmeans.fit_predict(data_pca)\n\n# 3. PCA pour visualisation interpr√©table\npca_2d = PCA(n_components=2)\nviz_pca = pca_2d.fit_transform(data_scaled)\n\n# 4. t-SNE pour visualisation exploratoire\ntsne_2d = TSNE(n_components=2, perplexity=30)\nviz_tsne = tsne_2d.fit_transform(data_scaled)\n\n# 5. Comparaison visuelle\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.scatter(viz_pca[:, 0], viz_pca[:, 1], c=clusters)\nax1.set_title('PCA - Variance pr√©serv√©e')\nax2.scatter(viz_tsne[:, 0], viz_tsne[:, 1], c=clusters)\nax2.set_title('t-SNE - Voisinages pr√©serv√©s')\n```\n\n**Cas pratiques concrets:**\n\n| Situation | Choix | Raison |\n|-----------|-------|--------|\n| Segmentation clients bancaires (50k clients) | PCA | Scalabilit√© + interpr√©tabilit√© pour r√©gulation |\n| Exploration de donn√©es g√©n√©tiques (500 √©chantillons) | t-SNE | Structures biologiques non-lin√©aires |\n| Dashboard temps r√©el e-commerce | PCA | Rapidit√© + reproductibilit√© |\n| Publication recherche (clustering cellules) | Les deux | PCA pour m√©thode, t-SNE pour figures |\n| R√©duction avant ML (100k lignes) | PCA | Performance computationnelle |\n\n**Erreurs √† √©viter:**\n\n- ‚ùå Utiliser t-SNE pour des donn√©es tr√®s high-dimensional sans pr√©-r√©duction PCA\n- ‚ùå Interpr√©ter les distances absolues dans t-SNE (seuls les voisinages comptent)\n- ‚ùå Utiliser PCA sur donn√©es non-normalis√©es\n- ‚ùå Fixer perplexity=30 sans tester d'autres valeurs pour t-SNE\n- ‚ùå Utiliser t-SNE en production sans consid√©rer le temps de calcul\n:::\n\n::: {.callout-note icon=false}\n## Question 3\nProposez une m√©trique business pour √©valuer l'efficacit√© du clustering au-del√† des m√©triques techniques.\n:::\n\n::: {.callout-note collapse=\"true\"}\n## R√©ponse Question 3\n\n**M√©triques Business pour √âvaluer l'Efficacit√© du Clustering**\n\nLes m√©triques techniques (silhouette score, inertie) mesurent la qualit√© math√©matique, mais pas l'impact business. Voici des m√©triques orient√©es valeur:\n\n---\n\n**1. AUGMENTATION DU TAUX DE CONVERSION PAR SEGMENT**\n\n**D√©finition:**\n```\nTaux conversion post-segmentation - Taux conversion baseline\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ √ó 100\n           Taux conversion baseline\n```\n\n**Exemple concret:**\n\n```\nBaseline (pas de segmentation): 2.5% conversion\nApr√®s segmentation et marketing cibl√©:\n- Segment Premium: 5.2% (+108%)\n- Segment √âconome: 3.1% (+24%)\n- Segment Moyen: 2.8% (+12%)\n\nM√©trique globale: +45% conversion moyenne pond√©r√©e\n```\n\n**Avantages:**\n- Mesure directe de l'impact financier\n- Facile √† communiquer aux stakeholders\n- Comparable dans le temps\n\n---\n\n**2. CUSTOMER LIFETIME VALUE (CLV) PAR SEGMENT**\n\n**D√©finition:**\n```\nCLV_segment = (Revenu moyen par achat √ó Fr√©quence d'achat √ó Dur√©e de vie client)\n              - (Co√ªt acquisition + Co√ªt service)\n```\n\n**Application:**\n```python\n# Calcul apr√®s 6 mois de campagnes segment√©es\nsegments_clv = {\n    'Cluster 0': {\n        'CLV': 1250‚Ç¨,\n        'Co√ªt acquisition': 45‚Ç¨,\n        'ROI': 27.8\n    },\n    'Cluster 1': {\n        'CLV': 2100‚Ç¨,\n        'Co√ªt acquisition': 85‚Ç¨,\n        'ROI': 24.7\n    },\n    'Cluster 2': {\n        'CLV': 890‚Ç¨,\n        'Co√ªt acquisition': 35‚Ç¨,\n        'ROI': 25.4\n    }\n}\n\n# M√©trique: CLV pond√©r√© total vs approche non-segment√©e\nCLV_improvement = (weighted_avg_clv_segmented - clv_baseline) / clv_baseline\n```\n\n**KPI d'√©valuation:**\n- CLV moyen par segment > CLV baseline\n- Variance du CLV entre segments (plus √©lev√©e = meilleure diff√©renciation)\n- Budget marketing optimis√© selon CLV/segment\n\n---\n\n**3. TAUX DE R√âTENTION DIFF√âRENTIEL**\n\n**D√©finition:**\n```\nR√©tention_segment(t) = Clients actifs en t / Clients actifs en t-1\n\nM√©trique: Diff√©rence de r√©tention entre segments vs approche globale\n```\n\n**Tableau de bord:**\n\n\n| Segment        | R√©tention M+3 | R√©tention M+6 | R√©tention M+12 | Am√©lioration vs baseline |\n|----------------|---------------|---------------|----------------|--------------------------|\n| Premium Jeune  | 92%           | 85%           | 78%            | +15%                     |\n| Conservateurs  | 95%           | 91%           | 88%            | +22%                     |\n| Mainstream     | 88%           | 79%           | 71%            | +8%                      |\n| Baseline       | 82%           | 73%           | 65%            | -                        |\n\n\n\n**M√©trique synth√©tique:**\n```\nScore_Efficacit√©_R√©tention = Œ£(R√©tention_segment √ó Poids_segment) - R√©tention_baseline\n```\n\n---\n\n**4. EFFICACIT√â OP√âRATIONNELLE DES CAMPAGNES**\n\n**D√©finition:**\n```\nCo√ªt par Acquisition (CPA) par segment\nROI marketing = (Revenu g√©n√©r√© - Co√ªt campagne) / Co√ªt campagne\n```\n\n**Exemple d'√©valuation:**\n```\nCampagne Email Marketing:\n\nSans segmentation:\n- Envois: 100,000\n- Taux ouverture: 18%\n- Conversions: 450\n- CPA: 22‚Ç¨\n\nAvec segmentation (3 messages adapt√©s):\n- Segment A: 35,000 envois, 28% ouverture, 280 conversions, CPA: 15‚Ç¨\n- Segment B: 40,000 envois, 22% ouverture, 200 conversions, CPA: 18‚Ç¨\n- Segment C: 25,000 envois, 32% ouverture, 220 conversions, CPA: 12‚Ç¨\n\nTotal conversions: 700 (+55%)\nCPA moyen pond√©r√©: 15.2‚Ç¨ (-31%)\n\nM√©trique: Efficacit√© = (700-450)/450 √ó (22-15.2)/22 = +90% d'efficacit√©\n```\n\n---\n\n**5. INDICE DE STABILIT√â DES SEGMENTS (ISS)**\n\n**D√©finition:**\nMesure si les segments restent coh√©rents dans le temps (crucial pour strat√©gie long-terme)\n\n```python\ndef indice_stabilite_segment(labels_t1, labels_t2):\n    \"\"\"\n    Mesure la stabilit√©: clients restent-ils dans leur segment?\n    \"\"\"\n    # Matrice de transition\n    transition_matrix = pd.crosstab(labels_t1, labels_t2, normalize='index')\n    \n    # Stabilit√© = moyenne des probabilit√©s diagonales\n    stabilite = np.mean(np.diag(transition_matrix))\n    \n    return stabilite\n\n# Exemple\nstabilite_3_mois = 0.87  # 87% des clients restent dans leur segment\nstabilite_6_mois = 0.82\nstabilite_12_mois = 0.76\n\n# M√©trique: Si stabilit√© < 0.7, les segments ne sont pas fiables\n```\n\n**Interpr√©tation:**\n- ISS > 0.80: Excellente stabilit√©, segments bien d√©finis\n- ISS 0.60-0.80: Stabilit√© acceptable, ajustements mineurs\n- ISS < 0.60: Segments peu fiables, revoir la segmentation\n\n---\n\n**6. SCORE DE DIFF√âRENCIATION ACTIONNABLE**\n\n**D√©finition:**\nLes segments doivent √™tre suffisamment diff√©rents pour justifier des actions distinctes\n\n```python\ndef score_differenciation_business(segments_data):\n    \"\"\"\n    Mesure si les segments justifient des strat√©gies diff√©rentes\n    \"\"\"\n    scores = []\n    \n    # 1. Diff√©rence de comportement d'achat\n    purchase_variance = segments_data.groupby('segment')['purchase_frequency'].var()\n    scores.append(normalize(purchase_variance.mean()))\n    \n    # 2. Diff√©rence de pr√©f√©rences produits\n    product_affinity_diff = calculate_product_affinity_distance(segments_data)\n    scores.append(normalize(product_affinity_diff))\n    \n    # 3. Diff√©rence de sensibilit√© prix\n    price_sensitivity_diff = calculate_price_elasticity_diff(segments_data)\n    scores.append(normalize(price_sensitivity_diff))\n    \n    # 4. Diff√©rence de canaux pr√©f√©r√©s\n    channel_preference_diff = calculate_channel_divergence(segments_data)\n    scores.append(normalize(channel_preference_diff))\n    \n    # Score final (0-100)\n    return np.mean(scores) * 100\n\n# Interpr√©tation:\n# Score > 70: Segments tr√®s diff√©renci√©s, strat√©gies distinctes justifi√©es\n# Score 40-70: Diff√©renciation mod√©r√©e, personnalisation partielle\n# Score < 40: Segments trop similaires, clustering peu utile\n```\n\n---\n\n**7. M√âTRIQUE COMPOSITE: BUSINESS VALUE SCORE (BVS)**\n\n**Formule int√©gr√©e:**\n```\nBVS = w1√ó(Lift_Conversion) + w2√ó(Am√©lioration_CLV) + w3√ó(R√©duction_CPA) \n      + w4√ó(Stabilit√©) + w5√ó(Diff√©renciation)\n\nAvec: Œ£wi = 1 (pond√©rations selon priorit√©s business)\n```\n\n**Exemple de calcul:**\n```python\n# Pond√©rations pour un e-commerce\nweights = {\n    'conversion_lift': 0.30,      # Priorit√© maximale\n    'clv_improvement': 0.25,\n    'cpa_reduction': 0.20,\n    'stability': 0.15,\n    'differentiation': 0.10\n}\n\nmetrics = {\n    'conversion_lift': 0.45,      # +45%\n    'clv_improvement': 0.32,      # +32%\n    'cpa_reduction': 0.28,        # -28% (normalis√© positivement)\n    'stability': 0.82,            # ISS = 0.82\n    'differentiation': 0.73       # Score = 73/100\n}\n\nBVS = sum(weights[k] * metrics[k] for k in weights.keys())\n# BVS = 0.456 ‚Üí Score de 45.6/100\n\n# Interpr√©tation:\n# BVS > 0.60: Clustering tr√®s efficace\n# BVS 0.40-0.60: Clustering efficace\n# BVS < 0.40: Revoir la segmentation\n```\n\n---\n\n**DASHBOARD DE SUIVI RECOMMAND√â:**\n\n```\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n‚ïë                 √âVALUATION BUSINESS DU CLUSTERING - Q1 2025      ‚ïë\n‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n‚ïë Business Value Score (BVS):                        47.2/100  [‚úì] ‚ïë\n‚ïë                                                                  ‚ïë\n‚ïë M√©triques D√©taill√©es:                                            ‚ïë\n‚ïë ‚îú‚îÄ Conversion Lift:                                +38%     [‚úì]  ‚ïë\n‚ïë ‚îú‚îÄ CLV Am√©lioration:                               +28%     [‚úì]  ‚ïë\n‚ïë ‚îú‚îÄ CPA R√©duction:                                  -22%     [‚úì]  ‚ïë\n‚ïë ‚îú‚îÄ Indice Stabilit√© (6 mois):                     0.79      [‚úì]  ‚ïë\n‚ïë ‚îî‚îÄ Score Diff√©renciation:                          68/100   [‚úì]  ‚ïë\n‚ïë                                                                  ‚ïë\n‚ïë ROI Global Clustering:                             324%          ‚ïë\n‚ïë Co√ªt impl√©mentation:                               45K‚Ç¨          ‚ïë\n‚ïë Gain annuel estim√©:                                146K‚Ç¨         ‚ïë\n‚ïë                                                                  ‚ïë\n‚ïë Recommandation:                     ‚úÖ Poursuivre et optimiser   ‚ïë     \n‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n```\n\n---\n\n**CONCLUSION:**\n\nLa meilleure approche combine:\n1. **M√©trique primaire**: Conversion Lift ou CLV (selon objectif business)\n2. **M√©trique secondaire**: Efficacit√© op√©rationnelle (CPA, ROI marketing)\n3. **M√©trique de contr√¥le**: Stabilit√© et diff√©renciation\n\n**R√®gle d'or**: Si le clustering n'am√©liore pas au moins une m√©trique business de 15-20% sur 3-6 mois, il faut revoir la segmentation ou son utilisation op√©rationnelle.\n:::\n\n## 8. R√©sum√© et Bonnes Pratiques\n\n::: {.callout-important icon=false}\n## Checklist des √©tapes d'un projet de clustering\n\n‚úÖ **1. Compr√©hension du probl√®me m√©tier**\n   - Quel est l'objectif business ?\n   - Comment les clusters seront-ils utilis√©s ?\n\n‚úÖ **2. Exploration et pr√©traitement**\n   - Analyse des distributions\n   - Traitement des valeurs manquantes\n   - Normalisation/standardisation\n\n‚úÖ **3. D√©termination du nombre de clusters**\n   - M√©thode du coude\n   - Score silhouette\n   - Analyse de stabilit√©\n\n‚úÖ **4. Application des algorithmes**\n   - Test de plusieurs m√©thodes\n   - Ajustement des hyperparam√®tres\n   - Validation des r√©sultats\n\n‚úÖ **5. √âvaluation et interpr√©tation**\n   - M√©triques internes (silhouette, etc.)\n   - Visualisation (PCA, t-SNE)\n   - Profilage des clusters\n   - Interpr√©tation m√©tier\n\n‚úÖ **6. D√©ploiement et monitoring**\n   - Documentation des segments\n   - Mise √† jour p√©riodique\n   - Suivi de la stabilit√© des clusters\n:::\n\n## 9. Ressources Compl√©mentaires\n\n1. [Scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html)\n2. [Interactive Clustering Visualization](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\n3. [PCA vs t-SNE Explained](https://towardsdatascience.com/pca-vs-t-sne-257d2b9cc7cb)\n4. [Customer Segmentation Case Study](https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3)\n\n---\n\n**Fichiers √† rendre**:\n1. Notebook Jupyter complet avec code et commentaires\n2. Rapport d'analyse (1-2 pages) incluant :\n   - M√©thodologie choisie\n   - R√©sultats obtenus\n   - Visualisations cl√©s\n   - Recommandations m√©tier\n\n",
    "supporting": [
      "seance10_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}