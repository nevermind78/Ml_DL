{
  "hash": "d4e7888f066c42d97b81c13d9c1af970",
  "result": {
    "engine": "jupyter",
    "markdown": "# S√©ance 4: TD1 - Mod√®les de Classification de Base\n\n::: {.callout-note icon=false}\n## Informations de la s√©ance\n- **Type**: Travaux Dirig√©s\n- **Dur√©e**: 2h\n- **Objectifs**: Obj4, Obj6\n:::\n\n## Introduction\n\nCe TD vous permet d'approfondir votre compr√©hension th√©orique et pratique des principaux algorithmes de classification. Vous allez travailler sur des exercices conceptuels et des probl√®mes appliqu√©s.\n\n## Partie 1: Arbres de D√©cision\n\n### Exercice 1.1: Construction d'un Arbre\n\nConsid√©rez le dataset suivant pour pr√©dire si un client va acheter un ordinateur:\n\n| Age | Revenu | √âtudiant | Cr√©dit | Ach√®te |\n|-----|--------|----------|--------|--------|\n| Jeune | √âlev√© | Non | Excellent | Non |\n| Jeune | √âlev√© | Non | Excellent | Non |\n| Moyen | √âlev√© | Non | Excellent | Oui |\n| Senior | Moyen | Non | Excellent | Oui |\n| Senior | Faible | Oui | Excellent | Oui |\n| Senior | Faible | Oui | Bon | Non |\n| Moyen | Faible | Oui | Bon | Oui |\n| Jeune | Moyen | Non | Excellent | Non |\n| Jeune | Faible | Oui | Excellent | Oui |\n| Senior | Moyen | Oui | Excellent | Oui |\n| Jeune | Moyen | Oui | Bon | Oui |\n| Moyen | Moyen | Non | Bon | Oui |\n| Moyen | √âlev√© | Oui | Excellent | Oui |\n| Senior | Moyen | Non | Bon | Non |\n\n**Questions:**\n\n1. Calculez l'entropie initiale du dataset\n2. Calculez le gain d'information pour chaque attribut (Age, Revenu, √âtudiant, Cr√©dit)\n3. Quel attribut sera choisi comme racine de l'arbre ?\n4. Dessinez l'arbre de d√©cision complet\n\n::: {.callout-tip collapse=\"true\"}\n## Rappels - Formules et Algorithme\n\n**Entropie:**\n$$H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n\n**Gain d'Information:**\n$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n\no√π:\n\n- $S$ = ensemble de donn√©es\n- $A$ = attribut\n- $c$ = nombre de classes\n- $p_i$ = proportion de la classe $i$\n- $S_v$ = sous-ensemble o√π $A = v$\n\n**Algorithme ID3 (construction de l'arbre) :**\n\n1. Si tous les exemples appartiennent √† la m√™me classe ‚Üí cr√©er une **feuille** avec cette classe\n2. Si plus aucun attribut √† tester ‚Üí cr√©er une **feuille** avec la classe majoritaire\n3. Sinon :\n   - Calculer $IG(S, A)$ pour chaque attribut $A$\n   - Choisir l'attribut $A^*$ avec le **gain d'information maximal**\n   - Cr√©er un **n≈ìud** de d√©cision sur $A^*$\n   - Pour chaque valeur $v$ de $A^*$ :\n     - Cr√©er une branche pour $S_v = \\{x \\in S \\mid A^*(x) = v\\}$\n     - **Appliquer r√©cursivement** l'algorithme sur $S_v$ sans l'attribut $A^*$\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 1.1\n\n::: {#cc2bbc65 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nfrom collections import Counter\n\n# Donn√©es\ndata = [\n    ('Jeune', '√âlev√©', 'Non', 'Excellent', 'Non'),\n    ('Jeune', '√âlev√©', 'Non', 'Excellent', 'Non'),\n    ('Moyen', '√âlev√©', 'Non', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Non', 'Excellent', 'Oui'),\n    ('Senior', 'Faible', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Faible', 'Oui', 'Bon', 'Non'),\n    ('Moyen', 'Faible', 'Oui', 'Bon', 'Oui'),\n    ('Jeune', 'Moyen', 'Non', 'Excellent', 'Non'),\n    ('Jeune', 'Faible', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Oui', 'Excellent', 'Oui'),\n    ('Jeune', 'Moyen', 'Oui', 'Bon', 'Oui'),\n    ('Moyen', 'Moyen', 'Non', 'Bon', 'Oui'),\n    ('Moyen', '√âlev√©', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Non', 'Bon', 'Non')\n]\n\ndef entropy(labels):\n    \"\"\"Calcule l'entropie\"\"\"\n    counter = Counter(labels)\n    total = len(labels)\n    ent = 0\n    for count in counter.values():\n        p = count / total\n        if p > 0:\n            ent -= p * np.log2(p)\n    return ent\n\ndef information_gain(data, attr_idx):\n    \"\"\"Calcule le gain d'information\"\"\"\n    # Entropie initiale\n    labels = [row[-1] for row in data]\n    h_s = entropy(labels)\n    \n    # Partition par attribut\n    partitions = {}\n    for row in data:\n        attr_value = row[attr_idx]\n        if attr_value not in partitions:\n            partitions[attr_value] = []\n        partitions[attr_value].append(row[-1])\n    \n    # Entropie pond√©r√©e\n    h_s_a = 0\n    total = len(data)\n    for partition_labels in partitions.values():\n        p = len(partition_labels) / total\n        h_s_a += p * entropy(partition_labels)\n    \n    return h_s - h_s_a\n\n# 1. Entropie initiale\nlabels = [row[-1] for row in data]\nprint(f\"1. Entropie initiale: {entropy(labels):.4f}\")\n\n# 2. Gain d'information pour chaque attribut\nattributes = ['Age', 'Revenu', '√âtudiant', 'Cr√©dit']\nprint(\"\\n2. Gain d'information:\")\ngains = {}\nfor idx, attr in enumerate(attributes):\n    ig = information_gain(data, idx)\n    gains[attr] = ig\n    print(f\"   {attr}: {ig:.4f}\")\n\n# 3. Meilleur attribut\nbest_attr = max(gains, key=gains.get)\nprint(f\"\\n3. Attribut racine: {best_attr} (IG = {gains[best_attr]:.4f})\")\n\n# 4. L'arbre complet n√©cessiterait une impl√©mentation r√©cursive\nprint(\"\\n4. Arbre de d√©cision (structure simplifi√©e):\")\nprint(\"\"\"\n         Age?\n        /    |    \\\\\n    Jeune  Moyen  Senior\n      |      |      |\n    [Classes selon donn√©es]\n\"\"\")\n```\n:::\n\n\n**R√©ponses:**\n\n1. Entropie initiale $\\approx$ 0.940\n2. Gains d'information:\n   - Age: ~0.246\n   - Revenu: ~0.029\n   - √âtudiant: ~0.151\n   - Cr√©dit: ~0.048\n3. **Age** sera choisi comme racine (gain le plus √©lev√©)\n:::\n\n### Exercice 1.2: Overfitting dans les Arbres\n\n**Question:** Expliquez pourquoi un arbre de d√©cision sans contraintes (profondeur illimit√©e) tend √† faire de l'overfitting.\n\n**Proposez 3 m√©thodes pour limiter l'overfitting dans les arbres de d√©cision.**\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 1.2\n\n**Pourquoi l'overfitting ?**\n\nUn arbre sans contraintes va cr√©er des branches jusqu'√† ce que chaque feuille soit \"pure\" (contient une seule classe). Cela signifie:\n- L'arbre m√©morise les donn√©es d'entra√Ænement, y compris le bruit\n- Il cr√©e des r√®gles tr√®s sp√©cifiques qui ne g√©n√©ralisent pas\n- La complexit√© du mod√®le est trop √©lev√©e par rapport aux donn√©es\n\n**3 m√©thodes pour limiter l'overfitting:**\n\n1. **Pr√©-√©lagage (Pre-pruning)**:\n   - `max_depth`: Limiter la profondeur maximale\n   - `min_samples_split`: Nombre minimum d'√©chantillons pour diviser un n≈ìud\n   - `min_samples_leaf`: Nombre minimum d'√©chantillons dans une feuille\n   - `max_leaf_nodes`: Nombre maximum de feuilles\n\n2. **Post-√©lagage (Post-pruning)**:\n   - Construire l'arbre complet\n   - √âlaguer les branches qui n'apportent pas assez d'am√©lioration\n   - Utiliser un ensemble de validation pour guider l'√©lagage\n\n3. **Ensemble Methods**:\n   - Random Forest: moyenne de plusieurs arbres\n   - Gradient Boosting: construction it√©rative d'arbres\n   - Bagging: bootstrap + agr√©gation\n\n::: {#23e1f71d .cell execution_count=2}\n``` {.python .cell-code}\n# Exemple pratique\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Arbre avec overfitting\ntree_overfit = DecisionTreeClassifier()  # Pas de contraintes\n\n# Arbre r√©gularis√©\ntree_regularized = DecisionTreeClassifier(\n    max_depth=5,              # Profondeur max\n    min_samples_split=10,     # Min √©chantillons pour split\n    min_samples_leaf=5,       # Min √©chantillons par feuille\n    max_leaf_nodes=20         # Max feuilles\n)\n```\n:::\n\n\n:::\n\n## Partie 2: R√©gression Logistique\n\n### Exercice 2.1: Intuition Probabiliste\n\nConsid√©rez le mod√®le de r√©gression logistique suivant pour pr√©dire l'admission √† l'universit√©:\n\n$$P(admission=1|score) = \\frac{1}{1 + e^{-(0.05 \\times score - 3)}}$$\n\n**Questions:**\n\n1. Quelle est la probabilit√© d'admission pour un score de 60?\n2. Quelle est la probabilit√© d'admission pour un score de 80?\n3. Quel score donne une probabilit√© d'admission de 50%?\n4. Interpr√©tez le coefficient 0.05\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 2.1\n\n::: {#1e0e9dcb .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    \"\"\"Fonction sigmo√Øde\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef prob_admission(score):\n    \"\"\"Probabilit√© d'admission\"\"\"\n    z = 0.05 * score - 3\n    return sigmoid(z)\n\n# 1. Probabilit√© pour score = 60\np_60 = prob_admission(60)\nprint(f\"1. P(admission|score=60) = {p_60:.4f} = {p_60*100:.2f}%\")\n\n# 2. Probabilit√© pour score = 80\np_80 = prob_admission(80)\nprint(f\"2. P(admission|score=80) = {p_80:.4f} = {p_80*100:.2f}%\")\n\n# 3. Score pour P = 0.5\n# 0.5 = 1/(1 + e^(-(0.05*score - 3)))\n# => 0.05*score - 3 = 0\n# => score = 60\nscore_50 = 3 / 0.05\nprint(f\"3. Score pour P=50%: {score_50}\")\n\n# 4. Interpr√©tation du coefficient\nprint(f\"\\n4. Coefficient 0.05:\")\nprint(f\"   - Augmenter le score de 1 point augmente z de 0.05\")\nprint(f\"   - Cela augmente les log-odds de 0.05\")\nprint(f\"   - Odds ratio = e^0.05 = {np.exp(0.05):.4f}\")\n\n# Visualisation\nscores = np.linspace(0, 100, 1000)\nprobs = [prob_admission(s) for s in scores]\n\nplt.figure(figsize=(10, 6))\nplt.plot(scores, probs, linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='P = 0.5')\nplt.axvline(x=60, color='r', linestyle='--', alpha=0.7, label='Score = 60')\nplt.scatter([60, 80], [p_60, p_80], color='red', s=100, zorder=5)\nplt.xlabel('Score')\nplt.ylabel('Probabilit√© d\\'admission')\nplt.title('R√©gression Logistique - Admission √† l\\'universit√©')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**R√©ponses:**\n\n1. P(admission|score=60) = 0.50 = 50%\n2. P(admission|score=80) = 0.73 = 73%\n3. Score pour P=50%: **60**\n4. Le coefficient 0.05 indique qu'augmenter le score de 1 point multiplie les odds d'admission par e^0.05 $\\approx$ 1.051 (5.1% d'augmentation)\n:::\n\n### Exercice 2.2: R√©gression Logistique Multiclasse\n\nExpliquez comment adapter la r√©gression logistique pour un probl√®me multiclasse (ex: 3 classes). Quelles sont les deux approches principales?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 2.2\n\n**Deux approches pour la classification multiclasse:**\n\n### 1. One-vs-Rest (OvR) ou One-vs-All (OvA)\n\n**Principe:**\n\n- Entra√Æner K mod√®les binaires (K = nombre de classes)\n- Chaque mod√®le s√©pare une classe vs toutes les autres\n- Pr√©diction: choisir la classe avec la probabilit√© la plus √©lev√©e\n\n**Exemple avec 3 classes:**\n\n- Mod√®le 1: Classe A vs (B, C)\n- Mod√®le 2: Classe B vs (A, C)\n- Mod√®le 3: Classe C vs (A, B)\n\n::: {#7b5aee77 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\n# One-vs-Rest (par d√©faut dans scikit-learn)\novr_model = LogisticRegression(multi_class='ovr')\novr_model.fit(X_train, y_train)\n```\n:::\n\n\n### 2. Softmax (ou Multinomial)\n\n**Principe:**\n\n- Un seul mod√®le qui produit K probabilit√©s (une par classe)\n- Utilise la fonction softmax au lieu de sigmoid\n- Les probabilit√©s somment √† 1\n\n**Fonction Softmax:**\n$$P(y=k|X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n\no√π $z_k = w_k^T X + b_k$\n\n::: {#60fb30d1 .cell execution_count=5}\n``` {.python .cell-code}\n# Softmax / Multinomial\nsoftmax_model = LogisticRegression(multi_class='multinomial')\nsoftmax_model.fit(X_train, y_train)\n```\n:::\n\n\n**Comparaison:**\n\n| Crit√®re | One-vs-Rest | Softmax |\n|---------|-------------|---------|\n| Nombre de mod√®les | K mod√®les | 1 mod√®le |\n| Probabilit√©s | Peuvent d√©passer 1 (total) | Somment √† 1 |\n| Entra√Ænement | Plus rapide | Plus lent |\n| Performance | G√©n√©ralement similaire | L√©g√®rement meilleur |\n| Calibration | Moins bonne | Meilleure |\n\n:::\n\n## Partie 3: k-Nearest Neighbors\n\n### Exercice 3.1: Distance et Voisinage\n\nConsid√©rez les points suivants dans un espace 2D:\n\n| Point | x1 | x2 | Classe |\n|-------|----|----|--------|\n| A | 2 | 3 | Rouge |\n| B | 3 | 4 | Rouge |\n| C | 5 | 6 | Bleu |\n| D | 5 | 4 | Bleu |\n| E | 7 | 8 | Bleu |\n\nNouveau point: **P (4, 5)**\n\n**Questions:**\n\n1. Calculez la distance euclidienne entre P et chaque point\n2. Avec k=3, quelle classe sera pr√©dite pour P?\n3. Que se passerait-il avec k=5?\n4. Pourquoi est-il important de normaliser les donn√©es avant d'utiliser k-NN?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 3.1\n\n::: {#f96f4070 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nfrom collections import Counter\n\n# Points\npoints = {\n    'A': ([2, 3], 'Rouge'),\n    'B': ([3, 4], 'Rouge'),\n    'C': ([5, 6], 'Bleu'),\n    'D': ([5, 4], 'Bleu'),\n    'E': ([7, 8], 'Bleu')\n}\n\n# Nouveau point\nP = np.array([4, 5])\n\n# 1. Distances euclidiennes\nprint(\"1. Distances euclidiennes:\")\ndistances = {}\nfor name, (coords, classe) in points.items():\n    dist = np.sqrt(np.sum((np.array(coords) - P)**2))\n    distances[name] = (dist, classe)\n    print(f\"   P ‚Üí {name}: {dist:.4f} (classe: {classe})\")\n\n# 2. k=3\nprint(\"\\n2. k=3:\")\nsorted_distances = sorted(distances.items(), key=lambda x: x[1][0])\nk3_neighbors = sorted_distances[:3]\nk3_classes = [classe for _, (_, classe) in k3_neighbors]\nk3_prediction = Counter(k3_classes).most_common(1)[0][0]\nprint(f\"   3 voisins les plus proches: {[n[0] for n in k3_neighbors]}\")\nprint(f\"   Classes: {k3_classes}\")\nprint(f\"   Pr√©diction: {k3_prediction}\")\n\n# 3. k=5\nprint(\"\\n3. k=5:\")\nk5_neighbors = sorted_distances[:5]\nk5_classes = [classe for _, (_, classe) in k5_neighbors]\nk5_prediction = Counter(k5_classes).most_common(1)[0][0]\nprint(f\"   5 voisins les plus proches: {[n[0] for n in k5_neighbors]}\")\nprint(f\"   Classes: {k5_classes}\")\nprint(f\"   Pr√©diction: {k5_prediction}\")\n\n# 4. Importance de la normalisation\nprint(\"\\n4. Importance de la normalisation:\")\nprint(\"   Sans normalisation, une feature avec une grande √©chelle\")\nprint(\"   dominera le calcul de distance.\")\nprint(\"\\n   Exemple:\")\nprint(\"   - Feature 1 (√¢ge): 20-80 ‚Üí √©chelle ~60\")\nprint(\"   - Feature 2 (revenu): 20000-100000 ‚Üí √©chelle ~80000\")\nprint(\"   ‚Üí La distance sera domin√©e par le revenu!\")\n```\n:::\n\n\n**R√©ponses:**\n\n1. Distances:\n   - P ‚Üí A: 2.828\n   - P ‚Üí B: 1.414 (plus proche)\n   - P ‚Üí C: 1.414 (plus proche)\n   - P ‚Üí D: 1.414 (plus proche)\n   - P ‚Üí E: 4.243\n\n2. Avec k=3: Les 3 voisins sont B (Rouge), C (Bleu), D (Bleu)\n   - Vote: 1 Rouge, 2 Bleus\n   - **Pr√©diction: Bleu**\n\n3. Avec k=5: Tous les points\n   - Vote: 2 Rouges, 3 Bleus\n   - **Pr√©diction: Bleu** (m√™me r√©sultat)\n\n4. **Normalisation importante** car:\n   - Les features avec de grandes valeurs dominent le calcul de distance\n   - Exemple: Si une feature est en milliers et l'autre en dizaines, la premi√®re √©crasera la seconde\n   - Solution: StandardScaler ou MinMaxScaler\n:::\n\n## Partie 4: Naive Bayes\n\n### Exercice 4.1: Application du Th√©or√®me de Bayes\n\nDataset pour classification de courriels:\n\n| Courriel | \"gratuit\" | \"argent\" | \"viagra\" | Classe |\n|----------|-----------|----------|----------|--------|\n| 1 | Oui | Non | Non | Spam |\n| 2 | Oui | Oui | Oui | Spam |\n| 3 | Non | Non | Non | Ham |\n| 4 | Non | Non | Non | Ham |\n| 5 | Oui | Oui | Non | Spam |\n\nNouveau courriel contient: **\"gratuit\"** et **\"argent\"**\n\n**Calculez P(Spam | gratuit, argent) et P(Ham | gratuit, argent)**\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 4.1\n\n::: {#26d8826e .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Donn√©es\nemails = [\n    {'gratuit': 1, 'argent': 0, 'viagra': 0, 'classe': 'Spam'},\n    {'gratuit': 1, 'argent': 1, 'viagra': 1, 'classe': 'Spam'},\n    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n    {'gratuit': 1, 'argent': 1, 'viagra': 0, 'classe': 'Spam'},\n]\n\n# Probabilit√©s a priori\nn_spam = sum(1 for e in emails if e['classe'] == 'Spam')\nn_ham = sum(1 for e in emails if e['classe'] == 'Ham')\ntotal = len(emails)\n\np_spam = n_spam / total\np_ham = n_ham / total\n\nprint(\"Probabilit√©s a priori:\")\nprint(f\"P(Spam) = {n_spam}/{total} = {p_spam}\")\nprint(f\"P(Ham) = {n_ham}/{total} = {p_ham}\")\n\n# Probabilit√©s conditionnelles pour Spam\nspam_emails = [e for e in emails if e['classe'] == 'Spam']\np_gratuit_spam = sum(e['gratuit'] for e in spam_emails) / n_spam\np_argent_spam = sum(e['argent'] for e in spam_emails) / n_spam\n\nprint(f\"\\nP(gratuit|Spam) = {p_gratuit_spam}\")\nprint(f\"P(argent|Spam) = {p_argent_spam}\")\n\n# Probabilit√©s conditionnelles pour Ham\nham_emails = [e for e in emails if e['classe'] == 'Ham']\np_gratuit_ham = sum(e['gratuit'] for e in ham_emails) / n_ham\np_argent_ham = sum(e['argent'] for e in ham_emails) / n_ham\n\nprint(f\"\\nP(gratuit|Ham) = {p_gratuit_ham}\")\nprint(f\"P(argent|Ham) = {p_argent_ham}\")\n\n# Calcul Naive Bayes (hypoth√®se d'ind√©pendance)\n# P(Spam | gratuit, argent) $\\propto$ P(gratuit|Spam) * P(argent|Spam) * P(Spam)\nnumerator_spam = p_gratuit_spam * p_argent_spam * p_spam\nnumerator_ham = p_gratuit_ham * p_argent_ham * p_ham\n\n# Normalisation\np_spam_given_words = numerator_spam / (numerator_spam + numerator_ham)\np_ham_given_words = numerator_ham / (numerator_spam + numerator_ham)\n\nprint(f\"\\nR√©sultats:\")\nprint(f\"P(Spam | gratuit, argent) = {p_spam_given_words:.4f}\")\nprint(f\"P(Ham | gratuit, argent) = {p_ham_given_words:.4f}\")\nprint(f\"\\nPr√©diction: {'Spam' if p_spam_given_words > p_ham_given_words else 'Ham'}\")\n```\n:::\n\n\n**R√©solution manuelle:**\n\n**√âtape 1: Probabilit√©s a priori**\n\n- P(Spam) = 3/5 = 0.6\n- P(Ham) = 2/5 = 0.4\n\n**√âtape 2: Probabilit√©s conditionnelles**\n\nPour Spam:\n\n- P(gratuit|Spam) = 3/3 = 1.0\n- P(argent|Spam) = 2/3 $\\approx$ 0.67\n\nPour Ham:\n\n- P(gratuit|Ham) = 0/2 = 0\n- P(argent|Ham) = 0/2 = 0\n\n**√âtape 3: Application de Bayes**\n\nP(Spam | gratuit, argent) $\\propto$ 1.0 √ó 0.67 √ó 0.6 = 0.4\n\nP(Ham | gratuit, argent) $\\propto$ 0 √ó 0 √ó 0.4 = 0\n\n**Pr√©diction: Spam** (avec 100% de confiance)\n:::\n\n## Partie 5: Gradient Boosting (XGBoost/LightGBM)\n\n### Exercice 5.1: Comprendre le Boosting\n\n**Questions conceptuelles:**\n\n1. Quelle est la diff√©rence fondamentale entre Random Forest (Bagging) et Gradient Boosting?\n2. Pourquoi le Gradient Boosting est-il plus sensible √† l'overfitting que Random Forest?\n3. Quels sont les 3 hyperparam√®tres les plus importants √† ajuster pour XGBoost/LightGBM?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 5.1\n\nüìé **Lien du Slide  :** [Gradient Boosting ‚Äî Classification](https://nevermind78.github.io/AN_slides/GB_C.html#/title-slide)\n\n\n**1. Diff√©rence Bagging vs Boosting:**\n\n| Aspect | Random Forest (Bagging) | Gradient Boosting |\n|--------|------------------------|-------------------|\n| **Construction** | Parall√®le (arbres ind√©pendants) | S√©quentielle (arbres d√©pendants) |\n| **Objectif** | R√©duire la variance | R√©duire le biais |\n| **Donn√©es** | Bootstrap (√©chantillonnage) | Totalit√© des donn√©es |\n| **Poids** | Tous arbres √©gaux | Arbres pond√©r√©s |\n| **Pr√©diction** | Moyenne simple | Somme pond√©r√©e |\n| **Focus** | Erreurs al√©atoires | Erreurs r√©siduelles |\n\n\n**Diagramme :**\n\n```{mermaid}\ngraph LR\n    A[Random Forest] --> B[Arbre 1]\n    A --> C[Arbre 2]\n    A --> D[Arbre N]\n    B --> E[Vote]\n```\n\n\n**2. Sensibilit√© √† l'overfitting:**\n\nGradient Boosting est plus sensible car:\n- Chaque arbre se concentre sur les erreurs pr√©c√©dentes\n- Risque d'apprendre le bruit si trop d'it√©rations\n- Peut \"m√©moriser\" les cas difficiles du train set\n- Pas de randomisation par d√©faut (contrairement √† RF)\n\n**Solutions:**\n- Limiter le nombre d'arbres (`n_estimators`)\n- R√©duire le taux d'apprentissage (`learning_rate`)\n- Limiter la profondeur (`max_depth`)\n- Early stopping avec validation set\n\n**3. Hyperparam√®tres cl√©s:**\n\n::: {#d4c9fead .cell execution_count=8}\n``` {.python .cell-code}\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier(\n    # 1. Nombre d'arbres\n    n_estimators=100,  # Plus = meilleur mais risque overfitting\n    \n    # 2. Taux d'apprentissage\n    learning_rate=0.1,  # Plus faible = besoin de plus d'arbres\n                        # Typage: 0.01-0.3\n    \n    # 3. Profondeur maximale\n    max_depth=6,  # Plus profond = plus complexe\n                  # Typique: 3-10\n    \n    # Bonus importants:\n    subsample=0.8,      # √âchantillonnage des donn√©es (0.5-1.0)\n    colsample_bytree=0.8,  # √âchantillonnage des features\n    min_child_weight=1,    # R√©gularisation\n    \n    random_state=42\n)\n```\n:::\n\n\n**Recommandations de tuning:**\n\n1. **Commencer avec:**\n\n   - `learning_rate=0.1`\n   - `max_depth=6`\n   - `n_estimators=100`\n\n2. **Puis optimiser:**\n\n   - Augmenter `n_estimators` + r√©duire `learning_rate`\n   - Ajuster `max_depth` (3-10)\n   - Ajouter r√©gularisation (`subsample`, `colsample_bytree`)\n\n3. **Utiliser early stopping:**\n\n::: {#85921b05 .cell execution_count=9}\n``` {.python .cell-code}\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=10,  # Stop si pas d'am√©lioration\n    verbose=False\n)\n```\n:::\n\n\n:::\n\n## Exercices R√©capitulatifs\n\n::: {.callout-warning icon=false}\n## Exercice Final: Choix d'Algorithme\n\nPour chacun des sc√©narios suivants, recommandez un algorithme et justifiez:\n\n1. **Diagnostic m√©dical** (interpr√©tabilit√© cruciale, 1000 patients, 20 features)\n2. **D√©tection de fraude** (millions de transactions, temps r√©el, d√©s√©quilibre 99/1)\n3. **Classification d'images** (50000 images, haute dimension, GPU disponible)\n4. **Pr√©diction de churn** (10000 clients, features mixtes, besoin de probabilit√©s calibr√©es)\n5. **Classification de textes** (emails spam, 100000 emails, features = mots)\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice Final\n\n**1. Diagnostic m√©dical:**\n\n- **Recommandation**: Decision Tree ou R√©gression Logistique\n- **Justification**:\n\n  - Interpr√©tabilit√© essentielle pour les m√©decins\n  - Dataset de taille mod√©r√©e\n  - Besoin de comprendre les r√®gles de d√©cision\n  - Alternative: Random Forest + feature importance\n\n**2. D√©tection de fraude:**\n\n- **Recommandation**: XGBoost/LightGBM\n\n- **Justification**:\n\n  - Excellent avec classes d√©s√©quilibr√©es (param√®tre `scale_pos_weight`)\n  - Tr√®s rapide en pr√©diction (important pour temps r√©el)\n  - G√®re bien les grandes donn√©es\n  - Robuste et performant\n  - Peut utiliser early stopping\n\n**3. Classification d'images:**\n\n- **Recommandation**: CNN (Deep Learning) - hors scope pour l'instant\n- **Justification actuelle avec ML classique**:\n\n  - Random Forest avec features extraites (HOG, SIFT)\n  - SVM avec kernel RBF\n  - Mais performances limit√©es vs Deep Learning\n\n**4. Pr√©diction de churn:**\n\n- **Recommandation**: R√©gression Logistique ou Gradient Boosting\n- **Justification**:\n\n  - Logistic Regression: probabilit√©s bien calibr√©es, interpr√©table\n  - Gradient Boosting: meilleures performances, feature importance\n  - Dataset de taille moyenne\n  - Features mixtes g√©r√©es par les deux\n\n**5. Classification de textes:**\n\n- **Recommandation**: Naive Bayes (Multinomial)\n- **Justification**:\n\n  - Tr√®s performant pour la classification de texte\n  - Rapide √† entra√Æner et pr√©dire\n  - G√®re bien les grandes dimensions (nombreux mots)\n  - Probabilit√©s natives\n  - Alternative: R√©gression Logistique\n:::\n\n\n## R√©sum√© du TD\n\n::: {.callout-important icon=false}\n## Points cl√©s √† retenir\n\n### Algorithmes et leurs forces\n\n1. **Decision Tree**\n   - $\\checkmark$ Interpr√©table, visuel\n   - X Overfitting, instable\n\n2. **Random Forest**\n   - $\\checkmark$ Robuste, performant\n   - X Moins interpr√©table, m√©moire\n\n3. **SVM**\n   - $\\checkmark$ Excellent en haute dimension\n   - X Lent, difficile √† interpr√©ter\n\n4. **Naive Bayes**\n   - $\\checkmark$ Rapide, bon pour texte\n   - X Hypoth√®se d'ind√©pendance forte\n\n5. **R√©gression Logistique**\n   - $\\checkmark$ Probabilit√©s calibr√©es, interpr√©table\n   - X Assume lin√©arit√©\n\n6. **k-NN**\n   - $\\checkmark$ Simple, pas de training\n   - X Lent en pr√©diction, besoin normalisation\n\n7. **Gradient Boosting**\n   - $\\checkmark$ Tr√®s performant, g√®re d√©s√©quilibre\n   - X Sensible overfitting, plus complexe\n:::\n\n## Pour le Prochain Cours\n\nPr√©parez-vous pour le **TD2 sur les Crit√®res d'√âvaluation** o√π nous approfondirons:\n- Matrice de confusion\n- Precision, Recall, F1-score\n- Courbe ROC et AUC\n- Choix de m√©triques selon le contexte\n\n## Ressources Compl√©mentaires\n\n1. [Scikit-learn: Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/)\n2. [StatQuest: Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n3. [XGBoost Documentation](https://xgboost.readthedocs.io/)</parameter>\n\n## Correction\n\nüìé **Lien de la correction  :** [TD1 ‚Äî Mod√®les de Classification de Base](https://nevermind78.github.io/AN_slides/td1_corr_eng.html#/title-slide)\n\n",
    "supporting": [
      "seance4_files"
    ],
    "filters": [],
    "includes": {}
  }
}