{
  "hash": "b55ef2898b8a70f5ddfbfe73c94bf6d4",
  "result": {
    "engine": "jupyter",
    "markdown": "# Séance 4: TD1 - Modèles de Classification de Base\n\n::: {.callout-note icon=false}\n## Informations de la séance\n- **Type**: Travaux Dirigés\n- **Durée**: 2h\n- **Objectifs**: Obj4, Obj6\n:::\n\n## Introduction\n\nCe TD vous permet d'approfondir votre compréhension théorique et pratique des principaux algorithmes de classification. Vous allez travailler sur des exercices conceptuels et des problèmes appliqués.\n\n## Partie 1: Arbres de Décision\n\n### Exercice 1.1: Construction d'un Arbre\n\nConsidérez le dataset suivant pour prédire si un client va acheter un ordinateur:\n\n| Age | Revenu | Étudiant | Crédit | Achète |\n|-----|--------|----------|--------|--------|\n| Jeune | Élevé | Non | Excellent | Non |\n| Jeune | Élevé | Non | Excellent | Non |\n| Moyen | Élevé | Non | Excellent | Oui |\n| Senior | Moyen | Non | Excellent | Oui |\n| Senior | Faible | Oui | Excellent | Oui |\n| Senior | Faible | Oui | Bon | Non |\n| Moyen | Faible | Oui | Bon | Oui |\n| Jeune | Moyen | Non | Excellent | Non |\n| Jeune | Faible | Oui | Excellent | Oui |\n| Senior | Moyen | Oui | Excellent | Oui |\n| Jeune | Moyen | Oui | Bon | Oui |\n| Moyen | Moyen | Non | Bon | Oui |\n| Moyen | Élevé | Oui | Excellent | Oui |\n| Senior | Moyen | Non | Bon | Non |\n\n**Questions:**\n\n1. Calculez l'entropie initiale du dataset\n2. Calculez le gain d'information pour chaque attribut (Age, Revenu, Étudiant, Crédit)\n3. Quel attribut sera choisi comme racine de l'arbre ?\n4. Dessinez l'arbre de décision complet\n\n::: {.callout-tip collapse=\"true\"}\n## Rappels - Formules\n\n**Entropie:**\n$$H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n\n**Gain d'Information:**\n$$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n\noù:\n- $S$ = ensemble de données\n- $A$ = attribut\n- $c$ = nombre de classes\n- $p_i$ = proportion de la classe $i$\n- $S_v$ = sous-ensemble où $A = v$\n:::\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 1.1\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nfrom collections import Counter\n\n# Données\ndata = [\n    ('Jeune', 'Élevé', 'Non', 'Excellent', 'Non'),\n    ('Jeune', 'Élevé', 'Non', 'Excellent', 'Non'),\n    ('Moyen', 'Élevé', 'Non', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Non', 'Excellent', 'Oui'),\n    ('Senior', 'Faible', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Faible', 'Oui', 'Bon', 'Non'),\n    ('Moyen', 'Faible', 'Oui', 'Bon', 'Oui'),\n    ('Jeune', 'Moyen', 'Non', 'Excellent', 'Non'),\n    ('Jeune', 'Faible', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Oui', 'Excellent', 'Oui'),\n    ('Jeune', 'Moyen', 'Oui', 'Bon', 'Oui'),\n    ('Moyen', 'Moyen', 'Non', 'Bon', 'Oui'),\n    ('Moyen', 'Élevé', 'Oui', 'Excellent', 'Oui'),\n    ('Senior', 'Moyen', 'Non', 'Bon', 'Non')\n]\n\ndef entropy(labels):\n    \"\"\"Calcule l'entropie\"\"\"\n    counter = Counter(labels)\n    total = len(labels)\n    ent = 0\n    for count in counter.values():\n        p = count / total\n        if p > 0:\n            ent -= p * np.log2(p)\n    return ent\n\ndef information_gain(data, attr_idx):\n    \"\"\"Calcule le gain d'information\"\"\"\n    # Entropie initiale\n    labels = [row[-1] for row in data]\n    h_s = entropy(labels)\n    \n    # Partition par attribut\n    partitions = {}\n    for row in data:\n        attr_value = row[attr_idx]\n        if attr_value not in partitions:\n            partitions[attr_value] = []\n        partitions[attr_value].append(row[-1])\n    \n    # Entropie pondérée\n    h_s_a = 0\n    total = len(data)\n    for partition_labels in partitions.values():\n        p = len(partition_labels) / total\n        h_s_a += p * entropy(partition_labels)\n    \n    return h_s - h_s_a\n\n# 1. Entropie initiale\nlabels = [row[-1] for row in data]\nprint(f\"1. Entropie initiale: {entropy(labels):.4f}\")\n\n# 2. Gain d'information pour chaque attribut\nattributes = ['Age', 'Revenu', 'Étudiant', 'Crédit']\nprint(\"\\n2. Gain d'information:\")\ngains = {}\nfor idx, attr in enumerate(attributes):\n    ig = information_gain(data, idx)\n    gains[attr] = ig\n    print(f\"   {attr}: {ig:.4f}\")\n\n# 3. Meilleur attribut\nbest_attr = max(gains, key=gains.get)\nprint(f\"\\n3. Attribut racine: {best_attr} (IG = {gains[best_attr]:.4f})\")\n\n# 4. L'arbre complet nécessiterait une implémentation récursive\nprint(\"\\n4. Arbre de décision (structure simplifiée):\")\nprint(\"\"\"\n         Age?\n        /    |    \\\\\n    Jeune  Moyen  Senior\n      |      |      |\n    [Classes selon données]\n\"\"\")\n```\n:::\n\n\n**Réponses:**\n\n1. Entropie initiale $\\approx$ 0.940\n2. Gains d'information:\n   - Age: ~0.246\n   - Revenu: ~0.029\n   - Étudiant: ~0.151\n   - Crédit: ~0.048\n3. **Age** sera choisi comme racine (gain le plus élevé)\n:::\n\n### Exercice 1.2: Overfitting dans les Arbres\n\n**Question:** Expliquez pourquoi un arbre de décision sans contraintes (profondeur illimitée) tend à faire de l'overfitting.\n\n**Proposez 3 méthodes pour limiter l'overfitting dans les arbres de décision.**\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 1.2\n\n**Pourquoi l'overfitting ?**\n\nUn arbre sans contraintes va créer des branches jusqu'à ce que chaque feuille soit \"pure\" (contient une seule classe). Cela signifie:\n- L'arbre mémorise les données d'entraînement, y compris le bruit\n- Il crée des règles très spécifiques qui ne généralisent pas\n- La complexité du modèle est trop élevée par rapport aux données\n\n**3 méthodes pour limiter l'overfitting:**\n\n1. **Pré-élagage (Pre-pruning)**:\n   - `max_depth`: Limiter la profondeur maximale\n   - `min_samples_split`: Nombre minimum d'échantillons pour diviser un nœud\n   - `min_samples_leaf`: Nombre minimum d'échantillons dans une feuille\n   - `max_leaf_nodes`: Nombre maximum de feuilles\n\n2. **Post-élagage (Post-pruning)**:\n   - Construire l'arbre complet\n   - Élaguer les branches qui n'apportent pas assez d'amélioration\n   - Utiliser un ensemble de validation pour guider l'élagage\n\n3. **Ensemble Methods**:\n   - Random Forest: moyenne de plusieurs arbres\n   - Gradient Boosting: construction itérative d'arbres\n   - Bagging: bootstrap + agrégation\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Exemple pratique\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Arbre avec overfitting\ntree_overfit = DecisionTreeClassifier()  # Pas de contraintes\n\n# Arbre régularisé\ntree_regularized = DecisionTreeClassifier(\n    max_depth=5,              # Profondeur max\n    min_samples_split=10,     # Min échantillons pour split\n    min_samples_leaf=5,       # Min échantillons par feuille\n    max_leaf_nodes=20         # Max feuilles\n)\n```\n:::\n\n\n:::\n\n## Partie 2: Régression Logistique\n\n### Exercice 2.1: Intuition Probabiliste\n\nConsidérez le modèle de régression logistique suivant pour prédire l'admission à l'université:\n\n$$P(admission=1|score) = \\frac{1}{1 + e^{-(0.05 \\times score - 3)}}$$\n\n**Questions:**\n\n1. Quelle est la probabilité d'admission pour un score de 60?\n2. Quelle est la probabilité d'admission pour un score de 80?\n3. Quel score donne une probabilité d'admission de 50%?\n4. Interprétez le coefficient 0.05\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 2.1\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    \"\"\"Fonction sigmoïde\"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef prob_admission(score):\n    \"\"\"Probabilité d'admission\"\"\"\n    z = 0.05 * score - 3\n    return sigmoid(z)\n\n# 1. Probabilité pour score = 60\np_60 = prob_admission(60)\nprint(f\"1. P(admission|score=60) = {p_60:.4f} = {p_60*100:.2f}%\")\n\n# 2. Probabilité pour score = 80\np_80 = prob_admission(80)\nprint(f\"2. P(admission|score=80) = {p_80:.4f} = {p_80*100:.2f}%\")\n\n# 3. Score pour P = 0.5\n# 0.5 = 1/(1 + e^(-(0.05*score - 3)))\n# => 0.05*score - 3 = 0\n# => score = 60\nscore_50 = 3 / 0.05\nprint(f\"3. Score pour P=50%: {score_50}\")\n\n# 4. Interprétation du coefficient\nprint(f\"\\n4. Coefficient 0.05:\")\nprint(f\"   - Augmenter le score de 1 point augmente z de 0.05\")\nprint(f\"   - Cela augmente les log-odds de 0.05\")\nprint(f\"   - Odds ratio = e^0.05 = {np.exp(0.05):.4f}\")\n\n# Visualisation\nscores = np.linspace(0, 100, 1000)\nprobs = [prob_admission(s) for s in scores]\n\nplt.figure(figsize=(10, 6))\nplt.plot(scores, probs, linewidth=2)\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='P = 0.5')\nplt.axvline(x=60, color='r', linestyle='--', alpha=0.7, label='Score = 60')\nplt.scatter([60, 80], [p_60, p_80], color='red', s=100, zorder=5)\nplt.xlabel('Score')\nplt.ylabel('Probabilité d\\'admission')\nplt.title('Régression Logistique - Admission à l\\'université')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**Réponses:**\n\n1. P(admission|score=60) = 0.50 = 50%\n2. P(admission|score=80) = 0.73 = 73%\n3. Score pour P=50%: **60**\n4. Le coefficient 0.05 indique qu'augmenter le score de 1 point multiplie les odds d'admission par e^0.05 $\\approx$ 1.051 (5.1% d'augmentation)\n:::\n\n### Exercice 2.2: Régression Logistique Multiclasse\n\nExpliquez comment adapter la régression logistique pour un problème multiclasse (ex: 3 classes). Quelles sont les deux approches principales?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 2.2\n\n**Deux approches pour la classification multiclasse:**\n\n### 1. One-vs-Rest (OvR) ou One-vs-All (OvA)\n\n**Principe:**\n- Entraîner K modèles binaires (K = nombre de classes)\n- Chaque modèle sépare une classe vs toutes les autres\n- Prédiction: choisir la classe avec la probabilité la plus élevée\n\n**Exemple avec 3 classes:**\n\n- Modèle 1: Classe A vs (B, C)\n- Modèle 2: Classe B vs (A, C)\n- Modèle 3: Classe C vs (A, B)\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\n# One-vs-Rest (par défaut dans scikit-learn)\novr_model = LogisticRegression(multi_class='ovr')\novr_model.fit(X_train, y_train)\n```\n:::\n\n\n### 2. Softmax (ou Multinomial)\n\n**Principe:**\n- Un seul modèle qui produit K probabilités (une par classe)\n- Utilise la fonction softmax au lieu de sigmoid\n- Les probabilités somment à 1\n\n**Fonction Softmax:**\n$$P(y=k|X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n\noù $z_k = w_k^T X + b_k$\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Softmax / Multinomial\nsoftmax_model = LogisticRegression(multi_class='multinomial')\nsoftmax_model.fit(X_train, y_train)\n```\n:::\n\n\n**Comparaison:**\n\n| Critère | One-vs-Rest | Softmax |\n|---------|-------------|---------|\n| Nombre de modèles | K modèles | 1 modèle |\n| Probabilités | Peuvent dépasser 1 (total) | Somment à 1 |\n| Entraînement | Plus rapide | Plus lent |\n| Performance | Généralement similaire | Légèrement meilleur |\n| Calibration | Moins bonne | Meilleure |\n\n:::\n\n## Partie 3: k-Nearest Neighbors\n\n### Exercice 3.1: Distance et Voisinage\n\nConsidérez les points suivants dans un espace 2D:\n\n| Point | x1 | x2 | Classe |\n|-------|----|----|--------|\n| A | 2 | 3 | Rouge |\n| B | 3 | 4 | Rouge |\n| C | 5 | 6 | Bleu |\n| D | 5 | 4 | Bleu |\n| E | 7 | 8 | Bleu |\n\nNouveau point: **P (4, 5)**\n\n**Questions:**\n\n1. Calculez la distance euclidienne entre P et chaque point\n2. Avec k=3, quelle classe sera prédite pour P?\n3. Que se passerait-il avec k=5?\n4. Pourquoi est-il important de normaliser les données avant d'utiliser k-NN?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 3.1\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nfrom collections import Counter\n\n# Points\npoints = {\n    'A': ([2, 3], 'Rouge'),\n    'B': ([3, 4], 'Rouge'),\n    'C': ([5, 6], 'Bleu'),\n    'D': ([5, 4], 'Bleu'),\n    'E': ([7, 8], 'Bleu')\n}\n\n# Nouveau point\nP = np.array([4, 5])\n\n# 1. Distances euclidiennes\nprint(\"1. Distances euclidiennes:\")\ndistances = {}\nfor name, (coords, classe) in points.items():\n    dist = np.sqrt(np.sum((np.array(coords) - P)**2))\n    distances[name] = (dist, classe)\n    print(f\"   P → {name}: {dist:.4f} (classe: {classe})\")\n\n# 2. k=3\nprint(\"\\n2. k=3:\")\nsorted_distances = sorted(distances.items(), key=lambda x: x[1][0])\nk3_neighbors = sorted_distances[:3]\nk3_classes = [classe for _, (_, classe) in k3_neighbors]\nk3_prediction = Counter(k3_classes).most_common(1)[0][0]\nprint(f\"   3 voisins les plus proches: {[n[0] for n in k3_neighbors]}\")\nprint(f\"   Classes: {k3_classes}\")\nprint(f\"   Prédiction: {k3_prediction}\")\n\n# 3. k=5\nprint(\"\\n3. k=5:\")\nk5_neighbors = sorted_distances[:5]\nk5_classes = [classe for _, (_, classe) in k5_neighbors]\nk5_prediction = Counter(k5_classes).most_common(1)[0][0]\nprint(f\"   5 voisins les plus proches: {[n[0] for n in k5_neighbors]}\")\nprint(f\"   Classes: {k5_classes}\")\nprint(f\"   Prédiction: {k5_prediction}\")\n\n# 4. Importance de la normalisation\nprint(\"\\n4. Importance de la normalisation:\")\nprint(\"   Sans normalisation, une feature avec une grande échelle\")\nprint(\"   dominera le calcul de distance.\")\nprint(\"\\n   Exemple:\")\nprint(\"   - Feature 1 (âge): 20-80 → échelle ~60\")\nprint(\"   - Feature 2 (revenu): 20000-100000 → échelle ~80000\")\nprint(\"   → La distance sera dominée par le revenu!\")\n```\n:::\n\n\n**Réponses:**\n\n1. Distances:\n   - P → A: 2.828\n   - P → B: 1.414 (plus proche)\n   - P → C: 1.414 (plus proche)\n   - P → D: 1.414 (plus proche)\n   - P → E: 4.243\n\n2. Avec k=3: Les 3 voisins sont B (Rouge), C (Bleu), D (Bleu)\n   - Vote: 1 Rouge, 2 Bleus\n   - **Prédiction: Bleu**\n\n3. Avec k=5: Tous les points\n   - Vote: 2 Rouges, 3 Bleus\n   - **Prédiction: Bleu** (même résultat)\n\n4. **Normalisation importante** car:\n   - Les features avec de grandes valeurs dominent le calcul de distance\n   - Exemple: Si une feature est en milliers et l'autre en dizaines, la première écrasera la seconde\n   - Solution: StandardScaler ou MinMaxScaler\n:::\n\n## Partie 4: Naive Bayes\n\n### Exercice 4.1: Application du Théorème de Bayes\n\nDataset pour classification de courriels:\n\n| Courriel | \"gratuit\" | \"argent\" | \"viagra\" | Classe |\n|----------|-----------|----------|----------|--------|\n| 1 | Oui | Non | Non | Spam |\n| 2 | Oui | Oui | Oui | Spam |\n| 3 | Non | Non | Non | Ham |\n| 4 | Non | Non | Non | Ham |\n| 5 | Oui | Oui | Non | Spam |\n\nNouveau courriel contient: **\"gratuit\"** et **\"argent\"**\n\n**Calculez P(Spam | gratuit, argent) et P(Ham | gratuit, argent)**\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 4.1\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Données\nemails = [\n    {'gratuit': 1, 'argent': 0, 'viagra': 0, 'classe': 'Spam'},\n    {'gratuit': 1, 'argent': 1, 'viagra': 1, 'classe': 'Spam'},\n    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n    {'gratuit': 0, 'argent': 0, 'viagra': 0, 'classe': 'Ham'},\n    {'gratuit': 1, 'argent': 1, 'viagra': 0, 'classe': 'Spam'},\n]\n\n# Probabilités a priori\nn_spam = sum(1 for e in emails if e['classe'] == 'Spam')\nn_ham = sum(1 for e in emails if e['classe'] == 'Ham')\ntotal = len(emails)\n\np_spam = n_spam / total\np_ham = n_ham / total\n\nprint(\"Probabilités a priori:\")\nprint(f\"P(Spam) = {n_spam}/{total} = {p_spam}\")\nprint(f\"P(Ham) = {n_ham}/{total} = {p_ham}\")\n\n# Probabilités conditionnelles pour Spam\nspam_emails = [e for e in emails if e['classe'] == 'Spam']\np_gratuit_spam = sum(e['gratuit'] for e in spam_emails) / n_spam\np_argent_spam = sum(e['argent'] for e in spam_emails) / n_spam\n\nprint(f\"\\nP(gratuit|Spam) = {p_gratuit_spam}\")\nprint(f\"P(argent|Spam) = {p_argent_spam}\")\n\n# Probabilités conditionnelles pour Ham\nham_emails = [e for e in emails if e['classe'] == 'Ham']\np_gratuit_ham = sum(e['gratuit'] for e in ham_emails) / n_ham\np_argent_ham = sum(e['argent'] for e in ham_emails) / n_ham\n\nprint(f\"\\nP(gratuit|Ham) = {p_gratuit_ham}\")\nprint(f\"P(argent|Ham) = {p_argent_ham}\")\n\n# Calcul Naive Bayes (hypothèse d'indépendance)\n# P(Spam | gratuit, argent) $\\propto$ P(gratuit|Spam) * P(argent|Spam) * P(Spam)\nnumerator_spam = p_gratuit_spam * p_argent_spam * p_spam\nnumerator_ham = p_gratuit_ham * p_argent_ham * p_ham\n\n# Normalisation\np_spam_given_words = numerator_spam / (numerator_spam + numerator_ham)\np_ham_given_words = numerator_ham / (numerator_spam + numerator_ham)\n\nprint(f\"\\nRésultats:\")\nprint(f\"P(Spam | gratuit, argent) = {p_spam_given_words:.4f}\")\nprint(f\"P(Ham | gratuit, argent) = {p_ham_given_words:.4f}\")\nprint(f\"\\nPrédiction: {'Spam' if p_spam_given_words > p_ham_given_words else 'Ham'}\")\n```\n:::\n\n\n**Résolution manuelle:**\n\n**Étape 1: Probabilités a priori**\n\n- P(Spam) = 3/5 = 0.6\n- P(Ham) = 2/5 = 0.4\n\n**Étape 2: Probabilités conditionnelles**\n\nPour Spam:\n- P(gratuit|Spam) = 3/3 = 1.0\n- P(argent|Spam) = 2/3 $\\approx$ 0.67\n\nPour Ham:\n- P(gratuit|Ham) = 0/2 = 0\n- P(argent|Ham) = 0/2 = 0\n\n**Étape 3: Application de Bayes**\n\nP(Spam | gratuit, argent) $\\propto$ 1.0 × 0.67 × 0.6 = 0.4\nP(Ham | gratuit, argent) $\\propto$ 0 × 0 × 0.4 = 0\n\n**Prédiction: Spam** (avec 100% de confiance)\n:::\n\n## Partie 5: Gradient Boosting (XGBoost/LightGBM)\n\n### Exercice 5.1: Comprendre le Boosting\n\n**Questions conceptuelles:**\n\n1. Quelle est la différence fondamentale entre Random Forest (Bagging) et Gradient Boosting?\n2. Pourquoi le Gradient Boosting est-il plus sensible à l'overfitting que Random Forest?\n3. Quels sont les 3 hyperparamètres les plus importants à ajuster pour XGBoost/LightGBM?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 5.1\n\n**1. Différence Bagging vs Boosting:**\n\n| Aspect | Random Forest (Bagging) | Gradient Boosting |\n|--------|------------------------|-------------------|\n| **Construction** | Parallèle (arbres indépendants) | Séquentielle (arbres dépendants) |\n| **Objectif** | Réduire la variance | Réduire le biais |\n| **Données** | Bootstrap (échantillonnage) | Totalité des données |\n| **Poids** | Tous arbres égaux | Arbres pondérés |\n| **Prédiction** | Moyenne simple | Somme pondérée |\n| **Focus** | Erreurs aléatoires | Erreurs résiduelles |\n\n\n**Diagramme mermaid (conversion échouée):**\n```\ngraph LR\n    A[Random Forest] --> B[Arbre 1]\n    A --> C[Arbre 2]\n    A --> D[Arbre N]\n    B --> E[V...\n```\n\n\n**2. Sensibilité à l'overfitting:**\n\nGradient Boosting est plus sensible car:\n- Chaque arbre se concentre sur les erreurs précédentes\n- Risque d'apprendre le bruit si trop d'itérations\n- Peut \"mémoriser\" les cas difficiles du train set\n- Pas de randomisation par défaut (contrairement à RF)\n\n**Solutions:**\n- Limiter le nombre d'arbres (`n_estimators`)\n- Réduire le taux d'apprentissage (`learning_rate`)\n- Limiter la profondeur (`max_depth`)\n- Early stopping avec validation set\n\n**3. Hyperparamètres clés:**\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier(\n    # 1. Nombre d'arbres\n    n_estimators=100,  # Plus = meilleur mais risque overfitting\n    \n    # 2. Taux d'apprentissage\n    learning_rate=0.1,  # Plus faible = besoin de plus d'arbres\n                        # Typage: 0.01-0.3\n    \n    # 3. Profondeur maximale\n    max_depth=6,  # Plus profond = plus complexe\n                  # Typique: 3-10\n    \n    # Bonus importants:\n    subsample=0.8,      # Échantillonnage des données (0.5-1.0)\n    colsample_bytree=0.8,  # Échantillonnage des features\n    min_child_weight=1,    # Régularisation\n    \n    random_state=42\n)\n```\n:::\n\n\n**Recommandations de tuning:**\n\n1. **Commencer avec:**\n\n   - `learning_rate=0.1`\n   - `max_depth=6`\n   - `n_estimators=100`\n\n2. **Puis optimiser:**\n\n   - Augmenter `n_estimators` + réduire `learning_rate`\n   - Ajuster `max_depth` (3-10)\n   - Ajouter régularisation (`subsample`, `colsample_bytree`)\n\n3. **Utiliser early stopping:**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=10,  # Stop si pas d'amélioration\n    verbose=False\n)\n```\n:::\n\n\n:::\n\n## Exercices Récapitulatifs\n\n::: {.callout-warning icon=false}\n## Exercice Final: Choix d'Algorithme\n\nPour chacun des scénarios suivants, recommandez un algorithme et justifiez:\n\n1. **Diagnostic médical** (interprétabilité cruciale, 1000 patients, 20 features)\n2. **Détection de fraude** (millions de transactions, temps réel, déséquilibre 99/1)\n3. **Classification d'images** (50000 images, haute dimension, GPU disponible)\n4. **Prédiction de churn** (10000 clients, features mixtes, besoin de probabilités calibrées)\n5. **Classification de textes** (emails spam, 100000 emails, features = mots)\n::: {.callout-note collapse=\"true\"}\n\n<function_calls>\n<invoke name=\"artifacts\">\n<parameter name=\"command\">update</parameter>\n<parameter name=\"id\">seance4</parameter>\n<parameter name=\"old_str\">5. **Classification de textes** (emails spam</parameter>\n<parameter name=\"new_str\">5. **Classification de textes** (emails spam, 100000 emails, features = mots)\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice Final\n\n**1. Diagnostic médical:**\n\n- **Recommandation**: Decision Tree ou Régression Logistique\n- **Justification**:\n\n  - Interprétabilité essentielle pour les médecins\n  - Dataset de taille modérée\n  - Besoin de comprendre les règles de décision\n  - Alternative: Random Forest + feature importance\n\n**2. Détection de fraude:**\n\n- **Recommandation**: XGBoost/LightGBM\n\n- **Justification**:\n\n  - Excellent avec classes déséquilibrées (paramètre `scale_pos_weight`)\n  - Très rapide en prédiction (important pour temps réel)\n  - Gère bien les grandes données\n  - Robuste et performant\n  - Peut utiliser early stopping\n\n**3. Classification d'images:**\n\n- **Recommandation**: CNN (Deep Learning) - hors scope pour l'instant\n- **Justification actuelle avec ML classique**:\n\n  - Random Forest avec features extraites (HOG, SIFT)\n  - SVM avec kernel RBF\n  - Mais performances limitées vs Deep Learning\n\n**4. Prédiction de churn:**\n\n- **Recommandation**: Régression Logistique ou Gradient Boosting\n- **Justification**:\n\n  - Logistic Regression: probabilités bien calibrées, interprétable\n  - Gradient Boosting: meilleures performances, feature importance\n  - Dataset de taille moyenne\n  - Features mixtes gérées par les deux\n\n**5. Classification de textes:**\n\n- **Recommandation**: Naive Bayes (Multinomial)\n- **Justification**:\n\n  - Très performant pour la classification de texte\n  - Rapide à entraîner et prédire\n  - Gère bien les grandes dimensions (nombreux mots)\n  - Probabilités natives\n  - Alternative: Régression Logistique\n:::\n:::\n\n## Résumé du TD\n\n::: {.callout-important icon=false}\n## Points clés à retenir\n\n### Algorithmes et leurs forces\n\n1. **Decision Tree**\n   - $\\checkmark$ Interprétable, visuel\n   - X Overfitting, instable\n\n2. **Random Forest**\n   - $\\checkmark$ Robuste, performant\n   - X Moins interprétable, mémoire\n\n3. **SVM**\n   - $\\checkmark$ Excellent en haute dimension\n   - X Lent, difficile à interpréter\n\n4. **Naive Bayes**\n   - $\\checkmark$ Rapide, bon pour texte\n   - X Hypothèse d'indépendance forte\n\n5. **Régression Logistique**\n   - $\\checkmark$ Probabilités calibrées, interprétable\n   - X Assume linéarité\n\n6. **k-NN**\n   - $\\checkmark$ Simple, pas de training\n   - X Lent en prédiction, besoin normalisation\n\n7. **Gradient Boosting**\n   - $\\checkmark$ Très performant, gère déséquilibre\n   - X Sensible overfitting, plus complexe\n:::\n\n## Pour le Prochain Cours\n\nPréparez-vous pour le **TD2 sur les Critères d'Évaluation** où nous approfondirons:\n- Matrice de confusion\n- Precision, Recall, F1-score\n- Courbe ROC et AUC\n- Choix de métriques selon le contexte\n\n## Ressources Complémentaires\n\n1. [Scikit-learn: Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/)\n2. [StatQuest: Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n3. [XGBoost Documentation](https://xgboost.readthedocs.io/)</parameter>\n\n",
    "supporting": [
      "seance4_files\\figure-pdf"
    ],
    "filters": []
  }
}