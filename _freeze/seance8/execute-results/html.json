{
  "hash": "76e953dbf7e01fa256b27af62a7970fd",
  "result": {
    "engine": "jupyter",
    "markdown": "# Séance 8: TP3 - Régression & Optimisation\n\n::: {.callout-note icon=false}\n## Informations de la séance\n- **Type**: Travaux Pratiques\n- **Durée**: 2h\n- **Objectifs**: Obj6, Obj7\n:::\n\n## Introduction\nDans ce TP, nous allons mettre en pratique les concepts de régression vus en cours. Nous travaillerons sur un dataset réel pour prédire les prix de maisons, en comparant différents modèles de régression et en optimisant leurs hyperparamètres.\n\n**Objectifs du TP:**\n\n1. Prétraiter des données pour la régression\n2. Implémenter et comparer Linear, Ridge, Lasso, ElasticNet, SVR\n3. Optimiser les hyperparamètres avec CV\n4. Évaluer avec métriques multiples (MAE, RMSE, R²)\n5. Interpréter et visualiser les résultats\n\n## 1. Chargement et Exploration du Dataset\n\nNous utilisons le **Boston Housing Dataset** (ou California Housing si Boston non disponible).\n\n::: {#ea90add6 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Chargement\ncalifornia = fetch_california_housing()\nX, y = california.data, california.target\n\n# DataFrame pour exploration\ndf = pd.DataFrame(X, columns=california.feature_names)\ndf['MedHouseVal'] = y\n\nprint(\"=\" * 70)\nprint(\"DATASET: CALIFORNIA HOUSING\")\nprint(\"=\" * 70)\nprint(f\"\\nShape: {X.shape}\")\nprint(f\"Features: {california.feature_names}\")\nprint(f\"\\nDescription du target (prix médian en 100k$):\")\nprint(df['MedHouseVal'].describe())\n\n# Statistiques descriptives\nprint(\"\\n\" + \"=\" * 70)\nprint(\"STATISTIQUES DESCRIPTIVES\")\nprint(\"=\" * 70)\nprint(df.describe().T)\n\n# Vérifier les valeurs manquantes\nprint(f\"\\nValeurs manquantes par feature:\")\nprint(df.isnull().sum())\n\n# Visualisations\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Distribution du target\naxes[0, 0].hist(y, bins=50, edgecolor='black', alpha=0.7)\naxes[0, 0].set_xlabel('Prix Médian (100k$)')\naxes[0, 0].set_ylabel('Fréquence')\naxes[0, 0].set_title('Distribution des Prix')\naxes[0, 0].axvline(y.mean(), color='red', linestyle='--', label=f'Moyenne: {y.mean():.2f}')\naxes[0, 0].legend()\n\n# Boxplot des features (normalisées pour comparaison)\ndf_normalized = (df - df.mean()) / df.std()\naxes[0, 1].boxplot([df_normalized[col] for col in california.feature_names],\n                    labels=california.feature_names, vert=True)\naxes[0, 1].set_xticklabels(california.feature_names, rotation=45, ha='right')\naxes[0, 1].set_ylabel('Valeurs normalisées')\naxes[0, 1].set_title('Distribution des Features (normalisées)')\naxes[0, 1].grid(alpha=0.3)\n\n# Matrice de corrélation\ncorr_matrix = df.corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\nsns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n            center=0, ax=axes[1, 0], cbar_kws={'label': 'Corrélation'})\naxes[1, 0].set_title('Matrice de Corrélation')\n\n# Scatter: Feature la plus corrélée avec target\ncorr_with_target = corr_matrix['MedHouseVal'].drop('MedHouseVal').abs().sort_values(ascending=False)\nbest_feature = corr_with_target.index[0]\naxes[1, 1].scatter(df[best_feature], df['MedHouseVal'], alpha=0.3)\naxes[1, 1].set_xlabel(best_feature)\naxes[1, 1].set_ylabel('Prix Médian')\naxes[1, 1].set_title(f'Prix vs {best_feature} (corr={corr_matrix.loc[best_feature, \"MedHouseVal\"]:.2f})')\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CORRÉLATIONS AVEC LE TARGET\")\nprint(\"=\" * 70)\nprint(corr_with_target)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nDATASET: CALIFORNIA HOUSING\n======================================================================\n\nShape: (20640, 8)\nFeatures: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n\nDescription du target (prix médian en 100k$):\ncount    20640.000000\nmean         2.068558\nstd          1.153956\nmin          0.149990\n25%          1.196000\n50%          1.797000\n75%          2.647250\nmax          5.000010\nName: MedHouseVal, dtype: float64\n\n======================================================================\nSTATISTIQUES DESCRIPTIVES\n======================================================================\n               count         mean          std         min         25%  \\\nMedInc       20640.0     3.870671     1.899822    0.499900    2.563400   \nHouseAge     20640.0    28.639486    12.585558    1.000000   18.000000   \nAveRooms     20640.0     5.429000     2.474173    0.846154    4.440716   \nAveBedrms    20640.0     1.096675     0.473911    0.333333    1.006079   \nPopulation   20640.0  1425.476744  1132.462122    3.000000  787.000000   \nAveOccup     20640.0     3.070655    10.386050    0.692308    2.429741   \nLatitude     20640.0    35.631861     2.135952   32.540000   33.930000   \nLongitude    20640.0  -119.569704     2.003532 -124.350000 -121.800000   \nMedHouseVal  20640.0     2.068558     1.153956    0.149990    1.196000   \n\n                     50%          75%           max  \nMedInc          3.534800     4.743250     15.000100  \nHouseAge       29.000000    37.000000     52.000000  \nAveRooms        5.229129     6.052381    141.909091  \nAveBedrms       1.048780     1.099526     34.066667  \nPopulation   1166.000000  1725.000000  35682.000000  \nAveOccup        2.818116     3.282261   1243.333333  \nLatitude       34.260000    37.710000     41.950000  \nLongitude    -118.490000  -118.010000   -114.310000  \nMedHouseVal     1.797000     2.647250      5.000010  \n\nValeurs manquantes par feature:\nMedInc         0\nHouseAge       0\nAveRooms       0\nAveBedrms      0\nPopulation     0\nAveOccup       0\nLatitude       0\nLongitude      0\nMedHouseVal    0\ndtype: int64\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance8_files/figure-html/cell-2-output-2.png){width=1142 height=950}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n======================================================================\nCORRÉLATIONS AVEC LE TARGET\n======================================================================\nMedInc        0.688075\nAveRooms      0.151948\nLatitude      0.144160\nHouseAge      0.105623\nAveBedrms     0.046701\nLongitude     0.045967\nPopulation    0.024650\nAveOccup      0.023737\nName: MedHouseVal, dtype: float64\n```\n:::\n:::\n\n\n### Exercice 1.1: Analyse Exploratoire\n\n**Questions:**\n\n1. Quelle feature est la plus corrélée avec le prix?\n2. Y a-t-il des features fortement corrélées entre elles? (Multicolinéarité potentielle?)\n3. La distribution du target est-elle gaussienne? Quel traitement pourrait être appliqué si non?\n4. Identifiez des outliers potentiels\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 1.1\n\n::: {#d1b08902 .cell execution_count=2}\n``` {.python .cell-code}\nprint(\"=\" * 70)\nprint(\"ANALYSE EXPLORATOIRE - RÉPONSES\")\nprint(\"=\" * 70)\n\n# 1. Feature la plus corrélée\nprint(f\"\\n1. Feature la plus corrélée avec le prix:\")\nprint(f\"   → {best_feature} (corrélation = {corr_matrix.loc[best_feature, 'MedHouseVal']:.3f})\")\n\n# 2. Multicolinéarité\nprint(f\"\\n2. Paires de features fortement corrélées (|corr| > 0.7):\")\ncorr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if i != j:\n            corr_val = corr_matrix.iloc[i, j]\n            if abs(corr_val) > 0.7:\n                corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n\nif corr_pairs:\n    for feat1, feat2, corr_val in corr_pairs:\n        print(f\"   • {feat1} <-> {feat2}: {corr_val:.3f}\")\nelse:\n    print(\"   → Aucune multicolinéarité forte détectée\")\n\n# 3. Distribution du target\nfrom scipy import stats\nshapiro_stat, shapiro_p = stats.shapiro(y[:1000])  # Test sur échantillon\nprint(f\"\\n3. Test de normalité (Shapiro-Wilk):\")\nprint(f\"   Statistique: {shapiro_stat:.4f}, p-value: {shapiro_p:.4e}\")\nif shapiro_p < 0.05:\n    print(\"   → Distribution NON gaussienne (p < 0.05)\")\n    print(\"   Traitements possibles:\")\n    print(\"   • Transformation log(y)\")\n    print(\"   • Transformation Box-Cox\")\n    print(\"   • Utiliser des modèles robustes aux distributions non-gaussiennes\")\nelse:\n    print(\"   → Distribution gaussienne (p >= 0.05)\")\n\n# Skewness et Kurtosis\nskewness = stats.skew(y)\nkurtosis = stats.kurtosis(y)\nprint(f\"   Asymétrie (Skewness): {skewness:.3f}\")\nprint(f\"   Aplatissement (Kurtosis): {kurtosis:.3f}\")\n\n# 4. Outliers\nprint(f\"\\n4. Détection d'outliers:\")\nQ1 = df['MedHouseVal'].quantile(0.25)\nQ3 = df['MedHouseVal'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = df[(df['MedHouseVal'] < lower_bound) | (df['MedHouseVal'] > upper_bound)]\nprint(f\"   Intervalle IQR: [{Q1:.2f}, {Q3:.2f}]\")\nprint(f\"   Bornes: [{lower_bound:.2f}, {upper_bound:.2f}]\")\nprint(f\"   Nombre d'outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n\n# Visualisation des outliers\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(range(len(y)), np.sort(y), alpha=0.5, s=10)\nax.axhline(lower_bound, color='red', linestyle='--', label='Lower bound')\nax.axhline(upper_bound, color='red', linestyle='--', label='Upper bound')\nax.set_xlabel('Index (trié)')\nax.set_ylabel('Prix')\nax.set_title('Détection d\\'Outliers par IQR')\nax.legend()\nax.grid(alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nANALYSE EXPLORATOIRE - RÉPONSES\n======================================================================\n\n1. Feature la plus corrélée avec le prix:\n   → MedInc (corrélation = 0.688)\n\n2. Paires de features fortement corrélées (|corr| > 0.7):\n   • AveRooms <-> AveBedrms: 0.848\n   • Latitude <-> Longitude: -0.925\n\n3. Test de normalité (Shapiro-Wilk):\n   Statistique: 0.9460, p-value: 1.2095e-18\n   → Distribution NON gaussienne (p < 0.05)\n   Traitements possibles:\n   • Transformation log(y)\n   • Transformation Box-Cox\n   • Utiliser des modèles robustes aux distributions non-gaussiennes\n   Asymétrie (Skewness): 0.978\n   Aplatissement (Kurtosis): 0.328\n\n4. Détection d'outliers:\n   Intervalle IQR: [1.20, 2.65]\n   Bornes: [-0.98, 4.82]\n   Nombre d'outliers: 1071 (5.2%)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance8_files/figure-html/cell-3-output-2.png){width=811 height=524}\n:::\n:::\n\n\n**Conclusions typiques:**\n\n1. **MedInc** (revenu médian) généralement le plus corrélé (~0.68)\n2. Multicolinéarité possible → considérer Ridge/ElasticNet\n3. Distribution souvent légèrement asymétrique → transformation log possible\n4. Quelques outliers mais acceptable (<5%)\n:::\n\n## 2. Prétraitement et Split\n\n::: {#57fcb896 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Split train/test (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"=\" * 70)\nprint(\"SPLIT DES DONNÉES\")\nprint(\"=\" * 70)\nprint(f\"Train set: {X_train.shape}\")\nprint(f\"Test set: {X_test.shape}\")\n\n# Standardisation (importante pour Ridge, Lasso, SVR)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"\\nStandardisation:\")\nprint(f\"Mean (train): {X_train_scaled.mean(axis=0)}\")  # Devrait être ~0\nprint(f\"Std (train): {X_train_scaled.std(axis=0)}\")    # Devrait être ~1\n\n# Vérification\nprint(f\"\\nVérification après scaling:\")\nprint(f\"  Mean proche de 0: {np.allclose(X_train_scaled.mean(axis=0), 0, atol=1e-10)}\")\nprint(f\"  Std proche de 1: {np.allclose(X_train_scaled.std(axis=0), 1, atol=1e-2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nSPLIT DES DONNÉES\n======================================================================\nTrain set: (16512, 8)\nTest set: (4128, 8)\n\nStandardisation:\nMean (train): [-6.59266865e-15 -6.68608149e-17  8.01559239e-15 -1.17273358e-15\n -2.60880895e-18 -1.13675656e-16  7.99652724e-14 -3.87910056e-13]\nStd (train): [1. 1. 1. 1. 1. 1. 1. 1.]\n\nVérification après scaling:\n  Mean proche de 0: True\n  Std proche de 1: True\n```\n:::\n:::\n\n\n::: {.callout-warning}\n## Importance de la Standardisation\n\n**Pourquoi standardiser?**\n- Ridge/Lasso: Pénalisation équitable des coefficients\n- SVR: Sensible à l'échelle des features\n- Convergence plus rapide des algorithmes itératifs\n\n**Quand ne PAS standardiser?**\n- Arbres de décision / Random Forest (invariants aux transformations monotones)\n- Régression linéaire simple (si pas de régularisation)\n\n**Règle:** Toujours fit sur train, transform sur test (éviter data leakage)\n:::\n\n## 3. Modèles de Base\n\n### Exercice 3.1: Régression Linéaire Simple\n\nEntraînez une régression linéaire et évaluez-la.\n\n**Instructions:**\n1. Créez et entraînez le modèle\n2. Prédisez sur train et test\n3. Calculez MAE, RMSE, R² pour les deux ensembles\n4. Affichez les 5 coefficients les plus importants\n5. Visualisez prédictions vs vraies valeurs\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 3.1\n\n::: {#1688c179 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nprint(\"=\" * 70)\nprint(\"RÉGRESSION LINÉAIRE SIMPLE\")\nprint(\"=\" * 70)\n\n# 1. Entraînement\nmodel_lr = LinearRegression()\nmodel_lr.fit(X_train_scaled, y_train)\n\n# 2. Prédictions\ny_train_pred = model_lr.predict(X_train_scaled)\ny_test_pred = model_lr.predict(X_test_scaled)\n\n# 3. Métriques\ndef evaluate_model(y_true, y_pred, dataset_name=\"\"):\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n    \n    print(f\"\\n{dataset_name}:\")\n    print(f\"  MAE:  {mae:.4f}\")\n    print(f\"  RMSE: {rmse:.4f}\")\n    print(f\"  R²:   {r2:.4f}\")\n    \n    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n\ntrain_metrics = evaluate_model(y_train, y_train_pred, \"Train Set\")\ntest_metrics = evaluate_model(y_test, y_test_pred, \"Test Set\")\n\n# 4. Coefficients importants\ncoef_df = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Coefficient': model_lr.coef_\n})\ncoef_df['Abs_Coef'] = np.abs(coef_df['Coefficient'])\ncoef_df = coef_df.sort_values('Abs_Coef', ascending=False)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COEFFICIENTS (TOP 5)\")\nprint(\"=\" * 70)\nprint(coef_df[['Feature', 'Coefficient']].head().to_string(index=False))\n\n# 5. Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(12, 5))\n\n# Prédictions vs Vraies valeurs (Test)\naxes[0].scatter(y_test, y_test_pred, alpha=0.3, s=10)\naxes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n             'r--', lw=2, label='Prédiction parfaite')\naxes[0].set_xlabel('Vraie valeur')\naxes[0].set_ylabel('Prédiction')\naxes[0].set_title(f'Régression Linéaire (Test R²={test_metrics[\"R2\"]:.3f})')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Résidus\nresiduals = y_test - y_test_pred\naxes[1].scatter(y_test_pred, residuals, alpha=0.3, s=10)\naxes[1].axhline(0, color='red', linestyle='--', lw=2)\naxes[1].set_xlabel('Prédictions')\naxes[1].set_ylabel('Résidus')\naxes[1].set_title('Graphique des Résidus')\naxes[1].grid(alpha=0.3)\n\n# Distribution des résidus\naxes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\naxes[2].axvline(residuals.mean(), color='red', linestyle='--', \n               label=f'Moyenne: {residuals.mean():.4f}')\naxes[2].set_xlabel('Résidus')\naxes[2].set_ylabel('Fréquence')\naxes[2].set_title('Distribution des Résidus')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Analyse des résidus\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANALYSE DES RÉSIDUS\")\nprint(\"=\" * 70)\nprint(f\"Moyenne: {residuals.mean():.6f} (devrait être $\\approx$ 0)\")\nprint(f\"Std: {residuals.std():.4f}\")\nprint(f\"Min: {residuals.min():.4f}\")\nprint(f\"Max: {residuals.max():.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nRÉGRESSION LINÉAIRE SIMPLE\n======================================================================\n\nTrain Set:\n  MAE:  0.5286\n  RMSE: 0.7197\n  R²:   0.6126\n\nTest Set:\n  MAE:  0.5332\n  RMSE: 0.7456\n  R²:   0.5758\n\n======================================================================\nCOEFFICIENTS (TOP 5)\n======================================================================\n  Feature  Coefficient\n Latitude    -0.896929\nLongitude    -0.869842\n   MedInc     0.854383\nAveBedrms     0.339259\n AveRooms    -0.294410\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance8_files/figure-html/cell-5-output-2.png){width=1141 height=468}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n======================================================================\nANALYSE DES RÉSIDUS\n======================================================================\nMoyenne: 0.003479 (devrait être $\u0007pprox$ 0)\nStd: 0.7456\nMin: -9.8753\nMax: 4.1484\n```\n:::\n:::\n\n\n**Interprétation:**\n- R² train > R² test → léger surapprentissage (normal)\n- Résidus centrés sur 0 → modèle non biaisé\n- Distribution des résidus approximativement gaussienne → hypothèses vérifiées\n:::\n\n## 4. Modèles Régularisés\n\n### Exercice 4.1: Comparaison Ridge, Lasso, ElasticNet\n\nComparez les 3 modèles régularisés avec différentes valeurs de alpha.\n\n**Instructions:**\n\n1. Testez alpha dans [0.001, 0.01, 0.1, 1, 10, 100]\n2. Pour chaque modèle et chaque alpha:\n   - Entraînez sur train\n   - Calculez R² sur test\n3. Tracez R² vs alpha pour les 3 modèles\n4. Identifiez le meilleur modèle et le meilleur alpha\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 4.1\n\n::: {#e68ff7c6 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\nprint(\"=\" * 70)\nprint(\"COMPARAISON RIDGE, LASSO, ELASTICNET\")\nprint(\"=\" * 70)\n\n# 1. Grille d'alphas\nalphas = [0.001, 0.01, 0.1, 1, 10, 100]\n\n# Stocker les résultats\nresults = {'Ridge': [], 'Lasso': [], 'ElasticNet': []}\n\n# 2. Entraînement\nfor alpha in alphas:\n    # Ridge\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train_scaled, y_train)\n    r2_ridge = r2_score(y_test, ridge.predict(X_test_scaled))\n    results['Ridge'].append(r2_ridge)\n    \n    # Lasso\n    lasso = Lasso(alpha=alpha, max_iter=10000)\n    lasso.fit(X_train_scaled, y_train)\n    r2_lasso = r2_score(y_test, lasso.predict(X_test_scaled))\n    results['Lasso'].append(r2_lasso)\n    \n    # ElasticNet\n    elastic = ElasticNet(alpha=alpha, l1_ratio=0.5, max_iter=10000)\n    elastic.fit(X_train_scaled, y_train)\n    r2_elastic = r2_score(y_test, elastic.predict(X_test_scaled))\n    results['ElasticNet'].append(r2_elastic)\n\n# 3. Visualisation\nplt.figure(figsize=(12, 6))\nfor model_name, r2_scores in results.items():\n    plt.plot(alphas, r2_scores, marker='o', label=model_name, linewidth=2)\n\nplt.xscale('log')\nplt.xlabel('alpha (log scale)')\nplt.ylabel('R² Score (Test)')\nplt.title('Comparaison Ridge, Lasso, ElasticNet')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.axhline(test_metrics['R2'], color='red', linestyle='--', \n           label=f'Linear (alpha=0): {test_metrics[\"R2\"]:.4f}', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n# 4. Meilleur modèle\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MEILLEURS HYPERPARAMÈTRES\")\nprint(\"=\" * 70)\n\nfor model_name, r2_scores in results.items():\n    best_idx = np.argmax(r2_scores)\n    best_alpha = alphas[best_idx]\n    best_r2 = r2_scores[best_idx]\n    print(f\"\\n{model_name}:\")\n    print(f\"  Meilleur alpha: {best_alpha}\")\n    print(f\"  R² Test: {best_r2:.4f}\")\n\n# Tableau comparatif\ndf_comparison = pd.DataFrame(results, index=[f'alpha={a}' for a in alphas])\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TABLEAU COMPARATIF (R² Test)\")\nprint(\"=\" * 70)\nprint(df_comparison.to_string())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nCOMPARAISON RIDGE, LASSO, ELASTICNET\n======================================================================\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](seance8_files/figure-html/cell-6-output-2.png){width=1141 height=563}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n======================================================================\nMEILLEURS HYPERPARAMÈTRES\n======================================================================\n\nRidge:\n  Meilleur alpha: 100\n  R² Test: 0.5778\n\nLasso:\n  Meilleur alpha: 0.01\n  R² Test: 0.5816\n\nElasticNet:\n  Meilleur alpha: 0.01\n  R² Test: 0.5803\n\n======================================================================\nTABLEAU COMPARATIF (R² Test)\n======================================================================\n                Ridge     Lasso  ElasticNet\nalpha=0.001  0.575788  0.576856    0.576543\nalpha=0.01   0.575788  0.581615    0.580319\nalpha=0.1    0.575791  0.481361    0.514765\nalpha=1      0.575816 -0.000219    0.203126\nalpha=10     0.576060 -0.000219   -0.000219\nalpha=100    0.577791 -0.000219   -0.000219\n```\n:::\n:::\n\n\n**Observations attendues:**\n- Ridge: R² stable, peu sensible à alpha\n- Lasso: R² peut chuter si alpha trop élevé (trop de coefficients à 0)\n- ElasticNet: Compromis entre Ridge et Lasso\n:::\n\n### Exercice 4.2: Sélection de Features avec Lasso\n\nUtilisez Lasso pour identifier les features importantes.\n\n**Instructions:**\n1. Entraînez Lasso avec alpha=0.1\n2. Affichez le nombre de coefficients non-nuls\n3. Identifiez les features sélectionnées\n4. Comparez les coefficients Lasso vs Linear\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 4.2\n\n```python\nprint(\"=\" * 70)\nprint(\"SÉLECTION DE FEATURES AVEC LASSO\")\nprint(\"=\" * 70)\n\n# 1. Entraînement\nlasso = Lasso(alpha=0.1, max_iter=10000)\nlasso.fit(X_train_scaled, y_train)\n\n# 2. Coefficients non-nuls\nnon_zero_mask = np.abs(lasso.coef_) > 1e-5\nn_selected = np.sum(non_zero_mask)\nn_total = len(lasso.coef_)\n\nprint(f\"\\nNombre de features sélectionnées: {n_selected}/{n_total}\")\n\n# 3. Features sélectionnées\nselected_features = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Lasso_Coef': lasso.coef_,\n    'Linear_Coef': model_lr.coef_,\n    'Selected': non_zero_mask\n})\nselected_features['Abs_Lasso'] = np.abs(selected_features['Lasso_Coef'])\nselected_features = selected_features.sort_values('Abs_Lasso', ascending=False)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON COEFFICIENTS\")\nprint(\"=\" * 70)\nprint(selected_features[['Feature', 'Lasso_Coef', 'Linear_Coef', 'Selected']].to_string(index=False))\n\n# 4. Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Barplot comparatif\nx_pos = np.arange(len(california.feature_names))\nwidth = 0.35\n\naxes[0].bar(x_pos - width/2, model_lr.coef_, width, label='Linear', alpha=0.7)\naxes[0].bar(x_pos + width/2, lasso.coef_, width, label='Lasso (alpha=0.1)', alpha=0.7)\naxes[0].set_xticks(x_pos)\naxes[0].set_xticklabels(california.feature_names, rotation=45, ha='right')\naxes[0].set_ylabel('Coefficient')\naxes[0].set_title('Comparaison Coefficients: Linear vs Lasso')\naxes[0].legend()\naxes[0].axhline(0, color='black', linewidth=0.8)\naxes[0].grid(alpha=0.3, axis='y')\n\n# Scatter: Linear vs Lasso\naxes[1].scatter(model_lr.coef_, lasso.coef_, s=100, alpha=0.6)\nfor i, feature in enumerate(california.feature_names):\n    axes[1].annotate(feature, (model_lr.coef_[i], lasso.coef_[i]),\n                     fontsize=8, alpha=0.7)\naxes[1].plot([model_lr.coef_.min(), model_lr.coef_.max()],\n             [model_lr.coef_.min(), model_lr.coef_.max()],\n             'r--', label='y=x')\naxes[1].set_xlabel('Coefficient Linear')\naxes[1].set_ylabel('Coefficient Lasso')\naxes[1].set_title('Linear vs Lasso Coefficients')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FEATURES ÉLIMINÉES PAR LASSO\")\nprint(\"=\" * 70)\neliminated = selected_features[~selected_features['Selected']]['Feature'].tolist()\nif eliminated:\n    print(\"Features mises à 0:\")\n    for feat in eliminated:\n        print(f\"  • {feat}\")\nelse:\n    print(\"Aucune feature éliminée (alpha peut-être trop faible)\")\n```\n\n**Interprétation:**\n- Lasso réduit certains coefficients exactement à 0\n- Features éliminées = probablement redondantes ou peu informatives\n- Peut améliorer l'interprétabilité du modèle\n:::\n\n## 5. Optimisation avec Cross-Validation\n\n### Exercice 5.1: RidgeCV et LassoCV\n\nUtilisez les versions CV pour trouver automatiquement le meilleur alpha.\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 5.1\n\n```python\nfrom sklearn.linear_model import RidgeCV, LassoCV\n\nprint(\"=\" * 70)\nprint(\"OPTIMISATION AUTOMATIQUE AVEC CV\")\nprint(\"=\" * 70)\n\n# Grille d'alphas\nalphas_cv = np.logspace(-3, 3, 50)\n\n# RidgeCV\nprint(\"\\n1. RidgeCV...\")\nridge_cv = RidgeCV(alphas=alphas_cv, cv=5, scoring='r2')\nridge_cv.fit(X_train_scaled, y_train)\nprint(f\"   Meilleur alpha: {ridge_cv.alpha_:.4f}\")\n\ny_pred_ridge = ridge_cv.predict(X_test_scaled)\nridge_metrics = evaluate_model(y_test, y_pred_ridge, \"Ridge (CV)\")\n\n# LassoCV\nprint(\"\\n2. LassoCV...\")\nlasso_cv = LassoCV(alphas=alphas_cv, cv=5, max_iter=10000, random_state=42)\nlasso_cv.fit(X_train_scaled, y_train)\nprint(f\"   Meilleur alpha: {lasso_cv.alpha_:.4f}\")\n\ny_pred_lasso = lasso_cv.predict(X_test_scaled)\nlasso_metrics = evaluate_model(y_test, y_pred_lasso, \"Lasso (CV)\")\n\n# ElasticNetCV\nfrom sklearn.linear_model import ElasticNetCV\n\nprint(\"\\n3. ElasticNetCV...\")\nelastic_cv = ElasticNetCV(alphas=alphas_cv, l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n                          cv=5, max_iter=10000, random_state=42)\nelastic_cv.fit(X_train_scaled, y_train)\nprint(f\"   Meilleur alpha: {elastic_cv.alpha_:.4f}\")\nprint(f\"   Meilleur l1_ratio: {elastic_cv.l1_ratio_:.2f}\")\n\ny_pred_elastic = elastic_cv.predict(X_test_scaled)\nelastic_metrics = evaluate_model(y_test, y_pred_elastic, \"ElasticNet (CV)\")\n\n# Comparaison finale\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON FINALE\")\nprint(\"=\" * 70)\n\nfinal_comparison = pd.DataFrame({\n    'Modèle': ['Linear', 'Ridge (CV)', 'Lasso (CV)', 'ElasticNet (CV)'],\n    'MAE': [test_metrics['MAE'], ridge_metrics['MAE'], \n            lasso_metrics['MAE'], elastic_metrics['MAE']],\n    'RMSE': [test_metrics['RMSE'], ridge_metrics['RMSE'], \n             lasso_metrics['RMSE'], elastic_metrics['RMSE']],\n    'R²': [test_metrics['R2'], ridge_metrics['R2'], \n           lasso_metrics['R2'], elastic_metrics['R2']]\n})\n\nprint(final_comparison.to_string(index=False))\n\n# Meilleur modèle\nbest_idx = final_comparison['R²'].idxmax()\nbest_model_name = final_comparison.loc[best_idx, 'Modèle']\nprint(f\"\\n→ Meilleur modèle: {best_model_name}\")\n```\n:::\n\n## 6. SVR (Support Vector Regression)\n\n### Exercice 6.1: SVR avec différents kernels\n\nComparez SVR linéaire et RBF.\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 6.1\n\n```python\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"=\" * 70)\nprint(\"SUPPORT VECTOR REGRESSION\")\nprint(\"=\" * 70)\n\n# Note: SVR est lent, on réduit le dataset pour démo\nX_train_small = X_train_scaled[:5000]\ny_train_small = y_train[:5000]\n\n# 1. SVR Linéaire\nprint(\"\\n1. SVR Linéaire...\")\nsvr_lin = SVR(kernel='linear', C=1.0)\nsvr_lin.fit(X_train_small, y_train_small)\ny_pred_svr_lin = svr_lin.predict(X_test_scaled)\nsvr_lin_metrics = evaluate_model(y_test, y_pred_svr_lin, \"SVR Linear\")\n\n# 2. SVR RBF\nprint(\"\\n2. SVR RBF...\")\nsvr_rbf = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr_rbf.fit(X_train_small, y_train_small)\ny_pred_svr_rbf = svr_rbf.predict(X_test_scaled)\nsvr_rbf_metrics = evaluate_model(y_test, y_pred_svr_rbf, \"SVR RBF\")\n\n# 3. Comparaison\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON SVR\")\nprint(\"=\" * 70)\n\nsvr_comparison = pd.DataFrame({\n    'Kernel': ['linear', 'rbf'],\n    'MAE': [svr_lin_metrics['MAE'], svr_rbf_metrics['MAE']],\n    'RMSE': [svr_lin_metrics['RMSE'], svr_rbf_metrics['RMSE']],\n    'R²': [svr_lin_metrics['R2'], svr_rbf_metrics['R2']]\n})\nprint(svr_comparison.to_string(index=False))\n\n# 4. Optimisation SVR RBF\nprint(\"\\n\" + \"=\" * 70)\nprint(\"OPTIMISATION SVR RBF AVEC GRIDSEARCH\")\nprint(\"=\" * 70)\n\n# Réduction supplémentaire pour vitesse\nX_val = X_train_scaled[5000:6000]\ny_val = y_train[5000:6000]\n\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'gamma': ['scale', 'auto', 0.01, 0.1],\n    'epsilon': [0.01, 0.1, 0.5]\n}\n\ngrid_search = GridSearchCV(\n    SVR(kernel='rbf'),\n    param_grid,\n    cv=3,\n    scoring='r2',\n    n_jobs=-1,\n    verbose=1\n)\n\nprint(\"Entraînement GridSearch (peut être long)...\")\ngrid_search.fit(X_train_small, y_train_small)\n\nprint(f\"\\nMeilleurs paramètres: {grid_search.best_params_}\")\nbest_svr = grid_search.best_estimator_\ny_pred_best_svr = best_svr.predict(X_val)\nbest_svr_metrics = evaluate_model(y_val, y_pred_best_svr, \"SVR optimisé (val)\")\n\n# Visualisation SVR\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Prédictions vs Vraies valeurs\naxes[0].scatter(y_val, y_pred_best_svr, alpha=0.3, s=10)\naxes[0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], \n             'r--', lw=2, label='Prédiction parfaite')\naxes[0].set_xlabel('Vraie valeur')\naxes[0].set_ylabel('Prédiction')\naxes[0].set_title(f'SVR Optimisé (R²={best_svr_metrics[\"R2\"]:.3f})')\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Comparaison modèles\nmodels_comparison = pd.DataFrame({\n    'Modèle': ['Linear', 'Ridge', 'Lasso', 'SVR Linear', 'SVR RBF'],\n    'RMSE': [test_metrics['RMSE'], ridge_metrics['RMSE'], \n             lasso_metrics['RMSE'], svr_lin_metrics['RMSE'], svr_rbf_metrics['RMSE']],\n    'R²': [test_metrics['R2'], ridge_metrics['R2'], \n           lasso_metrics['R2'], svr_lin_metrics['R2'], svr_rbf_metrics['R2']]\n})\n\nx_pos = np.arange(len(models_comparison))\nwidth = 0.35\n\nbars1 = axes[1].bar(x_pos - width/2, models_comparison['RMSE'], width, label='RMSE')\nbars2 = axes[1].bar(x_pos + width/2, models_comparison['R²'], width, label='R²')\n\naxes[1].set_xticks(x_pos)\naxes[1].set_xticklabels(models_comparison['Modèle'], rotation=45, ha='right')\naxes[1].set_ylabel('Score')\naxes[1].set_title('Comparaison Modèles de Régression')\naxes[1].legend()\n\n# Annoter les barres\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        axes[1].annotate(f'{height:.3f}',\n                        xy=(bar.get_x() + bar.get_width() / 2, height),\n                        xytext=(0, 3),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CONSÉILS POUR SVR\")\nprint(\"=\" * 70)\nprint(\"\"\"\n• SVR est puissant mais COMPUTATIONNELLEMENT COÛTEUX\n• Scaling des features OBLIGATOIRE\n• GridSearch peut être très long\n• Pour grands datasets, considérez:\n  - LinearSVR (plus rapide que SVR kernel='linear')\n  - Réduction de features (PCA) avant SVR\n  - Échantillonnage pour hyperparamètres\n\"\"\")\n```\n:::\n\n## 7. Comparaison Finale de Tous les Modèles\n\n### Exercice 7.1: Tableau de Bord Complet\n\nCréez un tableau de bord comparatif de tous les modèles.\n\n::: {.callout-note collapse=\"true\"}\n```python\nprint(\"=\" * 70)\nprint(\"TABLEAU DE BORD COMPARATIF\")\nprint(\"=\" * 70)\n\n# Collecte de tous les résultats\nall_results = []\n\n# Fonction pour ajouter un modèle\ndef add_result(name, y_pred, metrics_func=evaluate_model):\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    return {'Modèle': name, 'MAE': mae, 'RMSE': rmse, 'R²': r2}\n\n# Ajout de tous les modèles\nall_results.append(add_result('Linear', y_test_pred))\nall_results.append(add_result('Ridge (CV)', y_pred_ridge))\nall_results.append(add_result('Lasso (CV)', y_pred_lasso))\nall_results.append(add_result('ElasticNet (CV)', y_pred_elastic))\nall_results.append(add_result('SVR Linear', y_pred_svr_lin))\nall_results.append(add_result('SVR RBF', y_pred_svr_rbf))\n\ndf_all_results = pd.DataFrame(all_results)\n\n# Tri par R²\ndf_all_results = df_all_results.sort_values('R²', ascending=False).reset_index(drop=True)\n\nprint(\"\\nClassement par R²:\")\nprint(df_all_results.to_string(index=False))\n\n# Visualisation\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\n# 1. Barplot R²\nx_pos = np.arange(len(df_all_results))\naxes[0, 0].barh(x_pos, df_all_results['R²'], color='skyblue')\naxes[0, 0].set_yticks(x_pos)\naxes[0, 0].set_yticklabels(df_all_results['Modèle'])\naxes[0, 0].set_xlabel('R² Score')\naxes[0, 0].set_title('Comparaison R² des Modèles')\naxes[0, 0].invert_yaxis()  # Meilleur en haut\n\n# Annoter les valeurs\nfor i, v in enumerate(df_all_results['R²']):\n    axes[0, 0].text(v + 0.001, i, f'{v:.4f}', va='center')\n\n# 2. Comparaison MAE vs RMSE\nscatter = axes[0, 1].scatter(df_all_results['MAE'], df_all_results['RMSE'], \n                             s=200, alpha=0.6, c=df_all_results['R²'], cmap='viridis')\nfor i, row in df_all_results.iterrows():\n    axes[0, 1].annotate(row['Modèle'], (row['MAE'], row['RMSE']), fontsize=8)\naxes[0, 1].set_xlabel('MAE')\naxes[0, 1].set_ylabel('RMSE')\naxes[0, 1].set_title('MAE vs RMSE (couleur = R²)')\nplt.colorbar(scatter, ax=axes[0, 1], label='R² Score')\n\n# 3. Temps d'entraînement (exemple)\n# Dans un cas réel, on mesurerait le temps\ntraining_times = [0.1, 0.2, 0.3, 0.4, 2.0, 5.0]  # valeurs d'exemple\ndf_all_results['Temps(s)'] = training_times\n\naxes[1, 0].scatter(df_all_results['Temps(s)'], df_all_results['R²'], s=100)\nfor i, row in df_all_results.iterrows():\n    axes[1, 0].annotate(row['Modèle'], (row['Temps(s)'], row['R²']), fontsize=8)\naxes[1, 0].set_xlabel('Temps d\\'entraînement (s)')\naxes[1, 0].set_ylabel('R² Score')\naxes[1, 0].set_title('Performance vs Temps d\\'entraînement')\naxes[1, 0].grid(alpha=0.3)\n\n# 4. Heatmap des métriques\nmetrics_df = df_all_results.set_index('Modèle')[['MAE', 'RMSE', 'R²']]\nsns.heatmap(metrics_df, annot=True, fmt='.4f', cmap='YlOrRd', \n            center=0, ax=axes[1, 1], cbar_kws={'label': 'Valeur'})\naxes[1, 1].set_title('Heatmap des Métriques par Modèle')\naxes[1, 1].tick_params(axis='x', rotation=45)\naxes[1, 1].tick_params(axis='y', rotation=0)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RECOMMANDATIONS FINALES\")\nprint(\"=\" * 70)\n\n# Recommandation basée sur différents critères\nprint(\"\"\"\n1. Pour PERFORMANCE MAX (R²):\n   → {} (R²={:.4f})\n   \n2. Pour INTERPRÉTABILITÉ (coefficients):\n   → Lasso (CV) (sélection de features)\n   \n3. Pour RAPIDITÉ:\n   → Linear ou Ridge (CV)\n   \n4. Pour COMPLEXITÉ NON-LINÉAIRE:\n   → SVR RBF (mais plus lent)\n   \n5. COMPROMIS PERFORMANCE/TEMPS:\n   → ElasticNet (CV)\n\"\"\".format(df_all_results.iloc[0]['Modèle'], df_all_results.iloc[0]['R²']))\n\n# Sauvegarde des résultats\ndf_all_results.to_csv('resultats_regression_comparaison.csv', index=False)\nprint(\"\\nRésultats sauvegardés dans 'resultats_regression_comparaison.csv'\")\n```\n:::\n\n## 8. Projet Bonus: Regression Avancée\n\n### Exercice 8.1: Ensemble Methods\n\nTestez Random Forest et Gradient Boosting pour la régression.\n\n```python\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# TODO: Implémentez Random Forest et Gradient Boosting\n# Optimisez avec RandomizedSearchCV\n# Comparez avec les modèles linéaires\n```\n\n**Indices:**\n- Pas besoin de standardisation pour les arbres\n- Optimisez: n_estimators, max_depth, min_samples_split\n- Métrique: RMSE ou MAE\n- Visualisez l'importance des features\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 8.1\n\n```python\nprint(\"=\" * 70)\nprint(\"ENSEMBLE METHODS: RANDOM FOREST & GRADIENT BOOSTING\")\nprint(\"=\" * 70)\n\n# Pas besoin de scaling pour les arbres\nX_train_trees = X_train\nX_test_trees = X_test\n\n# 1. Random Forest\nprint(\"\\n1. Random Forest Regressor...\")\nrf = RandomForestRegressor(random_state=42, n_jobs=-1)\n\n# Hyperparamètres\nparam_dist_rf = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf_random = RandomizedSearchCV(\n    rf, param_dist_rf, n_iter=20, cv=3, \n    scoring='neg_mean_squared_error', n_jobs=-1, random_state=42\n)\n\nrf_random.fit(X_train_trees, y_train)\nbest_rf = rf_random.best_estimator_\ny_pred_rf = best_rf.predict(X_test_trees)\n\nprint(f\"Meilleurs paramètres RF: {rf_random.best_params_}\")\nrf_metrics = evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n\n# 2. Gradient Boosting\nprint(\"\\n2. Gradient Boosting Regressor...\")\ngbr = GradientBoostingRegressor(random_state=42)\n\nparam_dist_gbr = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 4, 5],\n    'subsample': [0.8, 0.9, 1.0]\n}\n\ngbr_random = RandomizedSearchCV(\n    gbr, param_dist_gbr, n_iter=15, cv=3,\n    scoring='neg_mean_squared_error', n_jobs=-1, random_state=42\n)\n\ngbr_random.fit(X_train_trees, y_train)\nbest_gbr = gbr_random.best_estimator_\ny_pred_gbr = best_gbr.predict(X_test_trees)\n\nprint(f\"Meilleurs paramètres GBR: {gbr_random.best_params_}\")\ngbr_metrics = evaluate_model(y_test, y_pred_gbr, \"Gradient Boosting\")\n\n# 3. Comparaison\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COMPARAISON MODÈLES AVANCÉS\")\nprint(\"=\" * 70)\n\nensemble_results = pd.DataFrame([\n    {'Modèle': 'Random Forest', 'MAE': rf_metrics['MAE'], \n     'RMSE': rf_metrics['RMSE'], 'R²': rf_metrics['R2']},\n    {'Modèle': 'Gradient Boosting', 'MAE': gbr_metrics['MAE'], \n     'RMSE': gbr_metrics['RMSE'], 'R²': gbr_metrics['R2']},\n    {'Modèle': 'Meilleur Linéaire', 'MAE': df_all_results.iloc[0]['MAE'],\n     'RMSE': df_all_results.iloc[0]['RMSE'], 'R²': df_all_results.iloc[0]['R²']}\n])\n\nprint(ensemble_results.to_string(index=False))\n\n# 4. Importance des features\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Random Forest\nrf_importances = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Importance': best_rf.feature_importances_\n}).sort_values('Importance', ascending=True)\n\naxes[0].barh(rf_importances['Feature'], rf_importances['Importance'])\naxes[0].set_xlabel('Importance')\naxes[0].set_title('Importance des Features - Random Forest')\n\n# Gradient Boosting\ngbr_importances = pd.DataFrame({\n    'Feature': california.feature_names,\n    'Importance': best_gbr.feature_importances_\n}).sort_values('Importance', ascending=True)\n\naxes[1].barh(gbr_importances['Feature'], gbr_importances['Importance'])\naxes[1].set_xlabel('Importance')\naxes[1].set_title('Importance des Features - Gradient Boosting')\n\nplt.tight_layout()\nplt.show()\n\n# 5. Visualisation prédictions\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].scatter(y_test, y_pred_rf, alpha=0.3, s=10)\naxes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\naxes[0].set_xlabel('Vraie valeur')\naxes[0].set_ylabel('Prédiction')\naxes[0].set_title(f'Random Forest (R²={rf_metrics[\"R2\"]:.3f})')\naxes[0].grid(alpha=0.3)\n\naxes[1].scatter(y_test, y_pred_gbr, alpha=0.3, s=10)\naxes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\naxes[1].set_xlabel('Vraie valeur')\naxes[1].set_ylabel('Prédiction')\naxes[1].set_title(f'Gradient Boosting (R²={gbr_metrics[\"R2\"]:.3f})')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CONCLUSION ENSEMBLE METHODS\")\nprint(\"=\" * 70)\nprint(\"\"\"\n• Random Forest et Gradient Boosting PERFORMENT TRÈS BIEN\n• Pas besoin de feature scaling\n• Capturent relations non-linéaires complexes\n• MOINS INTERPRÉTABLES que les modèles linéaires\n• PLUS LENTS à entraîner\n• Risque de surapprentissage si pas bien régularisés\n\n→ Recommandation: Utiliser pour compétitions Kaggle\n→ Pour production: Privilégier modèles plus simples si performance similaire\n\"\"\")\n```\n:::\n\n## Conclusion\n\n### Résumé des Points Clés\n\n1. **Prétraitement**:\n   - Standardisation cruciale pour modèles régularisés et SVR\n   - Split stratifié (si target stratifiable)\n   - Pas de data leakage\n\n2. **Modèles Linéaires**:\n   - **Linear**: Simple, rapide, interprétable\n   - **Ridge**: Régularisation L2, réduit overfitting\n   - **Lasso**: Régularisation L1, sélection features\n   - **ElasticNet**: Compromis L1+L2\n\n3. **Modèles Non-Linéaires**:\n   - **SVR**: Puissant mais lent, sensible aux hyperparamètres\n   - **Random Forest**: Robuste, capture non-linéarités\n   - **Gradient Boosting**: Souvent meilleure performance\n\n4. **Optimisation**:\n   - Utiliser *CV (RidgeCV, LassoCV) pour alpha automatique\n   - GridSearch pour petit espace\n   - RandomizedSearch pour grand espace\n\n5. **Évaluation**:\n   - Multiples métriques: MAE, RMSE, R²\n   - Visualisations: résidus, prédictions vs vraies valeurs\n   - Importance des features pour interprétation\n\n### Checklist de Validation\n\nAvant de soumettre votre travail:\n\n- [ ] Exploratory Data Analysis complète\n- [ ] Prétraitement correct (train/test séparés)\n- [ ] Au moins 4 modèles comparés\n- [ ] Optimisation hyperparamètres avec CV\n- [ ] Évaluation sur test set (une seule fois)\n- [ ] Visualisations claires et annotées\n- [ ] Interprétation des résultats\n- [ ] Code commenté et organisé\n\n### Pour Aller Plus Loin\n\n**Extensions possibles:**\n\n1. **Feature Engineering**:\n   - Créer interactions entre features\n   - Transformations polynomiales\n   - Variables dummy pour catégorielles\n\n2. **Pipeline Scikit-learn**:\n   ```python\n   from sklearn.pipeline import Pipeline\n   from sklearn.compose import ColumnTransformer\n   \n   pipeline = Pipeline([\n       ('scaler', StandardScaler()),\n       ('regressor', RidgeCV())\n   ])\n   ```\n\n3. **Validation Croisée Temporelle**:\n   - Pour données chronologiques\n   - TimeSeriesSplit au lieu de KFold\n\n4. **Prédiction d'Intervalles**:\n   - Quantile Regression\n   - Bootstrap pour incertitude\n\n5. **Déploiement**:\n   - Sauvegarde modèle (joblib)\n   - API avec FastAPI/Flask\n   - Monitoring des performances\n\n**Exercices supplémentaires:**\n\n1. Testez PolynomialFeatures + Regression\n2. Implémentez une validation croisée imbriquée\n3. Ajoutez XGBoost ou LightGBM à la comparaison\n4. Créez un dashboard interactif avec Plotly\n\n**Prochain TP:** Séries Temporelles ou Deep Learning\n\n:::{.callout-tip}\n## Astuce Finale\n\n**La meilleure pratique:** Commencez toujours par un modèle simple (régression linéaire), puis complexifiez si nécessaire. Souvent, les modèles simples suffisent et sont plus faciles à maintenir en production!\n:::\n\n",
    "supporting": [
      "seance8_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}