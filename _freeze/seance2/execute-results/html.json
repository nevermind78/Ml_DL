{
  "hash": "260ddf1634619d5980294e81f901cedc",
  "result": {
    "engine": "jupyter",
    "markdown": "# Séance 2: Apprentissage Supervisé - Classification\n\n::: {.callout-note icon=false}\n## Informations de la séance\n- **Type**: Cours\n- **Durée**: 2h\n- **Objectifs**: Obj5, Obj6\n:::\n\n## 1. Introduction à la Classification\n\n### 1.1 Définition\n\nLa **classification** est une tâche d'apprentissage supervisé où l'objectif est de prédire une **classe** ou **catégorie** discrète à partir de caractéristiques d'entrée.\n\n::: {.callout-note}\n## Exemple\n**Entrée**: Caractéristiques d'un email (mots, expéditeur, longueur, etc.)  \n**Sortie**: Classe = \"Spam\" ou \"Non Spam\"\n:::\n\n### 1.2 Différence Classification vs Régression\n\n| Caractéristique | Classification | Régression |\n|----------------|----------------|------------|\n| **Sortie** | Catégorie discrète | Valeur continue |\n| **Exemple** | Spam/Non spam | Prix d'une maison |\n| **Métrique** | Accuracy, F1-score | MAE, RMSE |\n| **Fonction** | Probabilité → Classe | Valeur numérique |\n\n## 2. Types de Classification\n\n### 2.1 Classification Binaire\n\nDeux classes possibles: 0 ou 1, Vrai ou Faux, Positif ou Négatif\n\n**Exemples**:\n\n- Détection de spam (spam/non spam)\n- Diagnostic médical (malade/sain)\n- Détection de fraude (fraude/légitime)\n- Approbation de crédit (approuvé/rejeté)\n\n::: {#1f232f66 .cell execution_count=1}\n``` {.python .cell-code}\n# Exemple: Classification binaire\ny_binary = [0, 1, 1, 0, 1, 0, 0, 1]  # 0 = négatif, 1 = positif\n```\n:::\n\n\n### 2.2 Classification Multi-classes\n\nPlus de deux classes **mutuellement exclusives** (une seule classe par instance)\n\n**Exemples**:\n\n- Reconnaissance de chiffres manuscrits (0-9 = 10 classes)\n- Classification de fleurs Iris (Setosa, Versicolor, Virginica)\n- Catégorisation d'articles (Sport, Politique, Économie, Culture)\n\n::: {#ad6c96d2 .cell execution_count=2}\n``` {.python .cell-code}\n# Exemple: Classification multi-classes\ny_multiclass = [0, 1, 2, 1, 0, 2, 1]  # 3 classes: 0, 1, 2\n```\n:::\n\n\n### 2.3 Classification Multi-label\n\nPlusieurs classes **simultanées** possibles pour une instance\n\n**Exemples**:\n\n- Étiquetage de photos (peut contenir: personne, chien, extérieur)\n- Catégorisation de films (peut être: Action, Comédie, Drame)\n- Analyse de sentiments multiple (joie + surprise)\n\n::: {#25688345 .cell execution_count=3}\n``` {.python .cell-code}\n# Exemple: Classification multi-label\ny_multilabel = [\n    [1, 0, 1],  # instance a les labels 0 et 2\n    [0, 1, 1],  # instance a les labels 1 et 2\n    [1, 1, 0]   # instance a les labels 0 et 1\n]\n```\n:::\n\n\n## 3. Algorithmes de Classification\n\n### 3.1 Arbre de Décision (Decision Tree)\n\nModèle qui prend des décisions basées sur des questions successives.\n\n#### Principe\n\nL'arbre divise l'espace des caractéristiques en régions par des questions binaires.\n\n\n**Diagramme :**\n```{mermaid}\ngraph TD\n    A[Age > 30?] -->|Oui| B[Revenu > 50k?]\n    A -->|Non| C[Étudiant?]\n    B -->|Oui| D[Approuvé ]\n    B -->|Non| E[Rejeté ]\n    C -->|Oui| F[Rejeté ]\n    C -->|Non| G[Approuvé ]\n```\n\n\n#### Avantages\n- Facile à interpréter et visualiser\n- Pas besoin de normalisation des données\n- Gère les données non linéaires\n- Gère les variables catégorielles et numériques\n\n#### Inconvénients\n- Tendance à l'overfitting\n- Instable (petits changements de données → arbre différent)\n- Biais vers les classes majoritaires\n\n::: {#e27249b5 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Chargement des données\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Entraînement\nclf = DecisionTreeClassifier(max_depth=3, random_state=42)\nclf.fit(X_train, y_train)\n\n# Prédiction\ny_pred = clf.predict(X_test)\nprint(f\"Accuracy: {clf.score(X_test, y_test):.2f}\")\n```\n:::\n\n\n### 3.2 Random Forest\nEnsemble d'arbres de décision qui votent ensemble.\n\n#### Principe\n1. Créer N arbres sur des sous-ensembles aléatoires de données\n2. Chaque arbre vote pour une classe\n3. Prédiction finale = vote majoritaire\n\n#### Avantages\n- Très performant et robuste\n- Réduit l'overfitting par rapport à un arbre unique\n- Gère bien les grandes dimensions\n- Donne l'importance des features\n\n#### Inconvénients\n- Moins interprétable qu'un arbre unique\n- Plus lent à entraîner et prédire\n- Mémoire importante\n\n::: {#28e5783d .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Entraînement\nrf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\nrf.fit(X_train, y_train)\n\n# Prédiction\ny_pred = rf.predict(X_test)\nprint(f\"Accuracy: {rf.score(X_test, y_test):.2f}\")\n\n# Importance des features\nimportances = rf.feature_importances_\nfor i, imp in enumerate(importances):\n    print(f\"Feature {iris.feature_names[i]}: {imp:.3f}\")\n```\n:::\n\n\n### 3.3 Support Vector Machine (SVM)\n\nTrouve l'hyperplan optimal qui sépare les classes avec la marge maximale.\n\n#### Principe\n\n- **Marge**: distance entre l'hyperplan et les points les plus proches (vecteurs de support)\n- **Objectif**: Maximiser cette marge\n- **Kernel trick**: Permet de gérer des données non linéairement séparables\n\n::: {#a968ff27 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.svm import SVC\n\n# SVM linéaire\nsvm_linear = SVC(kernel='linear', C=1.0)\nsvm_linear.fit(X_train, y_train)\n\n# SVM avec kernel RBF (pour données non linéaires)\nsvm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\nsvm_rbf.fit(X_train, y_train)\n\nprint(f\"SVM Linear Accuracy: {svm_linear.score(X_test, y_test):.2f}\")\nprint(f\"SVM RBF Accuracy: {svm_rbf.score(X_test, y_test):.2f}\")\n```\n:::\n\n\n#### Avantages\n- Très efficace en haute dimension\n- Robuste aux outliers\n- Versatile (différents kernels)\n\n#### Inconvénients\n- Lent sur de grandes données\n- Difficile à interpréter\n- Sensible au choix des hyperparamètres\n\n### 3.4 Naïve Bayes\n\nBasé sur le théorème de Bayes avec hypothèse d'indépendance des features.\n\n#### Principe - Théorème de Bayes\n\n$$P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}$$\n\nOù:\n\n- $P(y|X)$ = probabilité de la classe $y$ sachant les features $X$ (posterior)\n- $P(X|y)$ = vraisemblance\n- $P(y)$ = probabilité a priori de la classe\n- $P(X)$ = évidence (constante)\n\n#### Hypothèse \"Naïve\"\n\nLes features sont **indépendantes** conditionnellement à la classe:\n\n$$P(X|y) = P(x_1|y) \\cdot P(x_2|y) \\cdot ... \\cdot P(x_n|y)$$\n\n::: {#cd52d5b8 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\n# Gaussian Naive Bayes (pour features continues)\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nprint(f\"Gaussian NB Accuracy: {gnb.score(X_test, y_test):.2f}\")\n\n# Multinomial NB (pour comptages, ex: mots dans un texte)\n# mnb = MultinomialNB()\n# mnb.fit(X_train_counts, y_train)\n```\n:::\n\n\n#### Avantages\n- Très rapide (entraînement et prédiction)\n- Fonctionne bien avec peu de données\n- Excellent pour la classification de texte\n\n#### Inconvénients\n- Hypothèse d'indépendance rarement vraie\n- Performance limitée si hypothèse violée\n\n### 3.5 Régression Logistique\n\n**Attention**: Malgré son nom, c'est un algorithme de **classification** !\n\n#### Principe\n\nModèle linéaire qui utilise la fonction sigmoïde pour produire des probabilités.\n\n**Fonction sigmoïde**:\n$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n\n**Modèle**:\n$$P(y=1|X) = \\sigma(w^T X + b) = \\frac{1}{1 + e^{-(w^T X + b)}}$$\n\n#### Interprétation Probabiliste\n\n- Sortie $\\in [0, 1]$ : probabilité d'appartenance à la classe positive\n- Si $P(y=1|X) \\geq 0.5$ → prédiction = classe 1\n- Si $P(y=1|X) < 0.5$ → prédiction = classe 0\n\n::: {#492b4ed6 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\n# Entraînement\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X_train, y_train)\n\n# Prédiction de classes\ny_pred = log_reg.predict(X_test)\n\n# Prédiction de probabilités\ny_proba = log_reg.predict_proba(X_test)\n\nprint(f\"Accuracy: {log_reg.score(X_test, y_test):.2f}\")\nprint(f\"\\nPremière prédiction:\")\nprint(f\"  Probabilités: {y_proba[0]}\")\nprint(f\"  Classe prédite: {y_pred[0]}\")\n```\n:::\n\n\n#### Avantages\n- Simple et interprétable\n- Donne des probabilités (utile pour la prise de décision)\n- Peu de paramètres à ajuster\n- Fonctionne bien sur données linéairement séparables\n\n#### Inconvénients\n- Assume une relation linéaire\n- Sensible aux outliers\n- Nécessite feature engineering pour les relations non linéaires\n\n### 3.6 k-Nearest Neighbors (k-NN)\n\nClassification basée sur la proximité avec les voisins.\n\n#### Principe\n1. Calculer la distance entre le nouveau point et tous les points d'entraînement\n2. Sélectionner les k points les plus proches\n3. Vote majoritaire parmi ces k voisins\n\n::: {#6ec19aec .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# k=5 voisins\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\nprint(f\"k-NN Accuracy: {knn.score(X_test, y_test):.2f}\")\n```\n:::\n\n\n#### Avantages\n- Simple et intuitif\n- Pas d'entraînement (lazy learning)\n- Fonctionne bien pour des frontières complexes\n\n#### Inconvénients\n- Lent pour la prédiction (calcule toutes les distances)\n- Sensible à l'échelle des features (nécessite normalisation)\n- Curse of dimensionality (mauvais en haute dimension)\n\n## 4. Critères d'Évaluation (Aperçu)\n\n### 4.1 Métriques Principales\n\n- **Accuracy**: Proportion de prédictions correctes\n- **Precision**: Proportion de vrais positifs parmi les prédictions positives\n- **Recall**: Proportion de vrais positifs parmi les cas réellement positifs\n- **F1-Score**: Moyenne harmonique de Precision et Recall\n\n::: {.callout-important}\nCes métriques seront détaillées en profondeur dans la **Séance 5 - TD2**\n:::\n\n### 4.2 Exemple Simple\n\n::: {#9f0e0e3e .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Calcul de l'accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Rapport complet\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))\n```\n:::\n\n\n## 5. Exemple Complet: Comparaison d'Algorithmes\n\n::: {#4a5cb865 .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Création d'un dataset synthétique\nX, y = make_classification(\n    n_samples=1000, \n    n_features=2, \n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=42\n)\n\n# Split et normalisation\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Entraînement de plusieurs modèles\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodels = {\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(kernel='rbf', random_state=42),\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Naive Bayes': GaussianNB(),\n    'k-NN': KNeighborsClassifier(n_neighbors=5)\n}\n\n# Comparaison des performances\nresults = {}\nfor name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    score = model.score(X_test_scaled, y_test)\n    results[name] = score\n    print(f\"{name:20s}: {score:.4f}\")\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nplt.barh(list(results.keys()), list(results.values()))\nplt.xlabel('Accuracy')\nplt.title('Comparaison des Algorithmes de Classification')\nplt.xlim([0, 1])\nplt.grid(axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Résumé de la Séance\n\n::: {.callout-important icon=false}\n## Points clés à retenir\n\n1. **Classification** = prédire une catégorie discrète\n2. **Types**: Binaire, Multi-classes, Multi-label\n3. **Algorithmes principaux**:\n   - Decision Tree: interprétable mais tendance à l'overfitting\n   - Random Forest: robuste et performant\n   - SVM: excellent en haute dimension\n   - Naïve Bayes: rapide, bon pour le texte\n   - Régression Logistique: simple, interprétable, probabiliste\n   - k-NN: simple mais coûteux en prédiction\n4. **Choix du modèle** dépend de: taille des données, interprétabilité, performance, ressources\n5. **Évaluation** avec métriques appropriées (détails en TD2)\n:::\n\n## Exercices\n\n::: {.callout-warning icon=false}\n## Exercice 1\nImplémentez une régression logistique sur le dataset Iris et analysez les coefficients appris. Que représentent-ils?\n:::\n\n::: {.callout-note collapse=\"true\"}\n\n## Réponse 1\n\n**Code d'implémentation :**\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Chargement et entraînement\niris = load_iris()\nX, y = iris.data, iris.target\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(X, y)\n\n# Analyse des coefficients\nprint(f\"Coefficients (W): \\n{log_reg.coef_}\")\nprint(f\"Intercept (b): {log_reg.intercept_}\")\n\n```\n\n**Explication des coefficients :**\n\n* **Signification** : Chaque coefficient  représente l'importance d'une caractéristique (feature) pour prédire une classe donnée.\n* **Direction** : Un coefficient positif augmente la probabilité que l'instance appartienne à la classe, tandis qu'un coefficient négatif la diminue.\n* **Ampleur** : Plus la valeur absolue du coefficient est élevée, plus la caractéristique a un impact déterminant sur la décision du modèle.\n:::\n\n::: {.callout-warning icon=false}\n## Exercice 2\nComparez les performances de Decision Tree vs Random Forest sur le dataset digits de sklearn. Expliquez les différences observées.\n:::\n\n::: {.callout-note collapse=\"true\"}\n\n## Réponse 2\n\n**Comparaison théorique et pratique :**\n\n1. **Performance** : Le **Random Forest** obtient généralement une meilleure précision (accuracy) que l'arbre seul car il combine les prédictions de nombreux arbres, réduisant ainsi les erreurs aléatoires.\n2. **Stabilité** : Un **Decision Tree** est très sensible aux petites variations des données (variance élevée). Le Random Forest stabilise cela par échantillonnage (bagging).\n3. **Sur-apprentissage (Overfitting)** : L'arbre de décision a tendance à mémoriser le bruit des données s'il n'est pas limité en profondeur. Le Random Forest limite ce risque en moyennant les résultats de plusieurs arbres entraînés sur des sous-ensembles différents.\n:::\n\n\n\n::: {.callout-warning icon=false}\n## Exercice 3\nPour un problème de détection de fraude bancaire, quel algorithme recommanderiez-vous et pourquoi? Considérez les aspects: interprétabilité, temps réel, déséquilibre des classes.\n:::\n\n::: {.callout-note collapse=\"true\"}\n\n## Réponse 3\n\nPour la détection de fraude, je recommanderais le **Random Forest** ou le **XGBoost** (méthodes d'ensemble), pour les raisons suivantes :\n\n1. **Interprétabilité** : Bien que moins direct qu'un arbre unique, le Random Forest permet d'extraire l'**importance des variables**, ce qui est crucial pour comprendre quels facteurs (montant, lieu, heure) déclenchent une alerte de fraude.\n2. **Déséquilibre des classes** : La fraude est rare (classe minoritaire). Ces algorithmes gèrent mieux les données déséquilibrées grâce à des techniques de pondération des classes.\n3. **Temps réel** : Une fois entraîné, la prédiction d'un Random Forest est très rapide, permettant de valider ou bloquer une transaction en quelques millisecondes.\n4. **Robustesse** : Ces modèles sont moins sensibles aux valeurs aberrantes (outliers) souvent présentes dans les données financières.\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Lectures Complémentaires\n\n1. Géron, A. (2019) - Chapitre 3: Classification\n2. Scikit-learn Documentation: [Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html)\n3. [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)\n\n",
    "supporting": [
      "seance2_files"
    ],
    "filters": [],
    "includes": {}
  }
}