{
  "hash": "f2e8f90c86e482061fa082b5776e9c99",
  "result": {
    "engine": "jupyter",
    "markdown": "# Séance 5: TD2 - Critères d'Évaluation\n\n::: {.callout-note icon=false}\n## Informations de la séance\n- **Type**: Travaux Dirigés\n- **Durée**: 2h\n- **Objectifs**: Obj4\n:::\n\n## Introduction\n\nL'évaluation correcte d'un modèle de classification est cruciale. Choisir la mauvaise métrique peut conduire à des conclusions erronées et à des modèles inadaptés en production. Dans ce TD, nous allons explorer en profondeur les différentes métriques d'évaluation.\n\n## 1. La Matrice de Confusion\n\n### 1.1 Définition et Structure\n\nLa **matrice de confusion** est un tableau qui visualise les performances d'un modèle de classification en comparant les prédictions aux vraies valeurs.\n\nPour un problème **binaire**:\n\n```\n                    Prédiction\n                 Négatif  Positif\nRéalité  Négatif    TN       FP\n         Positif    FN       TP\n```\n\nOù:\n\n- **TP (True Positive)**: Vrais Positifs - correctement classifiés comme positifs\n- **TN (True Negative)**: Vrais Négatifs - correctement classifiés comme négatifs\n- **FP (False Positive)**: Faux Positifs - incorrectement classifiés comme positifs (Erreur Type I)\n- **FN (False Negative)**: Faux Négatifs - incorrectement classifiés comme négatifs (Erreur Type II)\n\n::: {.callout-tip}\n## Mnémotechnique\n- **Type I (FP)**: Fausse alarme - \"On crie au loup alors qu'il n'y a pas de loup\"\n- **Type II (FN)**: Manque - \"On ne voit pas le loup alors qu'il est là\"\n:::\n\n### 1.2 Exemple Concret: Détection de Maladie\n\nConsidérez un test médical pour une maladie:\n\n| Patient | Vraie Classe | Prédiction | Résultat |\n|---------|--------------|------------|----------|\n| 1 | Malade | Malade | TP $\\checkmark$ |\n| 2 | Malade | Sain | FN X (Dangereux!) |\n| 3 | Sain | Malade | FP X (Fausse alarme) |\n| 4 | Sain | Sain | TN $\\checkmark$ |\n| 5 | Malade | Malade | TP $\\checkmark$ |\n| 6 | Sain | Sain | TN $\\checkmark$ |\n| 7 | Malade | Sain | FN X (Dangereux!) |\n| 8 | Sain | Malade | FP X (Fausse alarme) |\n\nMatrice de confusion:\n\n```\n              Prédiction\n           Sain  Malade\nRéalité Sain   2      2      (4 sains)\n      Malade   2      2      (4 malades)\n```\n\n- **TP = 2**: Malades correctement détectés\n- **TN = 2**: Sains correctement identifiés\n- **FP = 2**: Sains diagnostiqués malades (traitement inutile)\n- **FN = 2**: Malades non détectés (très dangereux!)\n\n### Exercice 1.1: Construction de Matrice de Confusion\n\nSoit les prédictions suivantes pour un détecteur de spam (1 = Spam, 0 = Ham):\n\n```python\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\ny_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n```\n\n**Questions:**\n1. Construisez la matrice de confusion\n2. Calculez TP, TN, FP, FN\n3. Interprétez chaque type d'erreur dans ce contexte\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 1.1\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ny_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 0, 1, 1, 0])\n\n# 1. Matrice de confusion\ncm = confusion_matrix(y_true, y_pred)\nprint(\"1. Matrice de Confusion:\")\nprint(cm)\n\n# Visualisation\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Ham (0)', 'Spam (1)'],\n            yticklabels=['Ham (0)', 'Spam (1)'])\nplt.ylabel('Vraie Classe')\nplt.xlabel('Prédiction')\nplt.title('Matrice de Confusion - Détection de Spam')\nplt.tight_layout()\nplt.show()\n\n# 2. Calcul manuel\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\n2. Valeurs:\")\nprint(f\"TP (Vrais Positifs) = {tp}\")\nprint(f\"TN (Vrais Négatifs) = {tn}\")\nprint(f\"FP (Faux Positifs) = {fp}\")\nprint(f\"FN (Faux Négatifs) = {fn}\")\n\n# Vérification manuelle\nprint(\"\\n3. Interprétation:\")\nprint(f\"TP = {tp}: Spams correctement détectés\")\nprint(f\"TN = {tn}: Hams correctement identifiés\")\nprint(f\"FP = {fp}: Hams classés comme spam (vont en indésirables)\")\nprint(f\"FN = {fn}: Spams non détectés (arrivent en boîte de réception)\")\n\n# Analyse détaillée\nprint(\"\\nAnalyse détaillée des prédictions:\")\nfor i, (true, pred) in enumerate(zip(y_true, y_pred)):\n    status = \"\"\n    if true == 1 and pred == 1:\n        status = \"TP $\\checkmark$\"\n    elif true == 0 and pred == 0:\n        status = \"TN $\\checkmark$\"\n    elif true == 0 and pred == 1:\n        status = \"FP X\"\n    elif true == 1 and pred == 0:\n        status = \"FN X\"\n    \n    true_label = \"Spam\" if true == 1 else \"Ham\"\n    pred_label = \"Spam\" if pred == 1 else \"Ham\"\n    print(f\"Email {i+1}: Vrai={true_label}, Prédit={pred_label} → {status}\")\n```\n:::\n\n\n**Réponses:**\n\n1. Matrice: `[[4, 1], [1, 4]]`\n2. TP=4, TN=4, FP=1, FN=1\n3. Interprétation:\n\n   - **FP (1 email)**: Email légitime envoyé dans spam (utilisateur peut manquer info importante)\n   - **FN (1 email)**: Spam non détecté dans boîte réception (nuisance mineure)\n:::\n\n## 2. Métriques Dérivées\n\n### 2.1 Accuracy (Exactitude)\n\n**Définition**: Proportion de prédictions correctes\n\n$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(y_true, y_pred)\n# ou manuellement:\naccuracy = (tp + tn) / (tp + tn + fp + fn)\n```\n:::\n\n\n::: {.callout-warning}\n## Piège de l'Accuracy!\n\nL'accuracy peut être **trompeuse** avec des classes déséquilibrées!\n\n**Exemple**: Détection de fraude\n\n- 990 transactions légitimes, 10 frauduleuses\n- Modèle naïf qui prédit toujours \"légitime\"\n- Accuracy = 990/1000 = **99%** \n- Mais 0% de fraudes détectées!\n:::\n\n### 2.2 Precision (Précision)\n\n**Définition**: Proportion de prédictions positives qui sont correctes\n\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n\n**Question répondue**: \"Parmi tous les cas prédits positifs, combien le sont vraiment?\"\n\n**Interprétation**:\n\n- Haute précision → Peu de faux positifs\n- Important quand le coût d'un FP est élevé\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_score\n\nprecision = precision_score(y_true, y_pred)\n# ou manuellement:\nprecision = tp / (tp + fp)\n```\n:::\n\n\n**Exemples où la Precision est cruciale**:\n\n1. **Recommandation de produits**: Ne pas recommander des produits non pertinents\n2. **Filtrage spam**: Ne pas mettre d'emails importants dans spam\n3. **Détection de visages**: Ne pas identifier de faux visages\n\n### 2.3 Recall (Rappel) ou Sensitivity (Sensibilité)\n\n**Définition**: Proportion de vrais positifs correctement identifiés\n\n$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n\n**Question répondue**: \"Parmi tous les cas réellement positifs, combien ai-je détectés?\"\n\n**Interprétation**:\n- Haut recall → Peu de faux négatifs\n- Important quand le coût d'un FN est élevé\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import recall_score\n\nrecall = recall_score(y_true, y_pred)\n# ou manuellement:\nrecall = tp / (tp + fn)\n```\n:::\n\n\n**Exemples où le Recall est crucial**:\n\n1. **Détection de cancer**: Ne manquer aucun malade\n2. **Détection de fraude**: Détecter toutes les fraudes\n3. **Systèmes de sécurité**: Ne rater aucune menace\n\n### 2.4 Compromis Precision-Recall\n\nIl existe généralement un **compromis** entre Precision et Recall:\n\n\n**Diagramme mermaid (conversion échouée):**\n```\ngraph LR\n    A[Seuil bas<br/>0.3] --> B[Haute Recall<br/>Basse Precision]\n    C[Seuil moyen<br/>0.5]...\n```\n\n\n**Exemple concret**:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Modèle de détection de maladie\n# Proba patient malade: 0.6\n\n# Seuil = 0.5: Prédit malade → Haute recall\n# Seuil = 0.8: Prédit sain → Haute precision (mais manque des cas)\n```\n:::\n\n\n### 2.5 F1-Score\n\n**Définition**: Moyenne harmonique de Precision et Recall\n\n$$F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n\n**Pourquoi moyenne harmonique?**\n\n- Pénalise les déséquilibres\n- Si Precision=100% et Recall=1%, F1 $\\approx$ 2% (pas 50.5%)\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(y_true, y_pred)\n# ou manuellement:\nf1 = 2 * (precision * recall) / (precision + recall)\n```\n:::\n\n\n**Quand utiliser F1?**\n- Besoin d'équilibre entre Precision et Recall\n- Classes déséquilibrées\n- Vouloir une seule métrique résumée\n\n### 2.6 Autres Métriques\n\n**Specificity (Spécificité)**:\n\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\nProportion de vrais négatifs correctement identifiés\n\n**F-Beta Score**:\n\n$$F_\\beta = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}$$\n\n- $\\beta > 1$: Favorise le Recall\n- $\\beta < 1$: Favorise la Precision\n\n### Exercice 2.1: Calcul de Métriques\n\nSoit la matrice de confusion suivante pour un détecteur de tumeurs malignes:\n\n```\n              Prédiction\n           Bénigne  Maligne\nRéalité Bénigne    850      50\n       Maligne     20       80\n```\n\n**Questions:**\n\n1. Calculez Accuracy, Precision, Recall, F1-Score\n2. Quelle métrique est la plus importante dans ce contexte? Pourquoi?\n3. Le modèle est-il satisfaisant? Justifiez.\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 2.1\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\n\n# Matrice de confusion\n#          Bénigne  Maligne\n# Bénigne    850      50\n# Maligne     20      80\n\ntn = 850  # Bénigne correctement identifié\nfp = 50   # Bénigne prédit Maligne\nfn = 20   # Maligne prédit Bénigne\ntp = 80   # Maligne correctement identifié\n\ntotal = tn + fp + fn + tp\n\n# 1. Calcul des métriques\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nf1 = 2 * (precision * recall) / (precision + recall)\nspecificity = tn / (tn + fp)\n\nprint(\"1. Métriques calculées:\")\nprint(f\"Accuracy:    {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"Precision:   {precision:.4f} ({precision*100:.2f}%)\")\nprint(f\"Recall:      {recall:.4f} ({recall*100:.2f}%)\")\nprint(f\"F1-Score:    {f1:.4f} ({f1*100:.2f}%)\")\nprint(f\"Specificity: {specificity:.4f} ({specificity*100:.2f}%)\")\n\n# 2. Métrique la plus importante\nprint(\"\\n2. Métrique la plus importante: RECALL\")\nprint(\"   Raison: En médical, ne pas détecter un cancer (FN) est\")\nprint(\"   beaucoup plus grave qu'une fausse alarme (FP).\")\nprint(f\"   Actuellement, Recall = {recall:.2%} signifie que\")\nprint(f\"   {fn} cancers sur {tp+fn} ne sont pas détectés!\")\n\n# 3. Évaluation\nprint(\"\\n3. Évaluation du modèle:\")\nprint(f\"   X Recall de {recall:.2%} est INSUFFISANT\")\nprint(f\"   X 20% de cancers manqués = inacceptable\")\nprint(f\"   $\\checkmark$ Precision de {precision:.2%} est correcte\")\nprint(f\"   $\\checkmark$ Specificity de {specificity:.2%} est bonne\")\nprint(\"\\n   CONCLUSION: Modèle à améliorer!\")\nprint(\"   Recommandation: Abaisser le seuil de décision pour\")\nprint(\"   augmenter le Recall, même au prix de la Precision.\")\n\n# Impact en nombre de patients\nprint(f\"\\nImpact sur 1000 patients:\")\nprint(f\"   - 20 cancers NON DÉTECTÉS (FN) ← CRITIQUE\")\nprint(f\"   - 50 fausses alarmes (FP) ← Acceptable (examens complémentaires)\")\n```\n:::\n\n\n**Réponses:**\n\n1. **Métriques:**\n   - Accuracy: 93.00%\n   - Precision: 61.54%\n   - Recall: 80.00%\n   - F1-Score: 69.57%\n   - Specificity: 94.44%\n\n2. **Métrique importante: RECALL**\n   - Un cancer non détecté (FN) peut être mortel\n   - Une fausse alarme (FP) → examens supplémentaires (acceptable)\n   - Donc: mieux vaut trop détecter que pas assez\n\n3. **Évaluation:**\n   - X Recall de 80% **insuffisant** (20% de cancers manqués)\n   - $\\checkmark$ Specificity correcte\n   - **Conclusion**: Modèle dangereux en l'état\n   - **Action**: Réduire le seuil pour augmenter Recall\n:::\n\n## 3. Courbe ROC et AUC\n\n### 3.1 Courbe ROC (Receiver Operating Characteristic)\n\nLa **courbe ROC** visualise les performances d'un classificateur binaire en variant le seuil de décision.\n\n**Axes:**\n\n- **X**: False Positive Rate (FPR) = $\\frac{FP}{FP + TN}$ = 1 - Specificity\n- **Y**: True Positive Rate (TPR) = $\\frac{TP}{TP + FN}$ = Recall\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Obtenir les probabilités (pas les classes)\ny_proba = model.predict_proba(X_test)[:, 1]\n\n# Calculer les points de la courbe ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\n\n# Tracer la courbe\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, linewidth=2, label='ROC Curve')\nplt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.5)')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Courbe ROC')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n**Interprétation des points:**\n\n- **Point (0, 0)**: Tout prédit négatif (seuil = 1.0)\n- **Point (1, 1)**: Tout prédit positif (seuil = 0.0)\n- **Point (0, 1)**: Classificateur parfait\n- **Diagonale**: Classificateur aléatoire\n\n### 3.2 AUC (Area Under Curve)\n\nL'**AUC** est l'aire sous la courbe ROC.\n\n**Interprétation:**\n\n- **AUC = 1.0**: Classificateur parfait\n- **AUC = 0.9 - 1.0**: Excellent\n- **AUC = 0.8 - 0.9**: Très bon\n- **AUC = 0.7 - 0.8**: Bon\n- **AUC = 0.6 - 0.7**: Médiocre\n- **AUC = 0.5**: Aléatoire (inutile)\n- **AUC < 0.5**: Pire qu'aléatoire (inverser les prédictions!)\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Calcul de l'AUC\nauc = roc_auc_score(y_test, y_proba)\nprint(f\"AUC Score: {auc:.4f}\")\n```\n:::\n\n\n**Avantages de l'AUC:**\n\n- Indépendant du seuil de décision\n- Robuste aux classes déséquilibrées\n- Facile à interpréter (une seule valeur)\n\n**Signification probabiliste:**\n\nAUC = probabilité qu'un exemple positif aléatoire ait un score plus élevé qu'un exemple négatif aléatoire\n\n### Exercice 3.1: Analyse de Courbe ROC\n\nVous avez trois modèles avec les AUC suivants:\n\n- Modèle A: AUC = 0.95\n- Modèle B: AUC = 0.75\n- Modèle C: AUC = 0.52\n\n**Questions:**\n\n1. Classez les modèles par performance\n2. Quel modèle choisiriez-vous pour détecter des fraudes bancaires?\n3. Dans quel cas le Modèle C pourrait-il être utile?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 3.1\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Simulation de courbes ROC\nnp.random.seed(42)\n\n# Modèle A (excellent)\nfpr_a = np.linspace(0, 1, 100)\ntpr_a = np.power(fpr_a, 0.2)  # Courbe très au-dessus de la diagonale\n\n# Modèle B (bon)\nfpr_b = np.linspace(0, 1, 100)\ntpr_b = np.power(fpr_b, 0.6)\n\n# Modèle C (quasi-aléatoire)\nfpr_c = np.linspace(0, 1, 100)\ntpr_c = fpr_c + np.random.normal(0, 0.05, 100)\ntpr_c = np.clip(tpr_c, 0, 1)\n\n# Visualisation\nplt.figure(figsize=(10, 8))\nplt.plot(fpr_a, tpr_a, linewidth=2, label=f'Modèle A (AUC=0.95)')\nplt.plot(fpr_b, tpr_b, linewidth=2, label=f'Modèle B (AUC=0.75)')\nplt.plot(fpr_c, tpr_c, linewidth=2, label=f'Modèle C (AUC=0.52)')\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Aléatoire (AUC=0.5)')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Comparaison des Courbes ROC')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"1. Classement par performance:\")\nprint(\"   1er: Modèle A (AUC=0.95) - Excellent\")\nprint(\"   2e:  Modèle B (AUC=0.75) - Bon\")\nprint(\"   3e:  Modèle C (AUC=0.52) - Quasi-aléatoire\")\n\nprint(\"\\n2. Pour détecter des fraudes:\")\nprint(\"   → Modèle A (AUC=0.95)\")\nprint(\"   Raison: Meilleure capacité à distinguer fraude/légitime\")\nprint(\"   Permet d'ajuster le seuil selon coût FP vs FN\")\n\nprint(\"\\n3. Utilité du Modèle C:\")\nprint(\"   → Quasiment aucune!\")\nprint(\"   AUC=0.52 $\\approx$ tirage à pile ou face\")\nprint(\"   SAUF: Si on inverse ses prédictions → AUC=0.48\")\nprint(\"   → Peut indiquer un bug dans le code/labels\")\n```\n:::\n\n\n**Réponses:**\n\n1. **Classement**: A > B > C\n\n   - A est excellent\n   - B est bon mais moins performant\n   - C est pratiquement inutile\n\n2. **Fraude bancaire: Modèle A**\n\n   - AUC élevé = meilleure discrimination\n   - Important car coûts élevés (fraudes manquées + fausses alarmes)\n   - Permet de choisir le seuil optimal selon les coûts métier\n\n3. **Utilité de C:**\n\n   - Pratiquement aucune (performance aléatoire)\n   - Pourrait indiquer un problème (bug, labels inversés, features non pertinentes)\n   - Si AUC < 0.5: inverser les prédictions pourrait aider!\n:::\n\n## 4. Choix de Métriques selon le Contexte\n\n### 4.1 Tableau de Décision\n\n| Contexte | Métrique Principale | Raison |\n|----------|-------------------|---------|\n| **Détection de spam** | Precision | Éviter de perdre emails importants (FP coûteux) |\n| **Détection de cancer** | Recall | Ne manquer aucun malade (FN critique) |\n| **Moteur de recherche** | Precision@K | Premiers résultats doivent être pertinents |\n| **Fraude bancaire** | F1 ou AUC | Équilibre entre détecter fraudes et éviter fausses alarmes |\n| **Système de recommandation** | Precision | Recommandations doivent être pertinentes |\n| **Détection d'intrusion réseau** | Recall | Ne rater aucune attaque (FN dangereux) |\n| **Diagnostic automatisé** | Recall + Precision | Les deux sont importants (vies humaines) |\n\n### 4.2 Classes Déséquilibrées\n\nLorsque les classes sont **très déséquilibrées** (ex: 1% positifs, 99% négatifs):\n\n**❌ À ÉVITER:**\n\n- Accuracy (trompeuse)\n\n**✅ À UTILISER:**\n\n- Precision, Recall, F1-Score\n- AUC-ROC\n- Precision-Recall curve\n- Balanced Accuracy = $\\frac{\\text{Recall} + \\text{Specificity}}{2}$\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.metrics import balanced_accuracy_score\n\n# Accuracy normale (biaisée si déséquilibre)\nacc = accuracy_score(y_true, y_pred)\n\n# Balanced accuracy (moyenne de recall et specificity)\nbalanced_acc = balanced_accuracy_score(y_true, y_pred)\n\nprint(f\"Accuracy: {acc:.4f}\")\nprint(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n```\n:::\n\n\n### Exercice 4.1: Choix de Métrique\n\nPour chaque scénario, choisissez la métrique la plus appropriée et justifiez:\n\n1. **Système de reconnaissance faciale pour déverrouillage de téléphone**\n2. **Filtre anti-spam Gmail**\n3. **Détection de COVID-19 par test rapide**\n4. **Système de recommandation Netflix**\n5. **Détection de transactions frauduleuses (0.1% de fraudes)**\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 4.1\n\n**1. Reconnaissance faciale pour téléphone:**\n- **Métrique**: **Precision** (prioritaire) + FPR faible\n- **Justification**:\n\n  - FP = Personne non autorisée accède au téléphone (GRAVE - sécurité)\n  - FN = Propriétaire doit retenter (MINEUR - inconvénient)\n  - Mieux vaut bloquer le propriétaire parfois que laisser entrer un intrus\n\n**2. Filtre anti-spam Gmail:**\n\n- **Métrique**: **Precision** (prioritaire)\n- **Justification**:\n\n  - FP = Email important dans spam (GRAVE - peut manquer info critique)\n  - FN = Spam en boîte de réception (MINEUR - simple nuisance)\n  - Gmail préfère laisser passer du spam que bloquer des emails légitimes\n\n**3. Détection COVID-19:**\n\n- **Métrique**: **Recall** (prioritaire)\n- **Justification**:\n  - FN = Malade non détecté → propage le virus (GRAVE - santé publique)\n  - FP = Personne saine isolée inutilement (MINEUR - test confirmatoire possible)\n  - Mieux vaut trop détecter que pas assez\n\n**4. Recommandation Netflix:**\n\n- **Métrique**: **Precision@K** (K = nombre de recommandations affichées)\n- **Justification**:\n  - Utilisateur voit seulement les K premières recommandations\n  - Celles-ci doivent être pertinentes pour satisfaction client\n  - Recall moins important (on ne peut pas tout recommander)\n\n**5. Fraude bancaire (0.1% de fraudes):**\n\n- **Métriques**: **F1-Score** + **AUC-ROC** + **Precision-Recall curve**\n- **Justification**:\n\n  - Classes très déséquilibrées → Accuracy inutile\n  - FP = Client bloqué à tort (coûteux - insatisfaction)\n  - FN = Fraude non détectée (très coûteux - pertes financières)\n  - Besoin d'équilibre → F1 ou optimiser selon coûts métier\n  - AUC pour comparer modèles indépendamment du seuil\n:::\n\n## 5. Exercice Récapitulatif Complet\n\n### Scénario: Détecteur de Défauts en Usine\n\nUne usine produit 10,000 pièces par jour. En moyenne, 100 pièces (1%) sont défectueuses.\n\nVous avez développé un modèle de détection automatique qui donne les résultats suivants sur 1000 pièces de test:\n\n```\n              Prédiction\n           OK    Défectueux\nRéalité OK    970      20\n     Défect.   3       7\n```\n\n**Coûts:**\n\n- Pièce défectueuse non détectée (FN): **500€** (vendue puis retournée par client)\n- Pièce OK rejetée à tort (FP): **10€** (inspection manuelle inutile)\n- Inspection manuelle d'une pièce: **5€**\n\n**Questions:**\n\n1. Calculez toutes les métriques (Accuracy, Precision, Recall, F1, Specificity)\n2. Le modèle est-il acceptable d'un point de vue métier?\n3. Calculez le coût total journalier des erreurs\n4. Quelle métrique devriez-vous optimiser en priorité?\n5. Proposez une amélioration du modèle\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice Récapitulatif\n\n::: {.cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Matrice de confusion\n#           OK    Défect\n# OK       970      20\n# Défect     3       7\n\ntn, fp = 970, 20\nfn, tp = 3, 7\n\ntotal = tn + fp + fn + tp\n\nprint(\"=\" * 60)\nprint(\"ANALYSE DU DÉTECTEUR DE DÉFAUTS\")\nprint(\"=\" * 60)\n\n# 1. Métriques\nprint(\"\\n1. MÉTRIQUES DE PERFORMANCE:\")\nprint(\"-\" * 60)\n\naccuracy = (tp + tn) / total\nprecision = tp / (tp + fp)\nrecall = tp / (tp + fn)\nf1 = 2 * (precision * recall) / (precision + recall)\nspecificity = tn / (tn + fp)\nfpr = fp / (fp + tn)\nfnr = fn / (fn + tp)\n\nmetrics = {\n    'Accuracy': accuracy,\n    'Precision': precision,\n    'Recall': recall,\n    'F1-Score': f1,\n    'Specificity': specificity,\n    'False Positive Rate': fpr,\n    'False Negative Rate': fnr\n}\n\nfor metric, value in metrics.items():\n    print(f\"{metric:25s}: {value:.4f} ({value*100:6.2f}%)\")\n\n# 2. Acceptabilité métier\nprint(\"\\n2. ACCEPTABILITÉ MÉTIER:\")\nprint(\"-\" * 60)\nprint(f\"X Recall = {recall:.2%} est TRÈS FAIBLE\")\nprint(f\"  → Sur 10 défauts, seulement {tp} détectés, {fn} manqués!\")\nprint(f\"  → {fnr:.2%} de défauts passent inaperçus\")\nprint(f\"\\n$\\checkmark$ Precision = {precision:.2%} est acceptable\")\nprint(f\"  → Peu de fausses alarmes\")\nprint(f\"\\nX CONCLUSION: Modèle INACCEPTABLE\")\nprint(f\"  → Trop de défauts non détectés atteignent les clients\")\n\n# 3. Coût journalier\nprint(\"\\n3. COÛT TOTAL JOURNALIER:\")\nprint(\"-\" * 60)\n\n# Extrapolation à 10,000 pièces\ntotal_pieces = 10000\ndefect_rate = 0.01\ntotal_defects = int(total_pieces * defect_rate)  # 100 défauts\ntotal_ok = total_pieces - total_defects  # 9900 OK\n\n# Calcul des erreurs attendues\nexpected_fn = int(total_defects * fnr)  # Défauts non détectés\nexpected_fp = int(total_ok * fpr)  # OK rejetés à tort\n\ncout_fn = expected_fn * 500  # Défauts non détectés\ncout_fp = expected_fp * 10   # OK rejetés\ncout_inspection = (expected_fn + expected_fp) * 0  # Pas d'inspection pour erreurs\n\ncout_total = cout_fn + cout_fp\n\nprint(f\"Production journalière:     {total_pieces:,} pièces\")\nprint(f\"Défauts réels:              {total_defects} pièces ({defect_rate:.1%})\")\nprint(f\"\\nErreurs attendues:\")\nprint(f\"  - FN (défauts manqués):   {expected_fn} × 500€ = {cout_fn:,}€\")\nprint(f\"  - FP (OK rejetés):        {expected_fp} × 10€  = {cout_fp:,}€\")\nprint(f\"\\nCoût total des erreurs:     {cout_total:,}€/jour\")\nprint(f\"Coût mensuel (22 jours):    {cout_total*22:,}€/mois\")\nprint(f\"Coût annuel (250 jours):    {cout_total*250:,}€/an\")\n\n# 4. Métrique à optimiser\nprint(\"\\n4. MÉTRIQUE À OPTIMISER:\")\nprint(\"-\" * 60)\nprint(\"→ RECALL (priorité absolue)\")\nprint(\"\\nRaison: Le coût d'un FN (500€) est 50× plus élevé\")\nprint(\"        que le coût d'un FP (10€)\")\nprint(f\"\\nRatio coût FN/FP: {500/10:.0f}:1\")\nprint(\"\\nStratégie: Maximiser le Recall, même au prix de\")\nprint(\"           la Precision (plus de fausses alarmes acceptable)\")\n\n# 5. Amélioration\nprint(\"\\n5. PROPOSITIONS D'AMÉLIORATION:\")\nprint(\"-\" * 60)\nprint(\"\\nA. Court terme (ajustement du seuil):\")\nprint(\"   1. Abaisser le seuil de décision (ex: 0.5 → 0.2)\")\nprint(\"   2. Objectif: Recall > 90%\")\nprint(\"   3. Conséquence: Plus de FP (mais coût acceptable)\")\n\n# Simulation avec recall amélioré\nrecall_target = 0.90\nfn_new = int(total_defects * (1 - recall_target))  # 10 défauts manqués\n# Supposons que Precision baisse à 20%\nprecision_new = 0.20\nfp_new = int((total_defects * recall_target) / precision_new * (1 - precision_new))\n\ncout_fn_new = fn_new * 500\ncout_fp_new = fp_new * 10\ncout_total_new = cout_fn_new + cout_fp_new\n\nprint(f\"\\n   Simulation avec Recall=90%, Precision=20%:\")\nprint(f\"   - FN: {fn_new} × 500€ = {cout_fn_new:,}€\")\nprint(f\"   - FP: {fp_new} × 10€ = {cout_fp_new:,}€\")\nprint(f\"   - Coût total: {cout_total_new:,}€/jour\")\nprint(f\"   - Économie: {cout_total - cout_total_new:,}€/jour\")\nprint(f\"   - ROI annuel: {(cout_total - cout_total_new)*250:,}€\")\n\nprint(\"\\nB. Moyen terme (amélioration du modèle):\")\nprint(\"   1. Collecter plus de données (surtout défauts)\")\nprint(\"   2. Feature engineering (nouvelles caractéristiques)\")\nprint(\"   3. Essayer d'autres algorithmes (XGBoost, CNN)\")\nprint(\"   4. Data augmentation pour la classe minoritaire\")\n\nprint(\"\\nC. Long terme (approche hybride):\")\nprint(\"   1. Modèle ML comme premier filtre (Recall élevé)\")\nprint(\"   2. Inspection manuelle des cas suspects\")\nprint(\"   3. Double vérification pour cas limites\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Matrice de confusion\ncm = np.array([[tn, fp], [fn, tp]])\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n            xticklabels=['OK', 'Défectueux'],\n            yticklabels=['OK', 'Défectueux'])\naxes[0].set_ylabel('Vraie Classe')\naxes[0].set_xlabel('Prédiction')\naxes[0].set_title('Matrice de Confusion')\n\n# Comparaison coûts\nscenarios = ['Modèle Actuel\\n(Recall=70%)', \n             'Modèle Amélioré\\n(Recall=90%)']\ncouts = [cout_total, cout_total_new]\ncolors = ['#ff6b6b', '#51cf66']\n\naxes[1].bar(scenarios, couts, color=colors, alpha=0.7)\naxes[1].set_ylabel('Coût journalier (€)')\naxes[1].set_title('Comparaison des Coûts')\naxes[1].grid(axis='y', alpha=0.3)\n\nfor i, (scenario, cout) in enumerate(zip(scenarios, couts)):\n    axes[1].text(i, cout + 100, f'{cout:,}€', \n                ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Tableau récapitulatif\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TABLEAU RÉCAPITULATIF\")\nprint(\"=\" * 60)\n\ncomparison = pd.DataFrame({\n    'Métrique': ['Recall', 'Precision', 'FN/jour', 'FP/jour', 'Coût/jour'],\n    'Actuel': [f'{recall:.1%}', f'{precision:.1%}', expected_fn, expected_fp, f'{cout_total:,}€'],\n    'Cible': ['90%', '20%', fn_new, fp_new, f'{cout_total_new:,}€'],\n    'Amélioration': ['↑ +20pp', '↓ -6pp', f'↓ -{expected_fn-fn_new}', f'↑ +{fp_new-expected_fp}', \n                     f'↓ -{cout_total-cout_total_new:,}€']\n})\n\nprint(comparison.to_string(index=False))\nprint(\"=\" * 60)\n```\n:::\n\n\n**Synthèse des réponses:**\n\n**1. Métriques:**\n\n- Accuracy: 97.70%\n- Precision: 25.93% (7 sur 27 prédictions de défauts sont correctes)\n- Recall: 70.00% (7 sur 10 vrais défauts détectés)\n- F1-Score: 38.89%\n- Specificity: 97.98%\n\n**2. Acceptabilité métier:**\n\n- ❌ **NON ACCEPTABLE**\n- Recall de 70% signifie 30% de défauts non détectés\n- Ces défauts atteignent les clients → réputation + coûts\n\n**3. Coût journalier:**\n\n- FN: 30 défauts × 500€ = **15,000€/jour**\n- FP: 200 pièces × 10€ = **2,000€/jour**\n- **Total: 17,000€/jour** ($\\approx$ 4.25M€/an!)\n\n**4. Métrique à optimiser:**\n\n- **RECALL** (priorité absolue)\n- Ratio coût 50:1 en faveur de l'augmentation du Recall\n- Accepter plus de FP pour réduire FN\n\n**5. Amélioration proposée:**\n\n- Abaisser seuil → Recall 90%+\n- Économie potentielle: ~10,000€/jour\n- ROI annuel: ~2.5M€\n:::\n\n## 6. Comparaison Critique des Résultats\n\n### Exercice 6.1: Analyse Comparative\n\nTrois data scientists ont entraîné des modèles pour détecter des défaillances machines:\n\n| Modèle | Accuracy | Precision | Recall | F1 | AUC |\n|--------|----------|-----------|--------|----|----|\n| A | 98% | 40% | 95% | 56% | 0.92 |\n| B | 95% | 80% | 70% | 75% | 0.88 |\n| C | 99% | 20% | 60% | 30% | 0.75 |\n\n**Contexte:** 2% de défaillances, coût FN = 10,000€, coût FP = 100€\n\n**Questions:**\n1. Quel modèle recommandez-vous?\n2. Justifiez votre choix avec calculs de coûts\n3. Que révèle l'Accuracy élevée du modèle C?\n\n::: {.callout-note collapse=\"true\"}\n## Solution Exercice 6.1\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Données\nmodels_data = {\n    'Modèle': ['A', 'B', 'C'],\n    'Accuracy': [0.98, 0.95, 0.99],\n    'Precision': [0.40, 0.80, 0.20],\n    'Recall': [0.95, 0.70, 0.60],\n    'F1': [0.56, 0.75, 0.30],\n    'AUC': [0.92, 0.88, 0.75]\n}\n\ndf_models = pd.DataFrame(models_data)\n\n# Contexte\ndefect_rate = 0.02\ntotal_samples = 10000\ntotal_defects = int(total_samples * defect_rate)  # 200\ntotal_normal = total_samples - total_defects  # 9800\n\ncost_fn = 10000  # Coût défaillance non détectée\ncost_fp = 100    # Coût fausse alarme\n\nprint(\"ANALYSE COMPARATIVE DES MODÈLES\")\nprint(\"=\" * 70)\n\n# Calcul des coûts pour chaque modèle\nresults = []\n\nfor idx, row in df_models.iterrows():\n    model = row['Modèle']\n    recall = row['Recall']\n    precision = row['Precision']\n    \n    # Calcul des erreurs\n    fn = int(total_defects * (1 - recall))  # Défauts manqués\n    tp = total_defects - fn  # Défauts détectés\n    \n    # De precision = TP / (TP + FP), on déduit FP\n    if precision > 0:\n        fp = int(tp / precision - tp)  # Fausses alarmes\n    else:\n        fp = 0\n    \n    # Coûts\n    cost_fn_total = fn * cost_fn\n    cost_fp_total = fp * cost_fp\n    cost_total = cost_fn_total + cost_fp_total\n    \n    results.append({\n        'Modèle': model,\n        'FN': fn,\n        'FP': fp,\n        'Coût FN': cost_fn_total,\n        'Coût FP': cost_fp_total,\n        'Coût Total': cost_total\n    })\n    \n    print(f\"\\nModèle {model}:\")\n    print(f\"  Recall: {recall:.0%} → FN = {fn} défaillances manquées\")\n    print(f\"  Precision: {precision:.0%} → FP = {fp} fausses alarmes\")\n    print(f\"  Coût FN: {fn} × {cost_fn:,}€ = {cost_fn_total:,}€\")\n    print(f\"  Coût FP: {fp} × {cost_fp:,}€ = {cost_fp_total:,}€\")\n    print(f\"  COÛT TOTAL: {cost_total:,}€\")\n\ndf_results = pd.DataFrame(results)\n\n# Meilleur modèle\nbest_model = df_results.loc[df_results['Coût Total'].idxmin()]\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RECOMMANDATION\")\nprint(\"=\" * 70)\nprint(f\"→ Modèle {best_model['Modèle']} (coût le plus faible)\")\nprint(f\"\\nJustification:\")\nprint(f\"  - Coût total minimal: {best_model['Coût Total']:,}€\")\nprint(f\"  - Recall élevé ({df_models[df_models['Modèle']==best_model['Modèle']]['Recall'].values[0]:.0%}) critique car FN très coûteux\")\nprint(f\"  - Le surcoût en FP est négligeable comparé aux économies en FN\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANALYSE DU MODÈLE C\")\nprint(\"=\" * 70)\nprint(\"Accuracy de 99% mais performances médiocres:\")\nprint(\"  → Piège des classes déséquilibrées!\")\nprint(f\"  → Avec 2% de défauts, prédire 'normal' partout\")\nprint(f\"     donnerait déjà 98% d'accuracy\")\nprint(\"  → Recall de 60% = 40% de défauts manqués (inacceptable)\")\nprint(\"  → AUC faible (0.75) confirme faible capacité discriminative\")\nprint(\"\\n  Conclusion: Accuracy est une métrique TROMPEUSE ici!\")\n\n# Visualisations\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Graphique 1: Comparaison des coûts\nx = df_results['Modèle']\nwidth = 0.35\n\nx_pos = range(len(x))\naxes[0].bar([p - width/2 for p in x_pos], df_results['Coût FN'], \n           width, label='Coût FN', color='#ff6b6b', alpha=0.8)\naxes[0].bar([p + width/2 for p in x_pos], df_results['Coût FP'], \n           width, label='Coût FP', color='#ffa94d', alpha=0.8)\n\naxes[0].set_xlabel('Modèle')\naxes[0].set_ylabel('Coût (€)')\naxes[0].set_title('Comparaison des Coûts par Type d\\'Erreur')\naxes[0].set_xticks(x_pos)\naxes[0].set_xticklabels(x)\naxes[0].legend()\naxes[0].grid(axis='y', alpha=0.3)\n\n# Ajouter coût total\nfor i, (idx, row) in enumerate(df_results.iterrows()):\n    total = row['Coût Total']\n    axes[0].text(i, total + 5000, f'{total:,}€', \n                ha='center', fontweight='bold', fontsize=10)\n\n# Graphique 2: Radar chart des métriques\nfrom math import pi\n\ncategories = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\nN = len(categories)\n\nangles = [n / float(N) * 2 * pi for n in range(N)]\nangles += angles[:1]\n\nax = plt.subplot(122, projection='polar')\n\nfor idx, row in df_models.iterrows():\n    values = [row['Accuracy'], row['Precision'], row['Recall'], \n              row['F1'], row['AUC']]\n    values += values[:1]\n    \n    ax.plot(angles, values, 'o-', linewidth=2, label=f\"Modèle {row['Modèle']}\")\n    ax.fill(angles, values, alpha=0.15)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(categories)\nax.set_ylim(0, 1)\nax.set_title('Comparaison Multidimensionnelle des Métriques', y=1.08)\nax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\nax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Tableau final\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TABLEAU RÉCAPITULATIF\")\nprint(\"=\" * 70)\nprint(df_results.to_string(index=False))\n```\n:::\n\n\n**Réponses:**\n\n1. **Modèle recommandé: A**\n\n2. **Justification avec coûts:**\n   - Modèle A: 10 FN × 10,000€ + 475 FP × 100€ = **147,500€**\n   - Modèle B: 60 FN × 10,000€ + 175 FP × 100€ = **617,500€**\n   - Modèle C: 80 FN × 10,000€ + 600 FP × 100€ = **860,000€**\n   \n   Le Modèle A minimise le coût total malgré plus de FP\n\n3. **Accuracy élevée du Modèle C:**\n   - Piège classique des classes déséquilibrées!\n   - 99% accuracy car prédit surtout \"normal\"\n   - Mais rate 40% des défaillances (Recall = 60%)\n   - **Leçon**: Accuracy seule est trompeuse avec déséquilibre\n:::\n\n## Résumé de la Séance\n\n::: {.callout-important icon=false}\n## Points clés à retenir\n\n### 1. Matrice de Confusion\n- Fondation de toutes les métriques\n- TP, TN, FP, FN à bien comprendre\n- Visualisation claire des erreurs\n\n### 2. Métriques Principales\n- **Accuracy**: % correct (attention au déséquilibre!)\n- **Precision**: Fiabilité des prédictions positives\n- **Recall**: Couverture des vrais positifs\n- **F1-Score**: Équilibre Precision-Recall\n\n### 3. Compromis\n- Precision ↑ → Recall ↓ (généralement)\n- Choix selon le coût des erreurs\n- Ajustement du seuil de décision\n\n### 4. Courbe ROC et AUC\n- Évaluation indépendante du seuil\n- AUC = mesure globale de discrimination\n- Comparaison facile de modèles\n\n### 5. Choix Contextuel\n- **Pas de métrique universelle!**\n- Dépend du problème métier\n- Considérer les coûts réels des erreurs\n- Classes déséquilibrées → éviter Accuracy seule\n\n### 6. Règles d'Or\n1. Toujours regarder plusieurs métriques\n2. Privilégier Recall si FN coûteux\n3. Privilégier Precision si FP coûteux\n4. Utiliser AUC pour comparer modèles\n5. Valider avec coûts métier réels\n:::\n\n## Checklist de Maîtrise\n\n- [ ] Je sais construire et interpréter une matrice de confusion\n- [ ] Je comprends la différence entre Precision et Recall\n- [ ] Je peux expliquer pourquoi Accuracy peut être trompeuse\n- [ ] Je sais calculer manuellement les métriques de base\n- [ ] Je comprends le compromis Precision-Recall\n- [ ] Je sais interpréter une courbe ROC et l'AUC\n- [ ] Je peux choisir la métrique appropriée selon le contexte\n- [ ] Je comprends l'impact des classes déséquilibrées\n\n## Exercices Supplémentaires\n\n::: {.callout-warning icon=false}\n## Pour s'entraîner\n\n1. **Implémentez** toutes les métriques vues en Python sans Scikit-learn\n2. **Créez** une fonction qui recommande la meilleure métrique selon le contexte\n3. **Analysez** un dataset déséquilibré (ex: fraude, maladie rare) sur Kaggle\n4. **Comparez** plusieurs modèles sur le même problème avec toutes les métriques\n5. **Explorez** l'effet du seuil de décision sur Precision et Recall\n:::\n\n## Préparation Séance Suivante\n\nLa **Séance 6 (TP2)** abordera:\n\n- Classification multi-classes\n- Optimisation d'hyperparamètres (GridSearchCV, RandomizedSearchCV)\n- Validation croisée\n- Comparaison avancée de modèles\n\n**À préparer:**\n\n- Relire les concepts de validation croisée\n- Installer scikit-learn à jour\n- Réfléchir aux hyperparamètres importants de chaque algorithme\n\n## Ressources Complémentaires\n\n1. [Scikit-learn: Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n2. [Google ML Crash Course: Classification](https://developers.google.com/machine-learning/crash-course/classification)\n3. [StatQuest: Sensitivity and Specificity](https://www.youtube.com/watch?v=vP06aMoz4v8)\n4. [Towards Data Science: Beyond Accuracy](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c)\n\n",
    "supporting": [
      "seance5_files\\figure-pdf"
    ],
    "filters": []
  }
}