{
  "hash": "fb308eed29c0a669ecc9216e5c756498",
  "result": {
    "engine": "jupyter",
    "markdown": "# S√©ance 3: TP1 - Pipeline de Classification Binaire\n\n::: {.callout-note icon=false}\n## Informations de la s√©ance\n- **Type**: Travaux Pratiques\n- **Dur√©e**: 2h\n- **Objectifs**: Obj6, Obj7\n- **Dataset**: Titanic (pr√©diction de survie)\n:::\n\n## Objectifs du TP\n\n√Ä la fin de ce TP, vous serez capable de:\n\n1. Charger et explorer un dataset\n2. Pr√©parer les donn√©es pour l'apprentissage\n3. Cr√©er un pipeline de pr√©traitement avec Scikit-learn\n4. Entra√Æner un mod√®le de classification binaire\n5. √âvaluer les performances du mod√®le\n\nüìé **Lien du TP :** [TP1 ‚Äî Pipeline Classification](https://nevermind78.github.io/AN_slides/TP1_Pipeline_Classification_pyodide.html#/title-slide)\n\n## 1. Configuration de l'Environnement\n\n::: {#881fec51 .cell execution_count=1}\n``` {.python .cell-code}\n# Installation des biblioth√®ques (si n√©cessaire)\n# !pip install scikit-learn pandas numpy matplotlib seaborn\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\n# Configuration\nplt.style.use('default')\nsns.set_palette(\"husl\")\nnp.random.seed(42)\n\nprint(\"‚úì Biblioth√®ques import√©es avec succ√®s\")\n```\n:::\n\n\n## 2. Chargement et Exploration des Donn√©es\n\n### 2.1 Chargement du Dataset Titanic\n\n::: {#c4a20b1e .cell execution_count=2}\n``` {.python .cell-code}\n# Chargement depuis seaborn\ntitanic = sns.load_dataset('titanic')\n\n# Affichage des premi√®res lignes\nprint(\"Aper√ßu des donn√©es:\")\nprint(titanic.head())\n\nprint(f\"\\nDimensions: {titanic.shape}\")\nprint(f\"Colonnes: {titanic.columns.tolist()}\")\n```\n:::\n\n\n### 2.2 Exploration Initiale\n\n::: {#ed89a909 .cell execution_count=3}\n``` {.python .cell-code}\n# Informations g√©n√©rales\nprint(\"Informations sur le dataset:\")\nprint(titanic.info())\n\nprint(\"\\nStatistiques descriptives:\")\nprint(titanic.describe())\n\n# V√©rification des valeurs manquantes\nprint(\"\\nValeurs manquantes:\")\nprint(titanic.isnull().sum())\n\n# Distribution de la variable cible\nprint(\"\\nDistribution de la survie:\")\nprint(titanic['survived'].value_counts())\nprint(f\"\\nTaux de survie: {titanic['survived'].mean():.2%}\")\n```\n:::\n\n\n### 2.3 Visualisations Exploratoires\n\n::: {#edb5752a .cell execution_count=4}\n``` {.python .cell-code}\n# Figure 1: Distribution de la survie\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Survie globale\naxes[0, 0].pie(\n    titanic['survived'].value_counts(), \n    labels=['D√©c√©d√©', 'Survivant'],\n    autopct='%1.1f%%',\n    startangle=90,\n    colors=['#ff6b6b', '#51cf66']\n)\naxes[0, 0].set_title('Distribution de la Survie')\n\n# Survie par sexe\nsurvival_by_sex = titanic.groupby(['sex', 'survived']).size().unstack()\nsurvival_by_sex.plot(kind='bar', ax=axes[0, 1], color=['#ff6b6b', '#51cf66'])\naxes[0, 1].set_title('Survie par Sexe')\naxes[0, 1].set_xlabel('Sexe')\naxes[0, 1].set_ylabel('Nombre de passagers')\naxes[0, 1].legend(['D√©c√©d√©', 'Survivant'])\naxes[0, 1].tick_params(axis='x', rotation=0)\n\n# Survie par classe\nsurvival_by_class = titanic.groupby(['pclass', 'survived']).size().unstack()\nsurvival_by_class.plot(kind='bar', ax=axes[1, 0], color=['#ff6b6b', '#51cf66'])\naxes[1, 0].set_title('Survie par Classe')\naxes[1, 0].set_xlabel('Classe')\naxes[1, 0].set_ylabel('Nombre de passagers')\naxes[1, 0].legend(['D√©c√©d√©', 'Survivant'])\n\n# Distribution de l'√¢ge\naxes[1, 1].hist(titanic[titanic['survived']==0]['age'].dropna(), \n                alpha=0.5, label='D√©c√©d√©', bins=30, color='#ff6b6b')\naxes[1, 1].hist(titanic[titanic['survived']==1]['age'].dropna(), \n                alpha=0.5, label='Survivant', bins=30, color='#51cf66')\naxes[1, 1].set_title('Distribution de l\\'√¢ge par survie')\naxes[1, 1].set_xlabel('√Çge')\naxes[1, 1].set_ylabel('Fr√©quence')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## 3. Pr√©paration des Donn√©es\n\n### 3.1 S√©lection des Features\n\n::: {#d0a1bd4e .cell execution_count=5}\n``` {.python .cell-code}\n# S√©lection des colonnes pertinentes\nfeatures = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\ntarget = 'survived'\n\n# Cr√©ation du dataset de travail\ndf = titanic[features + [target]].copy()\n\nprint(f\"Dataset de travail: {df.shape}\")\nprint(f\"\\nValeurs manquantes:\")\nprint(df.isnull().sum())\n```\n:::\n\n\n### 3.2 Traitement des Valeurs Manquantes\n\n::: {#8147a01c .cell execution_count=6}\n``` {.python .cell-code}\n# Strat√©gies de traitement\n# 1. Age: remplir avec la m√©diane\ndf['age'].fillna(df['age'].median(), inplace=True)\n\n# 2. Embarked: remplir avec le mode (valeur la plus fr√©quente)\ndf['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n\n# 3. Fare: remplir avec la m√©diane (si manquant)\ndf['fare'].fillna(df['fare'].median(), inplace=True)\n\n# V√©rification\nprint(\"Apr√®s traitement:\")\nprint(df.isnull().sum())\n```\n:::\n\n\n### 3.3 Encodage des Variables Cat√©gorielles\n\n::: {#91d338c2 .cell execution_count=7}\n``` {.python .cell-code}\n# Encodage de 'sex'\ndf['sex'] = df['sex'].map({'male': 0, 'female': 1})\n\n# Encodage de 'embarked' (One-Hot Encoding)\ndf = pd.get_dummies(df, columns=['embarked'], prefix='embarked', drop_first=True)\n\nprint(\"Dataset apr√®s encodage:\")\nprint(df.head())\nprint(f\"\\nNouvelles dimensions: {df.shape}\")\n```\n:::\n\n\n### 3.4 S√©paration Features / Target\n\n::: {#8bd7eee7 .cell execution_count=8}\n``` {.python .cell-code}\n# S√©paration X (features) et y (target)\nX = df.drop('survived', axis=1)\ny = df['survived']\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\nprint(f\"\\nFeatures utilis√©es:\\n{X.columns.tolist()}\")\n```\n:::\n\n\n## 4. Split Train/Validation/Test\n\n### 4.1 Split Train/Test\n\n::: {#53f832c2 .cell execution_count=9}\n``` {.python .cell-code}\n# Split 80/20\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2,      # 20% pour le test\n    random_state=42,    # reproductibilit√©\n    stratify=y          # pr√©server la distribution des classes\n)\n\nprint(f\"Train set: {X_train.shape}\")\nprint(f\"Test set:  {X_test.shape}\")\n\n# V√©rification de la distribution\nprint(f\"\\nDistribution train: {y_train.value_counts(normalize=True)}\")\nprint(f\"Distribution test:  {y_test.value_counts(normalize=True)}\")\n```\n:::\n\n\n### 4.2 Split Train/Validation (optionnel)\n\n::: {#da1eaa28 .cell execution_count=10}\n``` {.python .cell-code}\n# Optionnel: cr√©er un ensemble de validation\nX_train_full, X_val, y_train_full, y_val = train_test_split(\n    X_train, y_train,\n    test_size=0.2,  # 20% du train pour validation\n    random_state=42,\n    stratify=y_train\n)\n\nprint(f\"Train full: {X_train_full.shape}\")\nprint(f\"Validation: {X_val.shape}\")\nprint(f\"Test:       {X_test.shape}\")\n```\n:::\n\n\n## 5. Pipeline de Pr√©traitement et Entra√Ænement\n\n### 5.1 Cr√©ation du Pipeline\n\n::: {#51d0d858 .cell execution_count=11}\n``` {.python .cell-code}\n# Pipeline: Standardisation + Mod√®le\npipeline = Pipeline([\n    ('scaler', StandardScaler()),  # √âtape 1: Standardisation\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))  # √âtape 2: Mod√®le\n])\n\nprint(\"Pipeline cr√©√©:\")\nprint(pipeline)\n```\n:::\n\n\n### 5.2 Entra√Ænement du Mod√®le\n\n::: {#2cb4abb9 .cell execution_count=12}\n``` {.python .cell-code}\n# Entra√Ænement\nprint(\"Entra√Ænement en cours...\")\npipeline.fit(X_train, y_train)\nprint(\"‚úì Entra√Ænement termin√©\")\n\n# Pr√©dictions\ny_train_pred = pipeline.predict(X_train)\ny_test_pred = pipeline.predict(X_test)\n\nprint(\"‚úì Pr√©dictions effectu√©es\")\n```\n:::\n\n\n## 6. √âvaluation Initiale\n\n### 6.1 Accuracy\n\n::: {#16fb7c7a .cell execution_count=13}\n``` {.python .cell-code}\n# Calcul de l'accuracy\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\n\nprint(f\"Accuracy Train: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\nprint(f\"Accuracy Test:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n\n# Analyse de l'overfitting\ndiff = train_accuracy - test_accuracy\nprint(f\"\\nDiff√©rence Train-Test: {diff:.4f}\")\nif diff < 0.05:\n    print(\"‚Üí Bon √©quilibre biais-variance\")\nelif diff < 0.10:\n    print(\"‚Üí L√©ger overfitting\")\nelse:\n    print(\"‚Üí Overfitting significatif\")\n```\n:::\n\n\n### 6.2 Matrice de Confusion\n\n::: {#506797ce .cell execution_count=14}\n``` {.python .cell-code}\n# Calcul de la matrice de confusion\ncm = confusion_matrix(y_test, y_test_pred)\n\n# Visualisation\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['D√©c√©d√©', 'Survivant'],\n            yticklabels=['D√©c√©d√©', 'Survivant'])\nplt.title('Matrice de Confusion - Test Set')\nplt.ylabel('Vraie Classe')\nplt.xlabel('Classe Pr√©dite')\nplt.tight_layout()\nplt.show()\n\n# Interpr√©tation\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\nVrais N√©gatifs (TN):  {tn}\")\nprint(f\"Faux Positifs (FP):   {fp}\")\nprint(f\"Faux N√©gatifs (FN):   {fn}\")\nprint(f\"Vrais Positifs (TP):  {tp}\")\n```\n:::\n\n\n### 6.3 Rapport de Classification\n\n::: {#8ac9cb57 .cell execution_count=15}\n``` {.python .cell-code}\n# Rapport d√©taill√©\nprint(\"\\nRapport de Classification:\")\nprint(classification_report(y_test, y_test_pred, \n                          target_names=['D√©c√©d√©', 'Survivant']))\n```\n:::\n\n\n## 7. Comparaison de Plusieurs Mod√®les\n\n::: {#dad01eca .cell execution_count=16}\n``` {.python .cell-code}\n# D√©finition des mod√®les\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n}\n\n# Entra√Ænement et √©valuation\nresults = {}\nfor name, model in models.items():\n    # Pipeline pour chaque mod√®le\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', model)\n    ])\n    \n    # Entra√Ænement\n    pipe.fit(X_train, y_train)\n    \n    # √âvaluation\n    train_score = pipe.score(X_train, y_train)\n    test_score = pipe.score(X_test, y_test)\n    \n    results[name] = {\n        'train': train_score,\n        'test': test_score,\n        'diff': train_score - test_score\n    }\n    \n    print(f\"\\n{name}:\")\n    print(f\"  Train Accuracy: {train_score:.4f}\")\n    print(f\"  Test Accuracy:  {test_score:.4f}\")\n    print(f\"  Diff√©rence:     {train_score - test_score:.4f}\")\n\n# Visualisation comparative\ndf_results = pd.DataFrame(results).T\ndf_results[['train', 'test']].plot(kind='bar', figsize=(10, 6))\nplt.title('Comparaison des Performances des Mod√®les')\nplt.xlabel('Mod√®le')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'])\nplt.xticks(rotation=45, ha='right')\nplt.ylim([0, 1])\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## 8. Analyse des Pr√©dictions\n\n### 8.1 Exemples de Pr√©dictions\n\n::: {#1d7e5755 .cell execution_count=17}\n``` {.python .cell-code}\n# Pr√©dictions avec probabilit√©s\ny_proba = pipeline.predict_proba(X_test)\n\n# Affichage de quelques exemples\nn_samples = 5\nindices = np.random.choice(len(X_test), n_samples, replace=False)\n\nprint(\"Exemples de pr√©dictions:\\n\")\nfor idx in indices:\n    actual = y_test.iloc[idx]\n    predicted = y_test_pred[idx]\n    proba = y_proba[idx]\n    \n    print(f\"Passager {idx}:\")\n    print(f\"  Vraie classe:     {'Survivant' if actual == 1 else 'D√©c√©d√©'}\")\n    print(f\"  Pr√©diction:       {'Survivant' if predicted == 1 else 'D√©c√©d√©'}\")\n    print(f\"  Probabilit√©s:     D√©c√©d√©={proba[0]:.2%}, Survivant={proba[1]:.2%}\")\n    print(f\"  Correct:          {'+' if actual == predicted else '+'}\")\n    print()\n```\n:::\n\n\n### 8.2 Analyse des Erreurs\n\n::: {#1a22964c .cell execution_count=18}\n``` {.python .cell-code}\n# Identification des erreurs\nerrors = X_test[y_test != y_test_pred].copy()\nerrors['actual'] = y_test[y_test != y_test_pred]\nerrors['predicted'] = y_test_pred[y_test != y_test_pred]\n\nprint(f\"Nombre d'erreurs: {len(errors)}\")\nprint(f\"Taux d'erreur: {len(errors)/len(X_test):.2%}\")\n\nprint(\"\\nQuelques erreurs:\")\nprint(errors.head())\n\n# Analyse des caract√©ristiques des erreurs\nprint(\"\\nCaract√©ristiques moyennes des erreurs vs correctes:\")\ncorrect = X_test[y_test == y_test_pred]\n\ncomparison = pd.DataFrame({\n    'Erreurs': errors.drop(['actual', 'predicted'], axis=1).mean(),\n    'Correctes': correct.mean()\n})\nprint(comparison)\n```\n:::\n\n\n## Exercices Pratiques\n\n::: {.callout-warning icon=false collapse=\"true\"}\n## Exercice 1: Feature Engineering\n\nCr√©ez une nouvelle feature `family_size` = `sibsp` + `parch` + 1, puis r√©-entra√Ænez le mod√®le. La performance s'am√©liore-t-elle ?\n\n### Solution\n\n::: {#275d3272 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\n# Cr√©ation de la nouvelle feature\ndf['family_size'] = df['sibsp'] + df['parch'] + 1\n\n# Refaire le split et l'entra√Ænement\nX_new = df.drop('survived', axis=1)\ny_new = df['survived']\n\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n)\n\npipeline_new = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\n\npipeline_new.fit(X_train_new, y_train_new)\nnew_score = pipeline_new.score(X_test_new, y_test_new)\n\nprint(f\"Accuracy avec family_size: {new_score:.4f}\")\nprint(f\"Accuracy sans family_size: {test_accuracy:.4f}\")\nprint(f\"Am√©lioration: {new_score - test_accuracy:.4f}\")\n```\n:::\n\n\n:::\n\n::: {.callout-warning icon=false collapse=\"true\"}\n## Exercice 2: Optimisation des Hyperparam√®tres\n\nTestez diff√©rentes valeurs de `max_depth` pour le Decision Tree (3, 5, 7, 10, None). Quelle valeur donne les meilleures performances sur le test set ?\n\n### Solution\n\n::: {#a445a87a .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\ndepths = [3, 5, 7, 10, None]\nresults_depth = []\n\nfor depth in depths:\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', DecisionTreeClassifier(max_depth=depth, random_state=42))\n    ])\n    \n    pipe.fit(X_train, y_train)\n    train_score = pipe.score(X_train, y_train)\n    test_score = pipe.score(X_test, y_test)\n    \n    results_depth.append({\n        'max_depth': depth,\n        'train': train_score,\n        'test': test_score,\n        'diff': train_score - test_score\n    })\n    \ndf_depth = pd.DataFrame(results_depth)\nprint(df_depth)\n\n# Meilleure valeur\nbest_depth = df_depth.loc[df_depth['test'].idxmax(), 'max_depth']\nprint(f\"\\nMeilleur max_depth: {best_depth}\")\n```\n:::\n\n\n:::\n\n::: {.callout-warning icon=false collapse=\"true\"}\n## Exercice 3: Analyse d'Importance\n\nPour le Random Forest, affichez l'importance des features. Quelles sont les 3 features les plus importantes ?\n\n### Solution\n\n::: {#a7827f1b .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\n# Entra√Æner Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Importance des features\nimportances = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': rf.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Importance des features:\")\nprint(importances)\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nplt.barh(importances['feature'], importances['importance'])\nplt.xlabel('Importance')\nplt.title('Importance des Features - Random Forest')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTop 3 features:\")\nprint(importances.head(3))\n```\n:::\n\n\n:::\n\n## R√©sum√© du TP\n\n::: {.callout-important icon=false}\n## Ce que vous avez appris\n\n1. **Chargement et exploration** de donn√©es avec pandas\n2. **Pr√©traitement** des donn√©es:\n   - Traitement des valeurs manquantes\n   - Encodage des variables cat√©gorielles\n   - Standardisation\n3. **Pipeline Scikit-learn** pour automatiser le workflow\n4. **Split Train/Test** avec stratification\n5. **Entra√Ænement et √©valuation** de mod√®les de classification\n6. **Comparaison** de plusieurs algorithmes\n7. **Analyse des r√©sultats** et des erreurs\n:::\n\n## Checklist de Validation\n\n- [ ] Dataset charg√© et explor√©\n- [ ] Valeurs manquantes trait√©es\n- [ ] Variables cat√©gorielles encod√©es\n- [ ] Pipeline cr√©√© avec StandardScaler\n- [ ] Mod√®le entra√Æn√© avec succ√®s\n- [ ] Accuracy calcul√©e (train et test)\n- [ ] Matrice de confusion g√©n√©r√©e\n- [ ] Comparaison de plusieurs mod√®les effectu√©e\n- [ ] Analyse des erreurs r√©alis√©e\n\n## Pour Aller Plus Loin\n\n1. Testez d'autres features (titre extrait du nom, cabine, etc.)\n2. Exp√©rimentez avec le seuil de d√©cision (au lieu de 0.5)\n3. Utilisez la validation crois√©e (voir TP2)\n4. Essayez d'autres algorithmes (SVM, Gradient Boosting)\n\n",
    "supporting": [
      "seance3_files"
    ],
    "filters": [],
    "includes": {}
  }
}