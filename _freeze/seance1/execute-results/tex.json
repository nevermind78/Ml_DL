{
  "hash": "e15bec19cff4995f634f8f3f7d66af9a",
  "result": {
    "engine": "jupyter",
    "markdown": "# Séance 1: Introduction IA et Machine Learning\n\n::: {.callout-note icon=false}\n## Informations de la séance\n- **Type**: Cours\n- **Durée**: 2h\n- **Objectifs**: Obj1, Obj2, Obj3\n:::\n\n## 1. Définitions et Concepts de Base\n\n### 1.1 Intelligence Artificielle (IA)\n\nL'**Intelligence Artificielle** est un domaine de l'informatique qui vise à créer des systèmes capables d'effectuer des tâches nécessitant normalement l'intelligence humaine.\n\n::: {.callout-tip}\n## Exemples d'IA au quotidien\n- Assistants vocaux (Siri, Alexa, Google Assistant)\n- Recommandations Netflix/Spotify\n- Filtres anti-spam des emails\n- Reconnaissance faciale sur smartphones\n- Traduction automatique\n:::\n\n### 1.2 Machine Learning (Apprentissage Automatique)\n\nLe **Machine Learning** est une sous-discipline de l'IA qui permet aux ordinateurs d'apprendre à partir de données sans être explicitement programmés.\n\n**Différence clé**:\n- **Programmation traditionnelle**: Humain écrit les règles → Ordinateur applique\n- **Machine Learning**: Ordinateur apprend les règles à partir des données\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Approche traditionnelle\ndef classifier_email(email):\n    if \"viagra\" in email or \"lottery\" in email:\n        return \"spam\"\n    else:\n        return \"not spam\"\n\n# Approche Machine Learning\nmodel = train_model(emails_examples)\nprediction = model.predict(new_email)\n```\n:::\n\n\n### 1.3 Deep Learning\n\nLe **Deep Learning** est une sous-catégorie du ML utilisant des réseaux de neurones artificiels profonds (plusieurs couches).\n\n\n**Diagramme mermaid:**\n```{mermaid}\ngraph TD\n    A[Intelligence Artificielle] --> B[Machine Learning]\n    B --> C[Deep Learning]\n    A --> D[Systèmes experts]\n    A --> E[Robotique]\n    B --> F[Apprentissage supervisé]\n    B --> G[Apprentissage non supervisé]\n    B --> H[Apprentissage par renforcement]\n```\n\n\n## 2. Applications et Cas d'Utilisation\n\n### 2.1 Vision par Ordinateur\n- Détection d'objets\n- Reconnaissance faciale\n- Diagnostic médical (imagerie)\n- Voitures autonomes\n\n### 2.2 Traitement du Langage Naturel (NLP)\n- Chatbots et assistants virtuels\n- Traduction automatique\n- Analyse de sentiments\n- Résumé automatique de textes\n\n### 2.3 Systèmes de Recommandation\n- E-commerce (Amazon, Alibaba)\n- Streaming (Netflix, YouTube)\n- Réseaux sociaux (Facebook, Instagram)\n\n### 2.4 Finance\n- Détection de fraude\n- Trading algorithmique\n- Évaluation de risque de crédit\n\n### 2.5 Santé\n- Diagnostic de maladies\n- Découverte de médicaments\n- Analyse d'imagerie médicale\n\n## 3. Types d'Apprentissage\n\n### 3.1 Apprentissage Supervisé\n\nLe modèle apprend à partir de **données étiquetées** (avec réponses connues).\n\n::: {.callout-note}\n## Exemple\n**Données d'entraînement**: emails avec labels \"spam\" ou \"non spam\"\n**Objectif**: Prédire si un nouveau email est spam\n:::\n\n**Tâches principales**:\n- **Classification**: prédire une catégorie (spam/non spam, chat/chien)\n- **Régression**: prédire une valeur continue (prix maison, température)\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\n# Données étiquetées\nX_train = [[feature1, feature2], ...]  # caractéristiques\ny_train = [0, 1, 1, 0, ...]            # labels (0=non spam, 1=spam)\n\n# Entraînement\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Prédiction\nprediction = model.predict([[new_feature1, new_feature2]])\n```\n:::\n\n\n### 3.2 Apprentissage Non Supervisé\n\nLe modèle apprend à partir de **données non étiquetées** (sans réponses).\n\n::: {.callout-note}\n## Exemple\n**Données**: comportements d'achat de clients\n**Objectif**: Identifier des groupes de clients similaires (segmentation)\n:::\n\n**Tâches principales**:\n- **Clustering**: regrouper des données similaires\n- **Réduction de dimension**: simplifier les données\n- **Détection d'anomalies**: identifier des points inhabituels\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\n# Données non étiquetées\nX = [[feature1, feature2], ...]\n\n# Clustering\nkmeans = KMeans(n_clusters=3)\nclusters = kmeans.fit_predict(X)\n```\n:::\n\n\n### 3.3 Apprentissage Semi-Supervisé\n\nCombine données étiquetées (peu) et non étiquetées (beaucoup).\n\n**Cas d'usage**: Lorsque l'étiquetage est coûteux (imagerie médicale, reconnaissance vocale)\n\n### 3.4 Apprentissage par Renforcement\n\nL'agent apprend par **essai-erreur** en interagissant avec un environnement.\n\n::: {.callout-note}\n## Exemple\n- Jeux vidéo (AlphaGo, Chess AI)\n- Robotique\n- Contrôle de systèmes complexes\n:::\n\n**Composants**:\n- **Agent**: celui qui apprend\n- **Environnement**: le monde dans lequel l'agent évolue\n- **Actions**: ce que l'agent peut faire\n- **Récompenses**: feedback positif/négatif\n\n## 4. Étapes de Conception d'un Modèle IA\n\n### 4.1 Pipeline ML Standard\n\n\n**Diagramme mermaid:**\n```{mermaid}\ngraph TB\n    A[1 Définir le problème]\n    B[2 Collecter les données]\n    C[3 Explorer les données]\n    D[4 Préparer les données]\n    E[5 Choisir un modèle]\n    F[6 Entraîner le modèle]\n    G[7 Évaluer le modèle]\n    H{Performance OK?}\n    I[8 Déployer]\n    J[9 Monitorer]\n    \n    A --> B\n    B --> C\n    C --> D\n    D --> E\n    E --> F\n    F --> G\n    G --> H\n    H -->|Non| E\n    H -->|Oui| I\n    I --> J\n```\n\n\n### 4.2 Détails des Étapes\n\n#### Étape 1: Définir le Problème\n- Quel type de problème? (classification, régression, clustering)\n- Quelles sont les métriques de succès?\n- Quelles sont les contraintes?\n\n#### Étape 2: Collecter les Données\n- Sources de données\n- Quantité nécessaire\n- Qualité des données\n\n#### Étape 3: Explorer les Données (EDA)\n- Statistiques descriptives\n- Visualisations\n- Identifier les patterns, outliers, données manquantes\n\n#### Étape 4: Préparer les Données\n- Nettoyage (valeurs manquantes, doublons)\n- Transformation (normalisation, encodage)\n- Feature engineering\n- Split train/test\n\n#### Étape 5: Choisir un Modèle\n- Basé sur le type de problème\n- Complexité vs interprétabilité\n- Ressources disponibles\n\n#### Étape 6: Entraîner le Modèle\n- Ajuster les paramètres\n- Optimisation\n\n#### Étape 7: Évaluer le Modèle\n- Métriques appropriées\n- Validation croisée\n- Analyse des erreurs\n\n#### Étape 8: Déployer\n- Mise en production\n- API, application web, etc.\n\n#### Étape 9: Monitorer\n- Performances en production\n- Dérive des données (data drift)\n- Mise à jour du modèle\n\n## 5. Concepts Clés\n\n### 5.1 Overfitting vs Underfitting\n\n::: {.panel-tabset}\n\n## Underfitting\n- Modèle **trop simple**\n- Ne capture pas les patterns dans les données\n- **Biais élevé**, variance faible\n- Mauvaise performance train ET test\n\n## Overfitting\n- Modèle **trop complexe**\n- Mémorise les données d'entraînement (bruit inclus)\n- Biais faible, **variance élevée**\n- Bonne performance train, **mauvaise** performance test\n\n## Juste bien (Good fit)\n- Modèle équilibré\n- Capture les vrais patterns\n- Biais et variance faibles\n- Bonne généralisation\n\n:::\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Visualisation du concept\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Données\nnp.random.seed(42)\nX = np.linspace(0, 10, 50)\ny = 2*X + 1 + np.random.randn(50)*2\n\n# Underfitting: modèle constant\nunderfit = np.mean(y) * np.ones_like(X)\n\n# Good fit: régression linéaire\ncoeffs = np.polyfit(X, y, 1)\ngoodfit = np.polyval(coeffs, X)\n\n# Overfitting: polynôme degré 15\ncoeffs_over = np.polyfit(X, y, 15)\noverfit = np.polyval(coeffs_over, X)\n\nplt.figure(figsize=(15, 4))\n\nplt.subplot(131)\nplt.scatter(X, y, alpha=0.5)\nplt.plot(X, underfit, 'r-', linewidth=2)\nplt.title('Underfitting')\n\nplt.subplot(132)\nplt.scatter(X, y, alpha=0.5)\nplt.plot(X, goodfit, 'g-', linewidth=2)\nplt.title('Good Fit')\n\nplt.subplot(133)\nplt.scatter(X, y, alpha=0.5)\nplt.plot(X, overfit, 'b-', linewidth=2)\nplt.title('Overfitting')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### 5.2 Compromis Biais-Variance\n\n\n**Diagramme mermaid:**\n```{mermaid}\ngraph TD\n    A[Erreur Totale] --> B[Biais]\n    A --> C[Variance]\n    A --> D[Bruit irréductible]\n    B --> E[Underfitting]\n    C --> F[Overfitting]\n```\n\n\n## 6. Exercices de Réflexion\n\n::: {.callout-warning icon=false}\n## Question 1\nPour chacun des problèmes suivants, identifiez le type d'apprentissage approprié (supervisé, non supervisé, renforcement):\n\na) Prédire si un patient a une maladie cardiaque\nb) Regrouper des articles de presse par thème\nc) Apprendre à un robot à marcher\nd) Prédire le prix d'une maison\ne) Détecter des transactions frauduleuses inhabituelles\n:::\n\n::: {.callout-warning icon=false}\n## Question 2\nExpliquez pourquoi un modèle avec 100% de précision sur les données d'entraînement peut être problématique.\n:::\n\n::: {.callout-warning icon=false}\n## Question 3\nDonnez 3 exemples d'applications ML dans votre domaine d'intérêt et identifiez le type de problème (classification, régression, clustering).\n:::\n\n## Résumé de la Séance\n\n::: {.callout-important icon=false}\n## Points clés à retenir\n\n1. **ML** = apprentissage à partir de données sans programmation explicite\n2. **Trois types principaux**: supervisé, non supervisé, renforcement\n3. **Pipeline ML**: Problème → Données → Exploration → Préparation → Modèle → Évaluation → Déploiement\n4. **Overfitting vs Underfitting**: équilibre crucial pour la généralisation\n5. **Applications diverses**: vision, NLP, recommandations, finance, santé\n\n:::\n\n## Lectures Complémentaires\n\n1. Géron, A. (2019) - Chapitre 1: The Machine Learning Landscape\n2. [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)\n3. [Andrew Ng - What is Machine Learning?](https://www.coursera.org/learn/machine-learning)\n\n",
    "supporting": [
      "seance1_files\\figure-pdf"
    ],
    "filters": []
  }
}